{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "# General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {},
   "source": [
    "## 1. Create Datasets\n",
    "\n",
    "- read in the human proteome (use reviewed proteins)\n",
    "- filter for armadillo repeat proteins\n",
    "- filter for Transcription Factor proteins\n",
    "- Filter Transcription Factors for disordered Proteins\n",
    "- STRING database matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional, Set\n",
    "\n",
    "def contains_any_annotation(cell_value: Any, annotations_list: List[str]) -> bool:\n",
    "    \"\"\"check if any of the annotations are in the column value\n",
    "\n",
    "    Args:\n",
    "        cell_value (Any): Cell value from DataFrame column to check\n",
    "        annotations_list (List[str]): List of annotation strings to check for\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any annotation is found in the cell_value, False otherwise\n",
    "    \"\"\"\n",
    "    if pd.isna(cell_value):\n",
    "        return False\n",
    "    for annotation in annotations_list:\n",
    "        if annotation in cell_value:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.links.full.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "IUPRED3_PATH = '../../iupred3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proteins = pd.read_csv(PROTEOME_PATH, sep='\\t')\n",
    "reviewed_proteins = all_proteins[all_proteins['Reviewed'] == 'reviewed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Armadillos\n",
    "## all the interpro IDs that have the word \"armadillo\" in the description\n",
    "arm_accs_ipr = [\n",
    "    'IPR013636',\n",
    "    'IPR024574',\n",
    "    'IPR041322',\n",
    "    'IPR055241',\n",
    "    'IPR056252',\n",
    "    'IPR016617',\n",
    "    'IPR031524',\n",
    "    'IPR038739',\n",
    "    'IPR038905',\n",
    "    'IPR039868',\n",
    "    'IPR040268',\n",
    "    'IPR042462',\n",
    "    'IPR042834',\n",
    "    'IPR043379',\n",
    "    'IPR044282',\n",
    "    'IPR051303',\n",
    "    'IPR052441',\n",
    "    'IPR011989',\n",
    "    'IPR016024',\n",
    "    'IPR000225',\n",
    "    'IPR041209',\n",
    "    'IPR049152',\n",
    "    'IPR006911'\n",
    "]\n",
    "\n",
    "# all pfam accessions that have \"armadillo\" in the description\n",
    "arm_accs_pfam = [\n",
    "    'PF00514.29',\n",
    "    'PF17822.6',\n",
    "    'PF08427.15',\n",
    "    'PF15767.10',\n",
    "    'PF22915.1',\n",
    "    'PF04826.19',\n",
    "    'PF23295.1',\n",
    "    'PF16629.10',\n",
    "    'PF18770.7',\n",
    "    'PF21052.3',\n",
    "    'PF11841.14',\n",
    "    'PF14726.11',\n",
    "    'PF18581.7'\n",
    "]\n",
    "\n",
    "# strip version number since it is not included in the uniprot annotation\n",
    "for i in range(len(arm_accs_pfam)):\n",
    "    arm_accs_pfam[i] = arm_accs_pfam[i].split(\".\")[0]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = reviewed_proteins['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(reviewed_proteins[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = reviewed_proteins[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "armadillo_proteins['Entry'].to_csv('armadillo_proteins_entries.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = reviewed_proteins[reviewed_proteins['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d244af",
   "metadata": {},
   "source": [
    "### Filter disordered transcription factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37cf2f",
   "metadata": {},
   "source": [
    "#### AIUPred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecd2c5",
   "metadata": {},
   "source": [
    "##### Annotaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c22ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API requests (only necessary once, see cache file AIUPRED_PATH)\n",
    "\n",
    "# url = 'https://aiupred.elte.hu/rest_api'\n",
    "\n",
    "# AIUPred_data = []\n",
    "# c = 0\n",
    "# for acc in tf_proteins_curated_ds['Entry'].tolist():\n",
    "#     data = {'accession': acc, 'smoothing': 'default'}\n",
    "#     response = requests.get(url, params=data)\n",
    "#     if response.status_code == 200:\n",
    "#         AIUPred_data.append(json.loads(response.text))\n",
    "#         c += 1\n",
    "#         if c % 100 == 0:\n",
    "#             print(c)\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch data for accession {acc}: {response.status_code}\")\n",
    "        \n",
    "# with open('AIUPred_data.json', 'a') as json_file:\n",
    "#     json.dump(AIUPred_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cache file\n",
    "with open(AIUPRED_PATH, 'r') as file:\n",
    "    AIUPred_df = pd.DataFrame(json.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56f6c0",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def find_subranges(data: List[float], threshold: float, min_length: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Find continuous subranges in data where values exceed the threshold for at least min_length positions\n",
    "\n",
    "    Args:\n",
    "        data (List[float]): List of numerical values to analyze\n",
    "        threshold (float): Minimum value to be considered part of a subrange\n",
    "        min_length (int): Minimum length a subrange must have to be included in results\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: List of tuples containing start and end indices of subranges\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    start = None\n",
    "\n",
    "    for i, value in enumerate(data):\n",
    "        if value >= threshold:\n",
    "            if start is None:\n",
    "                start = i\n",
    "            # Check if it's the last element or the next element does not exceed x\n",
    "            if i == len(data) - 1 or data[i + 1] < threshold:\n",
    "                if i - start + 1 >= min_length:\n",
    "                    subranges.append((start, i))\n",
    "                start = None\n",
    "        else:\n",
    "            start = None\n",
    "\n",
    "    return subranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH_DISORDERED_REGION = 20\n",
    "AIUPRED_THRESHOLD = 0.9\n",
    "\n",
    "AIUPred_df['ind_disordered_regions'] = AIUPred_df['AIUPred'].apply(lambda x: find_subranges(x, AIUPRED_THRESHOLD, MIN_LENGTH_DISORDERED_REGION))\n",
    "AIUPred_df['num_disordered_regions'] = AIUPred_df['ind_disordered_regions'].apply(len)\n",
    "tf_proteins_curated_ds_AIUpred = tf_proteins_curated_ds.merge(AIUPred_df, left_on='Entry', right_on='accession', how='inner')\n",
    "tf_proteins_curated_ds_AIUpred_diso = tf_proteins_curated_ds_AIUpred[tf_proteins_curated_ds_AIUpred['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f'Num proteins before disorder filter: {len(tf_proteins_curated_ds_AIUpred)}')\n",
    "print(f'Num proteins after disorder filter: {len(tf_proteins_curated_ds_AIUpred_diso)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(IUPRED3_PATH)\n",
    "import iupred3_lib\n",
    "\n",
    "sequence = tf_proteins_curated_ds['Sequence'].iloc[0]\n",
    "print(sequence)\n",
    "iupred3_result = iupred3_lib.iupred(sequence, 'long', smoothing='no')\n",
    "print(iupred3_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IUPRED3_THRESHOLD = 0.5\n",
    "\n",
    "tf_proteins_curated_ds['iupred3'] = tf_proteins_curated_ds['Sequence'].apply(lambda x: iupred3_lib.iupred(x, 'long', smoothing='no')[0])\n",
    "\n",
    "tf_proteins_curated_ds['num_disordered_regions'] = tf_proteins_curated_ds['iupred3'].apply(lambda x: len(find_subranges(x, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION)))\n",
    "print(len(tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]))\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "#### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_pairs(arm_df: pd.DataFrame, tf_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates a dataframe with all possible pairs from the cartesian product of the two dataframes arm_df and tf_df.\n",
    "    This function performs a full cartesian join between the armadillo proteins dataframe and the transcription factor proteins\n",
    "    dataframe, generating all possible combinations between them.\n",
    "\n",
    "    Args:\n",
    "        arm_df (pd.DataFrame): DataFrame containing armadillo proteins\n",
    "        tf_df (pd.DataFrame): DataFrame containing transcription factor proteins\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all possible pairs between armadillo and transcription factor proteins\n",
    "        with a unique pair_id column for each combination\n",
    "    \"\"\"\n",
    "    # Create a key for cross join\n",
    "    arm_df_temp = arm_df.copy()\n",
    "    tf_df_temp = tf_df.copy()\n",
    "    \n",
    "    arm_df_temp['key'] = 1\n",
    "    tf_df_temp['key'] = 1\n",
    "    \n",
    "    # Perform a cross join using the dummy key\n",
    "    pairs_df = pd.merge(arm_df_temp, tf_df_temp, on='key', suffixes=('_arm', '_tf'))\n",
    "    \n",
    "    # Drop the dummy key column\n",
    "    pairs_df = pairs_df.drop('key', axis=1)\n",
    "    \n",
    "    # Create pair_id column for consistency with other functions in the pipeline\n",
    "    pairs_df['pair_id'] = pairs_df.apply(lambda row: str(tuple(sorted([row['Entry_arm'].upper(), row['Entry_tf'].upper()]))), axis=1)\n",
    "    \n",
    "    print(f\"Created {len(pairs_df)} possible protein pairs between {len(arm_df)} armadillo proteins and {len(tf_df)} transcription factors\")\n",
    "    \n",
    "    return pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "print(f\"Number of pairs in all_pairs: {len(all_pairs)}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {len(all_pairs_w_STRING)}\")\n",
    "print(f\"Number of unmatched pairs: {len(unmatched_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66782a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_df = pd.read_csv('../../data/IntAct/human/human.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f796f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_df['pair_id'] = intact_df.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', ''), row['ID(s) interactor B'].replace('uniprotkb:', '')]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_IntAct['pair_id'])]\n",
    "print(f\"Number of pairs in all_pairs: {len(all_pairs)}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {len(all_pairs_w_IntAct)}\")\n",
    "print(f\"Number of unmatched pairs: {len(unmatched_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "\n",
    "print(len(all_pairs_w_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fd92d",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55461481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Tuple\n",
    "import math, random\n",
    "import os\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf6053",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_af_jobs_to_individual_files(af_jobs: List[Dict[str, Any]], output_dir: str) -> None:\n",
    "    \"\"\"Write each Alphafold job to an individual file.\n",
    "    \n",
    "    Each job is saved as a JSON file in the specified output directory with the job's name as the filename.\n",
    "    \n",
    "    Args:\n",
    "        af_jobs (List[Dict[str, Any]]): List of AlphaFold job dictionaries\n",
    "        output_dir (str): Directory where job files will be saved\n",
    "    \"\"\"\n",
    " \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for job in af_jobs:\n",
    "        file_name = f\"{job['name']}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump([job], f, indent=2) # use [] so AF parser knows it's in alphafoldserver dialect\n",
    "\n",
    "def sort_rec(obj: Union[List, Dict, Any]) -> Any:\n",
    "    \"\"\"Sort a list or dictionary recursively.\n",
    "    \n",
    "    This function is used to create comparable job representations by sorting all nested structures.\n",
    "    \n",
    "    Args:\n",
    "        obj (Union[List, Dict, Any]): Object to sort recursively\n",
    "\n",
    "    Returns:\n",
    "        Any: Sorted object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return sorted((k, sort_rec(v)) for k, v in obj.items())\n",
    "    if isinstance(obj, list):\n",
    "        return sorted(sort_rec(x) for x in obj)\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def get_comparable_job(job_data: Dict[str, Any], deep_copy: bool = True) -> Any:\n",
    "    \"\"\"Create a comparable (optional deep-copy) representation of the job by removing the name field and sorting the other fields.\n",
    "    \n",
    "    This function creates a standardized representation of an AlphaFold job that can be compared\n",
    "    to other jobs to detect duplicates, regardless of the job name or field order.\n",
    "    \n",
    "    Args:\n",
    "        job_data (Dict[str, Any]): AlphaFold job dictionary\n",
    "        deep_copy (bool, optional): Whether to create a deep copy of the job data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Any: Comparable representation of the job\n",
    "    \"\"\"\n",
    "    if deep_copy:\n",
    "        # Create a deep copy to avoid modifying the original\n",
    "        comparable = copy.deepcopy(job_data)\n",
    "    else:\n",
    "        comparable = job_data\n",
    "    if 'name' in comparable:\n",
    "        del comparable['name']\n",
    "    return sort_rec(comparable)\n",
    "\n",
    "def collect_created_jobs(results_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect all jobs in a directory (all .json files are considered jobs).\n",
    "    \n",
    "    IMPORTANT: one file is considered to have one job!\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Directory containing AlphaFold job files\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of AlphaFold job dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    collected_jobs = []\n",
    "    \n",
    "    # Go through all .json files in the results directory (not recursively)\n",
    "    for file_name in os.listdir(results_dir):\n",
    "        if file_name.endswith('.json') and os.path.isfile(os.path.join(results_dir, file_name)):\n",
    "            try:\n",
    "                with open(os.path.join(results_dir, file_name), 'r') as f:\n",
    "                    collected_jobs += json.load(f)\n",
    "                    \n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "                continue\n",
    "    return collected_jobs\n",
    "\n",
    "def create_alphafold_job(job_name: str, sequence1: str, sequence2: str, dialect: str = 'alphafoldserver') -> Dict[str, Any]:\n",
    "    \"\"\"Create a standardized AlphaFold job dictionary from input parameters.\n",
    "\n",
    "    Args:\n",
    "        job_name (str): Name of the job, typically in the format \"protein1_start-end_protein2_start-end\"\n",
    "        sequence1 (str): Amino acid sequence of the first protein\n",
    "        sequence2 (str): Amino acid sequence of the second protein\n",
    "        dialect (str, optional): The dialect to use for AlphaFold. Defaults to 'alphafoldserver'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary representing the AlphaFold job in the specified format\n",
    "    \"\"\"\n",
    "    job = {\n",
    "        'name': job_name,\n",
    "        'modelSeeds': [],\n",
    "        'sequences': [\n",
    "            {\n",
    "                'proteinChain': {\n",
    "                    'sequence': sequence1,\n",
    "                    'count': 1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'proteinChain': {\n",
    "                    'sequence': sequence2,\n",
    "                    'count': 1\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'dialect': dialect,\n",
    "        'version': 1,\n",
    "    }\n",
    "    return job\n",
    "\n",
    "# FIXME: there is a possible bug with the redistribution of unused quotas of single categories\n",
    "def create_job_batch_w_scorePress(pair_df: pd.DataFrame, batch_size: int, categories: List[Tuple[int, int]], \n",
    "                                job_dirs: List[str], string_score_type: str = 'combined_score') -> List[Dict[str, Any]]:\n",
    "    \"\"\"Creates a new batch of batch_size jobs from pair_df.\n",
    "    \n",
    "    Don't create jobs that have been created previously.\n",
    "    For each category (range of string scores) in categories create \n",
    "    batch_size/len(categories) new jobs randomly sampled from all possible jobs in the category.\n",
    "    If a category does not have enough possible jobs to fill the limit, redistribute to the other\n",
    "    categories.\n",
    "    \n",
    "    Note: categories should be specified in a way so the category with the largest number of possible jobs comes in the end of the array.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        pair_df (pd.DataFrame): DataFrame containing protein pairs with sequences and other information\n",
    "        batch_size (int): Total number of jobs to create across all categories\n",
    "        categories (List[Tuple[int, int]]): List of tuples (min_score, max_score) defining score ranges for each category\n",
    "        job_dirs (List[str]): List of directories to search for existing jobs to avoid duplicates\n",
    "        string_score_type (str, optional): Column name in pair_df for the score to filter on. Defaults to 'combined_score'.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of newly created AlphaFold job dictionaries\n",
    "    \"\"\"\n",
    "    new_jobs = []\n",
    "    prev_jobs = []\n",
    "    for dir in job_dirs:\n",
    "        prev_jobs += collect_created_jobs(dir)\n",
    "        \n",
    "    for i in range(len(prev_jobs)):\n",
    "        prev_jobs[i] = get_comparable_job(prev_jobs[i], deep_copy=False)\n",
    "    \n",
    "    total_created = 0\n",
    "    num_categories = len(categories)\n",
    "    category_counts = {}  # Dictionary to track jobs created in each category\n",
    "    \n",
    "    for category in categories:\n",
    "        # Stop if we've already created enough jobs\n",
    "        if total_created >= batch_size:\n",
    "            break\n",
    "            \n",
    "        # dynamically adjust the sub_batch_size to account for categories that don't fill their quota\n",
    "        cat_quota = math.floor((batch_size-total_created)/num_categories)\n",
    "        num_categories -= 1\n",
    "        \n",
    "        min_score = category[0]\n",
    "        max_score = category[1]\n",
    "        created_in_category = 0\n",
    "        possible_pairs = pair_df[(pair_df[string_score_type] >= min_score) & (pair_df[string_score_type] <= max_score)]\n",
    "        possible_ind = possible_pairs.index.tolist()\n",
    "        \n",
    "        category_key = f\"{min_score}-{max_score}\"\n",
    "        category_counts[category_key] = 0\n",
    "        \n",
    "        while created_in_category < cat_quota and len(possible_ind) > 0:\n",
    "            ind = random.choice(possible_ind)\n",
    "            possible_ind.remove(ind)\n",
    "            \n",
    "            row = pair_df.iloc[ind]\n",
    "            armadillo_entry = row['Entry_arm']\n",
    "            tf_entry = row['Entry_tf']\n",
    "            armadillo_sequence = row['Sequence_arm']\n",
    "            tf_sequence = row['Sequence_tf']\n",
    "            \n",
    "            # Calculate indices for the sequences\n",
    "            armadillo_x, armadillo_y = 1, len(armadillo_sequence)\n",
    "            tf_x, tf_y = 1, len(tf_sequence)\n",
    "\n",
    "            # Generate job name in the specified format\n",
    "            job_name = f\"{armadillo_entry}_{armadillo_x}-{armadillo_y}_{tf_entry}_{tf_x}-{tf_y}\"\n",
    "            \n",
    "            # Create job using the helper function\n",
    "            job = create_alphafold_job(job_name, armadillo_sequence, tf_sequence)\n",
    "            \n",
    "            # check if job was already created earlier\n",
    "            job_comparable = get_comparable_job(job)\n",
    "            if not any(existing_comparable == job_comparable for existing_comparable in prev_jobs):\n",
    "                # no duplicate job found\n",
    "                created_in_category += 1\n",
    "                total_created += 1\n",
    "                category_counts[category_key] += 1\n",
    "                prev_jobs.append(get_comparable_job(job))\n",
    "                new_jobs.append(job)\n",
    "                \n",
    "                # Ensure we don't exceed batch_size\n",
    "                if total_created >= batch_size:\n",
    "                    break\n",
    "        \n",
    "    # Print the number of jobs created in each category\n",
    "    print(f\"Created {len(new_jobs)} new jobs total.\")\n",
    "    print(\"Jobs created per category:\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  Score range {category}: {count} jobs\")\n",
    "    \n",
    "    return new_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DIRS = ['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6', 'batch_7', 'batch_8', 'batch_9']\n",
    "BATCH_DIRS = [os.path.join('../../production1', d) for d in os.listdir('../../production1') if os.path.isdir(os.path.join('../../production1', d)) and 'batch' in d]\n",
    "BATCH_SIZE = 150\n",
    "categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (200,300), (0,100), (100,200)]\n",
    "# note that the order is important. The category (100,200) is verly large so it comes last to fill up the remaining jobs\n",
    "new_af_jobs = create_job_batch_w_scorePress(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/batch_12')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b2c0c",
   "metadata": {},
   "source": [
    "### Verify result completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hpc_results(hpc_results_dir: str, required_files_template: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Verify that all required files are present in each subfolder of the HPC_results directory.\n",
    "    \n",
    "    This function checks each job folder in the HPC results directory to ensure that all expected\n",
    "    output files have been generated correctly.\n",
    "\n",
    "    Args:\n",
    "        hpc_results_dir (str): Path to the HPC_results directory.\n",
    "        required_files_template (List[str]): List of required file names with 'JOB_NAME' as a placeholder.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary with job names as keys and a list of missing files as values.\n",
    "    \"\"\"\n",
    "    missing_files_report = {}\n",
    "\n",
    "    for job_folder in os.listdir(hpc_results_dir):\n",
    "        job_path = os.path.join(hpc_results_dir, job_folder)\n",
    "\n",
    "        if not os.path.isdir(job_path):\n",
    "            continue\n",
    "\n",
    "        # Replace 'JOB_NAME' in the template with the actual job name\n",
    "        required_files = [file.replace(\"JOB_NAME\", job_folder) for file in required_files_template]\n",
    "\n",
    "        missing_files = [file for file in required_files if not os.path.exists(os.path.join(job_path, file))]\n",
    "\n",
    "        if missing_files:\n",
    "            missing_files_report[job_folder] = missing_files\n",
    "\n",
    "    return missing_files_report\n",
    "\n",
    "def missing_files_report() -> None:\n",
    "    \"\"\"Generate a report of missing files in the HPC results directory.\n",
    "    \n",
    "    This function checks for all required output files from AlphaFold jobs and reports any missing files.\n",
    "    \"\"\"\n",
    "    required_files_template = [\n",
    "        \"JOB_NAME_confidences.json\",\n",
    "        \"JOB_NAME_data.json\",\n",
    "        \"JOB_NAME_model.cif\",\n",
    "        \"JOB_NAME_summary_confidences.json\",\n",
    "        \"ranking_scores.csv\",\n",
    "        \"seed-1_sample-0\",\n",
    "        \"seed-1_sample-1\",\n",
    "        \"seed-1_sample-2\",\n",
    "        \"seed-1_sample-3\",\n",
    "        \"seed-1_sample-4\",\n",
    "    ]\n",
    "    report = verify_hpc_results(HPC_RESULT_DIR, required_files_template)\n",
    "    if report:\n",
    "        print(\"Missing files detected:\")\n",
    "        for job, files in missing_files_report.items():\n",
    "            print(f\"Job: {job}, Missing Files: {files}\")\n",
    "    else:\n",
    "        print(\"All files are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed083c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_files_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143d563",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair_id(row: pd.Series) -> str:\n",
    "    \"\"\"Create a pair ID from the job name by extracting the Uniprot IDs and sorting them.\n",
    "    \n",
    "    This function parses an AlphaFold job name to extract the protein IDs and creates a\n",
    "    standardized pair identifier by sorting the IDs alphabetically.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): DataFrame row containing 'job_name' column\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Tuple of sorted protein IDs\n",
    "    \"\"\"\n",
    "    parts = str(row['job_name']).split('_')\n",
    "    return str(tuple(sorted([parts[0].upper(), parts[2].upper()])))\n",
    "\n",
    "def find_summary_files(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Recursively find summary_confidences.json files.\n",
    "    \n",
    "    This function searches through the given directory and all subdirectories to find\n",
    "    AlphaFold summary confidence files and loads them into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Root directory to search for summary files\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of dictionaries containing the contents of the summary files\n",
    "    \"\"\"\n",
    "    all_jobs_data = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Check for summary_confidences.json files\n",
    "        summary_files = [f for f in files if f.endswith('_summary_confidences.json')]\n",
    "        \n",
    "        if summary_files:\n",
    "            for summary_file in summary_files:\n",
    "                # Extract job name from the filename\n",
    "                job_name = summary_file.replace('_summary_confidences.json', '')\n",
    "                \n",
    "                # Check if job with this job_name is already in all_jobs_data\n",
    "                if any(job.get(\"job_name\") == job_name for job in all_jobs_data):\n",
    "                    continue\n",
    "                \n",
    "                summary_path = os.path.join(root, summary_file)\n",
    "                \n",
    "                # Read the summary file\n",
    "                try:\n",
    "                    with open(summary_path) as f:\n",
    "                        data = json.load(f)\n",
    "                        # Add job folder name to the data\n",
    "                        data[\"job_name\"] = job_name\n",
    "                        all_jobs_data.append(data)\n",
    "                except (json.JSONDecodeError, IOError) as e:\n",
    "                    print(f\"Error reading {summary_path}: {e}\")\n",
    "    return all_jobs_data\n",
    "\n",
    "def remove_0(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Remove all rows where the specified columns have a value of 0.\n",
    "    \n",
    "    This function filters a DataFrame to keep only rows where the values in the specified columns are greater than 0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to filter\n",
    "        cols (List[str]): List of column names to check for 0 values\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with rows where specified columns have non-zero values\n",
    "    \"\"\"\n",
    "    df[cols] = df[df[cols] > 0][cols]\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea998a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(pairs: pd.DataFrame, results: pd.DataFrame, boxplot: bool = False, description: str = '') -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Print important statistics for a dataset of protein pairs.\n",
    "    \n",
    "    This function filters the results dataframe by the pairs dataframe and calculates statistics\n",
    "    (mean, median, standard deviation, min, max) for the iptm, ptm, and ranking score metrics.\n",
    "    If boxplot=True, it also creates a boxplot visualization for these three scores.\n",
    "\n",
    "    Args:\n",
    "        pairs (pd.DataFrame): DataFrame containing protein pairs with 'pair_id' column\n",
    "        results (pd.DataFrame): DataFrame containing AlphaFold results with 'pair_id' column\n",
    "        boxplot (bool, optional): Whether to create boxplots. Defaults to False.\n",
    "        description (str, optional): Description to add to the plot title. Defaults to ''.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Dict[str, float]]: Dictionary with statistics for each metric\n",
    "    \"\"\"\n",
    "    # Ensure pairs DataFrame has pair_id column\n",
    "    if 'pair_id' not in pairs.columns:\n",
    "        # Create pair_id from p1_Uniprot and p2_Uniprot if needed\n",
    "        if 'p1_Uniprot' in pairs.columns and 'p2_Uniprot' in pairs.columns:\n",
    "            pairs['pair_id'] = pairs.apply(lambda row: tuple(sorted([row['p1_Uniprot'].upper(), row['p2_Uniprot'].upper()])), axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"pairs DataFrame must have 'pair_id' column or ('p1_Uniprot' and 'p2_Uniprot') columns\")\n",
    "    \n",
    "    # Ensure results DataFrame has pair_id column\n",
    "    if 'pair_id' not in results.columns:\n",
    "        # Create pair_id from job_name if needed\n",
    "        if 'job_name' in results.columns:\n",
    "            results['pair_id'] = results.apply(create_pair_id, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"results DataFrame must have 'pair_id' column or 'job_name' column\")\n",
    "    \n",
    "    # Merge dataframes to get matching pairs\n",
    "    merged_df = pd.merge(results, pairs, on='pair_id', how='inner')\n",
    "    \n",
    "    # Calculate statistics for iptm, ptm, and ranking_score\n",
    "    metrics = ['iptm', 'ptm', 'ranking_score']\n",
    "    stats = {}\n",
    "    \n",
    "    print(f\"Statistics for {len(merged_df)} protein pairs:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        mean_val = merged_df[metric].mean()\n",
    "        median_val = merged_df[metric].median()\n",
    "        std_val = merged_df[metric].std()\n",
    "        min_val = merged_df[metric].min()\n",
    "        max_val = merged_df[metric].max()\n",
    "        \n",
    "        stats[metric] = {\n",
    "            'mean': mean_val,\n",
    "            'median': median_val,\n",
    "            'std': std_val,\n",
    "            'min': min_val,\n",
    "            'max': max_val\n",
    "        }\n",
    "        \n",
    "        print(f\"{metric.upper()}\")\n",
    "        print(f\"  Mean: {mean_val:.4f}\")\n",
    "        print(f\"  Median: {median_val:.4f}\")\n",
    "        print(f\"  Standard Deviation: {std_val:.4f}\")\n",
    "        print(f\"  Min: {min_val:.4f}\")\n",
    "        print(f\"  Max: {max_val:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    if boxplot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Reshape data for boxplot\n",
    "        plot_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            plot_data.append(merged_df[metric])\n",
    "            labels.append(metric.upper())\n",
    "        \n",
    "        # Create boxplot\n",
    "        box = plt.boxplot(plot_data, patch_artist=True, labels=labels)\n",
    "        \n",
    "        # Add colors to boxplots\n",
    "        colors = ['lightblue', 'lightgreen', 'pink']\n",
    "        for patch, color in zip(box['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            \n",
    "        plt.annotate(f'Datapoints: {len(merged_df)}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "        \n",
    "        plt.title(f'Distribution of AlphaFold Scores: {description}')\n",
    "        plt.ylabel('Score Value')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_statistics(pairs1: pd.DataFrame, pairs2: pd.DataFrame, results: pd.DataFrame, \n",
    "                   label1: str = 'Group 1', label2: str = 'Group 2', description: str = '') -> Tuple[Dict[str, Dict[str, float]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compare statistics between two sets of protein pairs.\n",
    "    \n",
    "    This function filters the pairs1 and pairs2 dataframes by the results dataframe and creates\n",
    "    boxplots comparing iptm, ptm and ranking_score metrics between the two groups.\n",
    "    It also calculates and prints detailed statistics for each metric for both groups.\n",
    "\n",
    "    Args:\n",
    "        pairs1 (pd.DataFrame): First dataframe of protein pairs with 'pair_id' column \n",
    "                              or ('p1_Uniprot' and 'p2_Uniprot') columns\n",
    "        pairs2 (pd.DataFrame): Second dataframe of protein pairs with 'pair_id' column \n",
    "                              or ('p1_Uniprot' and 'p2_Uniprot') columns\n",
    "        results (pd.DataFrame): DataFrame containing AlphaFold results with 'pair_id' column \n",
    "                              or 'job_name' column\n",
    "        label1 (str, optional): Label for the first group. Defaults to 'Group 1'.\n",
    "        label2 (str, optional): Label for the second group. Defaults to 'Group 2'.\n",
    "        description (str, optional): Description for the plot title. Defaults to ''.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, float]], Dict[str, Dict[str, float]]]: Tuple containing two dictionaries \n",
    "                                                                         with statistics for each metric for both groups\n",
    "    \"\"\"\n",
    "    # Ensure pairs DataFrame has pair_id column\n",
    "    for pairs, label in [(pairs1, label1), (pairs2, label2)]:\n",
    "        if 'pair_id' not in pairs.columns:\n",
    "            # Create pair_id from p1_Uniprot and p2_Uniprot if needed\n",
    "            if 'p1_Uniprot' in pairs.columns and 'p2_Uniprot' in pairs.columns:\n",
    "                pairs['pair_id'] = pairs.apply(lambda row: tuple(sorted([row['p1_Uniprot'].upper(), row['p2_Uniprot'].upper()])), axis=1)\n",
    "            else:\n",
    "                raise ValueError(f\"{label} DataFrame must have 'pair_id' column or ('p1_Uniprot' and 'p2_Uniprot') columns\")\n",
    "    \n",
    "    # Ensure results DataFrame has pair_id column\n",
    "    if 'pair_id' not in results.columns:\n",
    "        # Create pair_id from job_name if needed\n",
    "        if 'job_name' in results.columns:\n",
    "            results['pair_id'] = results.apply(create_pair_id, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"results DataFrame must have 'pair_id' column or 'job_name' column\")\n",
    "    \n",
    "    # Merge dataframes to get matching pairs\n",
    "    merged_df1 = pd.merge(results, pairs1, on='pair_id', how='inner')\n",
    "    merged_df2 = pd.merge(results, pairs2, on='pair_id', how='inner')\n",
    "    \n",
    "    # Calculate statistics for iptm, ptm, and ranking_score\n",
    "    metrics = ['iptm', 'ptm', 'ranking_score']\n",
    "    stats1 = {}\n",
    "    stats2 = {}\n",
    "    \n",
    "    print(f\"Statistics comparison:\")\n",
    "    print(f\"{label1}: {len(merged_df1)} protein pairs\")\n",
    "    print(f\"{label2}: {len(merged_df2)} protein pairs\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate and print statistics for each metric\n",
    "    for metric in metrics:\n",
    "        # Group 1 statistics\n",
    "        mean_val1 = merged_df1[metric].mean()\n",
    "        median_val1 = merged_df1[metric].median()\n",
    "        std_val1 = merged_df1[metric].std()\n",
    "        min_val1 = merged_df1[metric].min()\n",
    "        max_val1 = merged_df1[metric].max()\n",
    "        \n",
    "        # Group 2 statistics\n",
    "        mean_val2 = merged_df2[metric].mean()\n",
    "        median_val2 = merged_df2[metric].median()\n",
    "        std_val2 = merged_df2[metric].std()\n",
    "        min_val2 = merged_df2[metric].min()\n",
    "        max_val2 = merged_df2[metric].max()\n",
    "        \n",
    "        stats1[metric] = {\n",
    "            'mean': mean_val1,\n",
    "            'median': median_val1,\n",
    "            'std': std_val1,\n",
    "            'min': min_val1,\n",
    "            'max': max_val1\n",
    "        }\n",
    "        \n",
    "        stats2[metric] = {\n",
    "            'mean': mean_val2,\n",
    "            'median': median_val2,\n",
    "            'std': std_val2,\n",
    "            'min': min_val2,\n",
    "            'max': max_val2\n",
    "        }\n",
    "        \n",
    "        print(f\"{metric.upper()}\")\n",
    "        print(f\"  {label1}:\")\n",
    "        print(f\"    Mean: {mean_val1:.4f}\")\n",
    "        print(f\"    Median: {median_val1:.4f}\")\n",
    "        print(f\"    Standard Deviation: {std_val1:.4f}\")\n",
    "        print(f\"    Min: {min_val1:.4f}\")\n",
    "        print(f\"    Max: {max_val1:.4f}\")\n",
    "        print(f\"  {label2}:\")\n",
    "        print(f\"    Mean: {mean_val2:.4f}\")\n",
    "        print(f\"    Median: {median_val2:.4f}\")\n",
    "        print(f\"    Standard Deviation: {std_val2:.4f}\")\n",
    "        print(f\"    Min: {min_val2:.4f}\")\n",
    "        print(f\"    Max: {max_val2:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Create comparison boxplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        data = [merged_df1[metric], merged_df2[metric]]\n",
    "        \n",
    "        # Create boxplot\n",
    "        bp = axes[i].boxplot(data, patch_artist=True, labels=[label1, label2])\n",
    "        \n",
    "        # Add colors to boxplots\n",
    "        colors = ['lightblue', 'lightgreen']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        axes[i].set_title(f'{metric.upper()}')\n",
    "        axes[i].grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "    plt.suptitle(f'Comparison of AlphaFold Scores: {description}', fontsize=14)\n",
    "    \n",
    "    # Add annotation with sample sizes\n",
    "    fig.text(0.05, 0.95, f\"{label1}: {len(merged_df1)} pairs\\n{label2}: {len(merged_df2)} pairs\", \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    return stats1, stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df = pd.DataFrame(data=find_summary_files(HPC_RESULT_DIR))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df)}\")\n",
    "\n",
    "results_df['pair_id'] = results_df.apply(create_pair_id, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_STRING_matched = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total rows in merged_df: {len(results_df_STRING_matched)}\")\n",
    "print(f\"Rows with STRING data: {results_df_STRING_matched['combined_score'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compare_statistics(score_gt_900_curated, score_lt150_curated, results_df, 'STRING score > 900', 'STRING score < 150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = print_statistics(pd.concat([score_gt_900_curated]), results_df, boxplot=True, description='STRING scores higher than 900')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = print_statistics(pd.concat([score_lt150_curated]), results_df, boxplot=True, description='STRING scores lower than 150')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea051cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_corr(c1: str, c2: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Create a scatter plot of two columns and calculate their correlation.\n",
    "    \n",
    "    This function creates a scatter plot between two columns of a DataFrame and\n",
    "    displays the correlation coefficient on the plot. It's useful for visualizing\n",
    "    relationships between different metrics like AlphaFold scores and STRING scores.\n",
    "\n",
    "    Args:\n",
    "        c1 (str): Name of the first column to plot on x-axis\n",
    "        c2 (str): Name of the second column to plot on y-axis\n",
    "        df (pd.DataFrame): DataFrame containing the columns to plot\n",
    "    \"\"\"\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(df[c1], df[c2], \n",
    "                        alpha=0.7, s=50)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(c1)\n",
    "    plt.ylabel(c2)\n",
    "    # plt.title('AlphaFold Ranking Score vs STRING Combined Score for Protein Pairs')\n",
    "\n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Calculate and display correlation\n",
    "    corr = df[c1].corr(df[c2])\n",
    "    plt.annotate(f'Correlation: {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scatter_corr('ranking_score', 'combined_score', merged_df)\n",
    "scatter_corr('ranking_score', 'experiments', remove_0(merged_df, ['experiments']))\n",
    "scatter_corr('ranking_score', 'database', remove_0(merged_df, ['database']))\n",
    "scatter_corr('ranking_score', 'experiments_transferred', remove_0(merged_df, ['experiments_transferred']))\n",
    "scatter_corr('ranking_score', 'coexpression', remove_0(merged_df, ['coexpression']))\n",
    "scatter_corr('ranking_score', 'coexpression_transferred', remove_0(merged_df, ['coexpression_transferred']))\n",
    "scatter_corr('iptm', 'combined_score', merged_df)\n",
    "scatter_corr('ptm', 'combined_score', merged_df)\n",
    "scatter_corr('iptm', 'ranking_score', merged_df)\n",
    "scatter_corr('has_clash', 'combined_score', merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of iptm vs ptm colored by combined_Score\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    x=results_df_STRING_matched['ptm'], \n",
    "    y=results_df_STRING_matched['iptm'], \n",
    "    c=results_df_STRING_matched['combined_score'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=50,  # Point size\n",
    "    edgecolors='w'  # White edge to make points stand out\n",
    ")\n",
    "\n",
    "# Add a color bar to show the scale of combined_Score\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Combined Score (STRING)', fontsize=12)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('PTM', fontsize=14)\n",
    "plt.ylabel('IPTM', fontsize=14)\n",
    "plt.title('Scatter Plot of IPTM vs PTM Colored by Combined Score (STRING)', fontsize=16)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Improve the appearance\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_plot(df: pd.DataFrame, x_metric: str, y_metric: str, color_metric: str, \n",
    "                       cmap: str = 'viridis', alpha: float = 0.7, size: int = 50) -> None:\n",
    "    \"\"\"Create a scatter plot with two metrics and color by a third metric.\n",
    "    \n",
    "    This function creates a scatter plot between two specified metrics with points colored\n",
    "    by a third metric. It's useful for visualizing relationships between different metrics\n",
    "    like iptm, ptm, and combined_Score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics to plot\n",
    "        x_metric (str): Name of the column to plot on x-axis\n",
    "        y_metric (str): Name of the column to plot on y-axis\n",
    "        color_metric (str): Name of the column to use for point colors\n",
    "        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n",
    "        alpha (float, optional): Transparency of points. Defaults to 0.7.\n",
    "        size (int, optional): Point size. Defaults to 50.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        x=df[x_metric], \n",
    "        y=df[y_metric], \n",
    "        c=df[color_metric],\n",
    "        cmap=cmap,\n",
    "        alpha=alpha,\n",
    "        s=size,\n",
    "        edgecolors='w'  # White edge to make points stand out\n",
    "    )\n",
    "\n",
    "    # Add a color bar to show the scale of the color metric\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(color_metric, fontsize=12)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_metric, fontsize=14)\n",
    "    plt.ylabel(y_metric, fontsize=14)\n",
    "    plt.title(f'Scatter Plot of {y_metric} vs {x_metric} Colored by {color_metric}', fontsize=16)\n",
    "\n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Improve the appearance\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots using the new function\n",
    "\n",
    "# 1. Recreate the original plot (iptm vs ptm colored by combined_Score)\n",
    "create_scatter_plot(results_df_STRING_matched, 'ptm', 'iptm', 'combined_score')\n",
    "\n",
    "# 2. Create ptm vs ranking_score colored by combined_Score\n",
    "create_scatter_plot(results_df_STRING_matched, 'ranking_score', 'ptm', 'combined_score')\n",
    "\n",
    "# 3. Create iptm vs ranking_score colored by combined_Score\n",
    "create_scatter_plot(results_df_STRING_matched, 'ranking_score', 'iptm', 'combined_score')\n",
    "\n",
    "\n",
    "create_scatter_plot(results_df_STRING_matched, 'ranking_score', 'iptm', 'experiments')\n",
    "\n",
    "create_scatter_plot(results_df_STRING_matched, 'ptm', 'iptm', 'experiments')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
