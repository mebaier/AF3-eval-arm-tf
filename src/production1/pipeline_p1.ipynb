{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "## General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.physical.links.detailed.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "# PROTEOME_PATH = '/home/markus/MPI_local/data/full_UP/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uniprot = pd.read_csv(PROTEOME_PATH, low_memory=False, sep='\\t')\n",
    "uniprot_filtered = all_uniprot[all_uniprot['Reviewed'] == 'reviewed']\n",
    "# uniprot_filtered = all_uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip version number since it is not included in the uniprot annotation\n",
    "arm_accs_pfam_stripped = [acc.split(\".\")[0] for acc in arm_accs_pfam]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = uniprot_filtered['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(uniprot_filtered[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = uniprot_filtered['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(uniprot_filtered[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = uniprot_filtered['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam_stripped))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(uniprot_filtered[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = uniprot_filtered[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "# TODO: use caching\n",
    "# TODO: adjust ensemble mapping if using other proteome?\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = uniprot_filtered[uniprot_filtered['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))\n",
    "\n",
    "# 4m 1.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds = add_iupred3(tf_proteins_curated_ds, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f\"Number of transcription factors with at least one disordered region (IUPred3, threshold={IUPRED3_THRESHOLD}, min length={MIN_LENGTH_DISORDERED_REGION}): {len(tf_proteins_curated_ds_IUPred3_diso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_over_token_limit = len(all_pairs[all_pairs['Length_arm'] + all_pairs['Length_tf'] > AF_TOKEN_LIMIT])\n",
    "print(f\"Pairs over token limit: {num_pairs_over_token_limit} ({(num_pairs_over_token_limit/len(all_pairs))*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pairs that are in structure dataset\n",
    "\n",
    "%store -r up_ids_structure_ds\n",
    "\n",
    "up_ids_structure_ds_sorted = []\n",
    "for up_pair in up_ids_structure_ds:\n",
    "    up_pair_clean = [x.upper() for x in up_pair if pd.notna(x)]\n",
    "    up_pair_clean = tuple(sorted(up_pair_clean))\n",
    "    up_ids_structure_ds_sorted.append(up_pair_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pair_id column to tuples for comparison\n",
    "all_pairs['pair_tuple'] = all_pairs['pair_id'].apply(eval)\n",
    "\n",
    "# Create a set of structure dataset pairs for faster lookup\n",
    "structure_pairs_set = set(up_ids_structure_ds_sorted)\n",
    "\n",
    "# Filter out pairs that are in the structure dataset\n",
    "initial_count = len(all_pairs)\n",
    "all_pairs = all_pairs[~all_pairs['pair_tuple'].isin(structure_pairs_set)]\n",
    "removed = initial_count - len(all_pairs)\n",
    "\n",
    "# Drop the temporary column\n",
    "all_pairs = all_pairs.drop('pair_tuple', axis=1)\n",
    "\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_STRING = len(all_pairs_w_STRING)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {num_all_pairs_w_STRING} ({(num_all_pairs_w_STRING/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_intact import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned = read_clean_intact('../../data/IntAct/human/human.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_cleaned, on='pair_id', how='inner')\n",
    "\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_IntAct = len(all_pairs_w_IntAct)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {num_all_pairs_w_IntAct} ({(num_all_pairs_w_IntAct/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_intersect_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "all_pairs_union_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='outer')\n",
    "\n",
    "\n",
    "print(f\"Intersection of STRING and IntAct: {len(all_pairs_intersect_STRING_IntAct)}\")\n",
    "print(f\"Union of STRING and IntAct: {len(all_pairs_union_STRING_IntAct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predicted dataset by removing pairs that have evidence in STRING or IntAct\n",
    "pred_ds = all_pairs[~all_pairs['pair_id'].isin(all_pairs_union_STRING_IntAct['pair_id'])]\n",
    "print(len(pred_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd617a",
   "metadata": {},
   "source": [
    "### write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in armadillo_proteins.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/arm_all_uniprot_rev_fasta', row['Reviewed'])\n",
    "# for idx, row in tf_proteins_curated_ds_IUPred3_diso.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/tf_all_uniprot_rev_fasta', row['Reviewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armadillo_proteins.to_csv('../../armadillo_proteins.csv', index=False)\n",
    "# tf_proteins_curated_ds_IUPred3_diso.to_csv('../../transcription_factors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f20999",
   "metadata": {},
   "source": [
    "## 1.1 PDB reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff831cb6",
   "metadata": {},
   "source": [
    "### imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "\n",
    "# Step 2: Re-import everything you need\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_report_arm_filter(pdb_report: pd.DataFrame, armadillo_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter entries that have at least one ARM, add column isARM = True|False\n",
    "    keep_pdbs = set()\n",
    "    armadillo_entries = armadillo_proteins['Entry'].tolist()\n",
    "    pdb_report['isARM'] = False\n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in armadillo_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isARM'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_tf_filter(pdb_report: pd.DataFrame, tf_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep_pdbs = set()\n",
    "    tf_entries = tf_proteins['Entry'].tolist()\n",
    "    pdb_report['isDisoTF'] = False\n",
    "    \n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in tf_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isDisoTF'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_disorder_filter(pdb_report: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter for entries that have at least one protein that is not ARm and has a disordered region\n",
    "    keep_pdbs = set()\n",
    "\n",
    "    for _, row in pdb_report.iterrows():\n",
    "        if row['isARM'] == False and row['num_disordered_regions'] > 0:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d065de",
   "metadata": {},
   "source": [
    "### report 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_1 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/1/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760715e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entry IDs from both datasets\n",
    "pdb_entry_ids = set(pdb_report_1['Accession Code(s)'].dropna())\n",
    "tf_entry_ids = set(tf_proteins_curated_ds_IUPred3_diso['Entry'])\n",
    "\n",
    "# Find intersection\n",
    "common_entries = pdb_entry_ids.intersection(tf_entry_ids)\n",
    "\n",
    "print(f\"Number of Entry IDs in pdb_report_1_two_seq: {len(pdb_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs in tf_proteins_curated_ds: {len(tf_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs that appear in both datasets: {len(common_entries)}\")\n",
    "print(f\"Percentage of PDB entries that are also TFs: {len(common_entries)/len(pdb_entry_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only entries where at least one Uniprot ID is in list of disordered TFs\n",
    "pdb_report_1 = pdb_report_tf_filter(pdb_report_1, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_1 = pdb_report_arm_filter(pdb_report_1, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "# pdb_report_1 = add_iupred3(pdb_report_1, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08220f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_1['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_1_seq2 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pairs where ARM and disordered TF is the same protein\n",
    "keep_pdbs = set()\n",
    "\n",
    "for ind, row in pdb_report_1.iterrows():\n",
    "    if not (row['isARM'] == True and row['isDisoTF'] == True):\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "\n",
    "pdb_report_1 = pdb_report_1[pdb_report_1['Entry ID'].isin(keep_pdbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "# pdb_report_1 = pdb_report_disorder_filter(pdb_report_1_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577268e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_1['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f620f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_appearing_3 = entry_counts[entry_counts == 3].index.tolist()\n",
    "pdb_report_1_seq3 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68903f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs by separating into arm and tf half\n",
    "report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "# report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True)]\n",
    "# report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False)]\n",
    "\n",
    "report_1_pairs = pd.merge(left=report_1_tf, right=report_1_arm, on='Entry ID', suffixes=['_tf', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_1_pairs = report_1_pairs.rename(columns={'Entry ID': 'pdb_id'})\n",
    "\n",
    "# print_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "\n",
    "report_1_pairs, no_model = append_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d904a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_1_pairs_sorted = report_1_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_1_pairs_sorted.iterrows():\n",
    "    pdb_id = row['pdb_id']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_tf'] + \" \" + row['Release Date_arm']  # Using tf release date\n",
    "    job_name = row.get('job_name', 'N/A')  # Use get() in case column doesn't exist\n",
    "\n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\\t\\t{job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabafded",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1_pairs = annotate_AF_metrics(report_1_pairs, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_1_pairs, 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ptm', 'dockq_score', ax=axes[1],corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2b91d",
   "metadata": {},
   "source": [
    "### report 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e16ba",
   "metadata": {},
   "source": [
    "one ARM and one disordered protein (see PDB report),\n",
    "only two proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_2 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/2/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f061688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entry IDs from both datasets\n",
    "pdb_entry_ids = set(pdb_report_2['Accession Code(s)'].dropna())\n",
    "tf_entry_ids = set(tf_proteins_curated_ds_IUPred3_diso['Entry'])\n",
    "\n",
    "# Find intersection\n",
    "common_entries = pdb_entry_ids.intersection(tf_entry_ids)\n",
    "\n",
    "print(f\"Number of Entry IDs in pdb_report_2: {len(pdb_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs in tf_proteins_curated_ds: {len(tf_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs that appear in both datasets: {len(common_entries)}\")\n",
    "print(f\"Percentage of PDB entries that are also TFs: {len(common_entries)/len(pdb_entry_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_2 = pdb_report_arm_filter(pdb_report_2, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "pdb_report_2 = add_iupred3(pdb_report_2, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some PDBs that have entries for seemingly non-existent chains. I don't know why. Therefore, I remove all rows that have an empty 'Sequence' field\n",
    "pdb_report_2 = pdb_report_2[pdb_report_2['Sequence'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice (redundant in this case)\n",
    "print(len(pdb_report_2))\n",
    "entry_counts = pdb_report_2['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_2_seq2 = pdb_report_2[pdb_report_2['Entry ID'].isin(entries_appearing_twice)]\n",
    "print(len(pdb_report_2_seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d383e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now filter for disordered regions\n",
    "pdb_report_2_seq2 = pdb_report_disorder_filter(pdb_report_2_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39830797",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_2_seq2['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_arm = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == True) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_2_diso = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == False) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "\n",
    "report_2_pairs = pd.merge(left=report_2_diso, right=report_2_arm, on='Entry ID', suffixes=['_diso', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_2_pairs.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "report_2_pairs, no_model = append_dockq_single_interface(report_2_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "report_2_pairs.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c04bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f65f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74224994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_2_pairs_sorted = report_2_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_2_pairs_sorted.iterrows():\n",
    "    pdb_id = row['Entry ID']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_diso']\n",
    "    length = len(row['Sequence_arm']) + len(row['Sequence_diso'])\n",
    "    \n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\\t\\t{length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs = annotate_AF_metrics(report_2_pairs, '/home/markus/MPI_local/HPC_results_full')\n",
    "report_2_pairs['in_training_set'] = report_2_pairs['Release Date_diso'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into directory for PDB2Net processing\n",
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = PDB2NET_PREFIX + 'report2/'\n",
    "report_2_pairs['file_path'] = report_2_pairs['Entry ID'].apply(lambda id: path_prefix + id.lower() + '.cif')\n",
    "\n",
    "report_2_pairs.drop_duplicates(subset=['file_path'], inplace=False)['file_path'].to_csv(PDB2NET_PREFIX + 'report2.csv', index=False)\n",
    "\n",
    "# download pdb structures for pdb2net\n",
    "download_pdb_structures(set(report_2_pairs['Entry ID'].tolist()), path_prefix, 'cif', '/home/markus/MPI_local/data/PDB', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a354137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "# define interface as having at least INTERFACE_MIN_ATOMS atoms within INTERFACE_MAX_DISTANCE A of each other \n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "INTERFACE_MAX_DISTANCE = 5 # higher not possible => change PDB2Net data\n",
    "# Check interface between TF chain and ARM chain\n",
    "report_2_pairs['chain_interface'] = False  # Initialize with False\n",
    "valid_rows = report_2_pairs[report_2_pairs['Entry ID'].notna() & report_2_pairs['Asym ID_diso'].notna() & report_2_pairs['Asym ID_arm'].notna()]\n",
    "\n",
    "for ind in valid_rows.index:\n",
    "    pdb_id = str(valid_rows.at[ind, 'Entry ID']).upper().strip()\n",
    "    chain_tf = valid_rows.at[ind, 'Asym ID_diso']\n",
    "    chains_arm = valid_rows.at[ind, 'Asym ID_arm']\n",
    "    \n",
    "    for chain_arm in chains_arm:\n",
    "        if check_interface(pdb_id, chain_tf, chain_arm, '/home/markus/MPI_local/data/PDB2Net/report2/2025-08-31_10-24-32', INTERFACE_MIN_ATOMS, INTERFACE_MAX_DISTANCE):\n",
    "            report_2_pairs.at[ind, 'chain_interface'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "report_2_pairs_interface = report_2_pairs[report_2_pairs['chain_interface'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_2_pairs, 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ptm', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('AF metrics vs DockQ score for interaction disordered-ARM (yellow = in training set)', fontsize = 18)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'iptm', 'dockq_score', 'in_training_set', ax=axes[0], corr=True)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'ptm', 'dockq_score', 'in_training_set', ax=axes[1], corr=True)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'ranking_score', 'dockq_score', 'in_training_set', ax=axes[2], corr=True)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.49]))\n",
    "print(len(report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('AF metrics vs DockQ score for interaction disordered-ARM (yellow = in training set)', fontsize = 18)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'ptm', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'ranking_score', 'dockq_score', ax=axes[2], corr=True)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e897a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs_interface_no_tranining = report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(report_2_pairs_interface_no_tranining[report_2_pairs_interface_no_tranining['dockq_score'] >= 0.49]))\n",
    "print(len(report_2_pairs_interface_no_tranining[report_2_pairs_interface_no_tranining['dockq_score'] >= 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(report_2_pairs_interface, report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.49], report_2_pairs_interface[report_2_pairs_interface['dockq_score'] < 0.49], 'ranking_score', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot_colour(report_2_pairs, 'iptm', 'dockq_score', 'chain_interface', ax=axes[0])\n",
    "create_scatter_plot_colour(report_2_pairs, 'ptm', 'dockq_score', 'chain_interface', ax=axes[1])\n",
    "create_scatter_plot_colour(report_2_pairs, 'ranking_score', 'dockq_score', 'chain_interface', ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_2_pairs[report_2_pairs['in_training_set'] == True], 'ranking_score', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs[report_2_pairs['in_training_set'] == False], 'ranking_score', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40735318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=report_2_pairs[report_2_pairs['in_training_set'] == False],\n",
    "    dataset1_name='report 2 not in training', \n",
    "    dataset2=report_2_pairs[report_2_pairs['in_training_set'] == True],\n",
    "    dataset2_name='report 2 in training',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_double_violin_plot(\n",
    "    dataset1=report_2_pairs[report_2_pairs['in_training_set'] == False],\n",
    "    dataset1_name='not in training',\n",
    "    metric1='dockq_score',\n",
    "    dataset2=report_2_pairs[report_2_pairs['in_training_set'] == True],\n",
    "    dataset2_name='in training',\n",
    "    metric2='dockq_score'\n",
    ")\n",
    "fig.suptitle('Disordered-ARM: dockq score', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_2_pairs['Entry ID'].to_csv('report_2_entry_ids.csv', index=False)\n",
    "download_pdb_structures(set(report_2_pairs['Entry ID'].tolist()), '/home/markus/PDB2Net/in/rep2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14e271",
   "metadata": {},
   "source": [
    "### report 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_3 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/3/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_3 = pdb_report_arm_filter(pdb_report_3, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "pdb_report_3 = add_iupred3(pdb_report_3, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_3['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_3_seq2 = pdb_report_3[pdb_report_3['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "pdb_report_3_seq2 = pdb_report_disorder_filter(pdb_report_3_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88289bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_3_seq2['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec179587",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3_arm = pdb_report_3_seq2[(pdb_report_3_seq2['isARM'] == True) & (pdb_report_3_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_3_diso = pdb_report_3_seq2[(pdb_report_3_seq2['isARM'] == False) & (pdb_report_3_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "\n",
    "report_3_pairs = pd.merge(left=report_3_diso, right=report_3_arm, on='Entry ID', suffixes=['_diso', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_3_pairs.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "report_3_pairs, no_model = append_dockq(report_3_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "report_3_pairs.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_3_pairs_sorted = report_3_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_3_pairs_sorted.iterrows():\n",
    "    pdb_id = row['Entry ID']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_diso']\n",
    "    \n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f166453",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3_pairs = annotate_AF_metrics(report_3_pairs, '/home/markus/MPI_local/HPC_results_full')\n",
    "report_3_pairs['in_training_set'] = report_3_pairs['Release Date_diso'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71de96",
   "metadata": {},
   "source": [
    "### job creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "# IMPORTANT: check for duplicates that were modelled in production1! They may need to be downloaded extra\n",
    "\n",
    "\n",
    "BATCH_DIRS = []\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "new_af_jobs = create_job_batch_from_PDB_IDs(list(set(pdb_report_2_seq2['Entry ID'].tolist())), BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98359aaa",
   "metadata": {},
   "source": [
    "## 1.2 Hadeer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83082c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadeer_df = pd.read_csv('/home/markus/MPI_local/downloads/transcription_factors_pdbs.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba571061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadeer_df.drop(columns=['Protein.names', 'Gene.Names', 'Organism', 'Length', 'InterPro', 'Pfam', 'DisProt', 'STRING', 'IntAct', 'Ensembl', 'Repeat', 'HGNC'])\n",
    "hadeer_df.rename(columns={'chain': 'chain_tf', 'Entry': 'Entry_tf'}, inplace=True)\n",
    "\n",
    "# add for each pdb the other chains and other pdbs\n",
    "# find out which chain is ARM\n",
    "# check interaction between chain_tf and chain_arm\n",
    "\n",
    "hadeer_df['chains_arm'] = None\n",
    "hadeer_df['Entrys_arm'] = None\n",
    "\n",
    "for ind,row in hadeer_df.iterrows():\n",
    "\n",
    "    pdb_id = row['pdb']\n",
    "    if pd.isna(pdb_id):\n",
    "        continue\n",
    "\n",
    "    # normalize pdb id and ensure chain is present\n",
    "    pdb_id = str(pdb_id).upper().strip()\n",
    "    chain_tf = row['chain_tf']\n",
    "    if pd.isna(chain_tf):\n",
    "        continue\n",
    "\n",
    "    all_pdbs = get_pdb_chains_to_uniprot(pdb_id, '../../production1/mapping_cache')\n",
    "    \n",
    "    arm_pairs = []\n",
    "    \n",
    "    for chain, id in all_pdbs.items():\n",
    "        if id in armadillo_proteins['Entry'].tolist():\n",
    "            arm_pairs.append((chain, id))\n",
    "            \n",
    "    \n",
    "    if len(arm_pairs) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        hadeer_df.at[ind, 'chains_arm'] = [pair[0] for pair in arm_pairs]\n",
    "        hadeer_df.at[ind, 'Entrys_arm'] = [pair[1] for pair in arm_pairs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define interface as having at least n atoms within 5 A of each other \n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "# Check interface between TF chain and ARM chain\n",
    "hadeer_df['arm_tf_interface'] = False  # Initialize with False\n",
    "valid_rows = hadeer_df[hadeer_df['pdb'].notna() & hadeer_df['chain_tf'].notna() & hadeer_df['chains_arm'].notna()]\n",
    "\n",
    "for ind in valid_rows.index:\n",
    "    pdb_id = str(valid_rows.at[ind, 'pdb']).upper().strip()\n",
    "    chain_tf = valid_rows.at[ind, 'chain_tf']\n",
    "    chains_arm = valid_rows.at[ind, 'chains_arm']\n",
    "    \n",
    "    for chain_arm in chains_arm:\n",
    "        if check_interface(pdb_id, chain_tf, chain_arm, '/home/markus/MPI_local/data/PDB2Net', INTERFACE_MIN_ATOMS):\n",
    "            hadeer_df.at[ind, 'arm_tf_interface'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique()))\n",
    "display(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e803956",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "new_af_jobs = create_job_batch_from_PDB_IDs(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique().tolist(), BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### create job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'\n",
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "### STRING\n",
    "# note that the order is important. The category (100,200) is very large so it comes last to fill up the remaining jobs\n",
    "# categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (100,200), (0,100), (200,300)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS, STRING_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### IntAct\n",
    "# categories = [(0.1,0.2), (0.9,1), (0.8,0.9), (0.7,0.8), (0.6,0.7), (0.5,0.6), (0.4,0.5), (0.2,0.3), (0.3,0.4)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_IntAct, BATCH_SIZE, categories, BATCH_DIRS, INTACT_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### all pairs\n",
    "# new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "### ID list:\n",
    "id_list_good = [\n",
    "    (\"Q13285\", \"A0A2R8YCH5\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"Q9H3D4\", \"A0A8I5KU01\"),\n",
    "    (\"Q8NHM5\", \"A1YPR0\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q6SJ96\", \"O14981\"),\n",
    "    (\"Q9NRY4\", \"O00750\"),\n",
    "    (\"Q6ZRS2\", \"A0A8V8TQN3\")\n",
    "]\n",
    "\n",
    "# id_list_complex = [\n",
    "#     (\"Q03181\", \"Q9H3U1\"),\n",
    "#     (\"P04637\", \"A0A994J4J0\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"Q9UBG7\", \"A0A8I5KU01\"),\n",
    "#     (\"P19838\", \"A0A1W2PRG6\")\n",
    "# ]\n",
    "\n",
    "# missing = [('Q13285', 'Q6BTZ4'), ('P04637', 'Q9VL06'), ('P04637', 'Q9VL06'), ('Q9UIF8', 'Q54U63'), ('Q15047', 'Q54U63'), ('Q15047', 'Q5R881'), ('Q9H3D4', 'Q9VL06'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('Q6ZRS2', 'P38811'), ('Q6ZRS2', 'P38811')]\n",
    "# new_af_jobs = create_job_batch_id_list(all_pairs, missing, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/AF_job_batches/batch_56')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "\n",
    "# Step 2: Re-import everything you need\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a644",
   "metadata": {},
   "source": [
    "### Negatomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a83e",
   "metadata": {},
   "source": [
    "#### IntAct Negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intact_negative = pd.read_csv('../../data/IntAct/human/human_negative.txt', sep='\\t')\n",
    "# intact_negative.drop(['Alias(es) interactor A', \n",
    "#                      'Alias(es) interactor B', \n",
    "#                      'Interaction detection method(s)',\n",
    "#                      'Publication 1st author(s)',\n",
    "#                      'Publication Identifier(s)',\n",
    "#                      'Taxid interactor A',\n",
    "#                      'Taxid interactor B',\n",
    "#                      'Biological role(s) interactor A',\n",
    "#                      'Biological role(s) interactor B',\n",
    "#                      'Experimental role(s) interactor A',\n",
    "#                      'Experimental role(s) interactor B',\n",
    "#                      'Type(s) interactor A',\n",
    "#                      'Type(s) interactor B',\n",
    "#                      'Xref(s) interactor A',\n",
    "#                      'Xref(s) interactor B',\n",
    "#                      'Interaction Xref(s)',\n",
    "#                      'Annotation(s) interactor A',\n",
    "#                      'Annotation(s) interactor B',\n",
    "#                      'Interaction annotation(s)',\n",
    "#                      'Host organism(s)',\n",
    "#                      'Interaction parameter(s)',\n",
    "#                      'Creation date',\n",
    "#                      'Update date',\n",
    "#                      'Checksum(s) interactor A',\n",
    "#                      'Checksum(s) interactor B',\n",
    "#                      'Interaction Checksum(s)',\n",
    "#                      'Feature(s) interactor A',\n",
    "#                      'Feature(s) interactor B',\n",
    "#                      'Stoichiometry(s) interactor A',\n",
    "#                      'Stoichiometry(s) interactor B',\n",
    "#                      'Identification method participant A',\n",
    "#                      'Identification method participant B',\n",
    "#                      'Expansion method(s)'\n",
    "#                      ], axis=1, inplace=True)\n",
    "# intact_negative.loc[:, 'intact_score'] = intact_negative.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "# intact_negative['pair_id'] = intact_negative.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', '').split('-')[0], row['ID(s) interactor B'].replace('uniprotkb:', '').split('-')[0]]))), axis=1)\n",
    "# intact_negative = intact_negative.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "# all_pairs_intact_negative = pd.merge(all_pairs, intact_negative, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_intact_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4c239",
   "metadata": {},
   "source": [
    "#### Blohm negatome2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatome2 = pd.read_csv('../../data/negatome2.0/combined.txt', sep='\\t', names=['ID_1', 'ID_2'])\n",
    "# negatome2['pair_id'] = negatome2.apply(lambda row: str(tuple(sorted([row['ID_1'].split('-')[0], row['ID_2'].split('-')[0]]))), axis=1)\n",
    "# all_pairs_negatome2 = pd.merge(all_pairs, negatome2, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_negatome2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63893e",
   "metadata": {},
   "source": [
    "#### Stelzl 2005 negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stelzl_neg = pd.read_csv('../../data/16169070_neg.mitab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Read in HPC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df_uc = pd.DataFrame(data=find_summary_files([HPC_RESULT_DIR]))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df_uc)}\")\n",
    "\n",
    "results_df_uc['pair_id'] = results_df_uc.apply(create_pair_id, axis=1)\n",
    "\n",
    "print(f\"jobs before cleaning: {len(results_df_uc)}\")\n",
    "results_df = clean_results(results_df_uc)\n",
    "print(f\"jobs after cleaning: {len(results_df)}\")\n",
    "\n",
    "results_df['interface_pae_max'] = results_df['chain_pair_pae_min'].apply(lambda c: max([c[0][1], c[1][0]]))\n",
    "results_df['interface_pae_min'] = results_df['chain_pair_pae_min'].apply(lambda c: min([c[0][1], c[1][0]]))\n",
    "results_df['chain_pair_iptm_max'] = results_df['chain_pair_iptm'].apply(lambda c: max([c[0][0], c[1][1]]))\n",
    "results_df['chain_pair_iptm_min'] = results_df['chain_pair_iptm'].apply(lambda c: min([c[0][0], c[1][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, intact_cleaned, on='pair_id', how='left')\n",
    "\n",
    "# Add IUPred data for TF proteins\n",
    "tf_iupred_cols = ['Entry', 'num_disordered_regions', 'iupred3', 'disordered_regions_mask']\n",
    "tf_iupred_data = tf_proteins_curated_ds_IUPred3_diso[tf_iupred_cols].add_suffix('_tf')\n",
    "\n",
    "arm_cols = ['Entry']\n",
    "arm_data = armadillo_proteins[arm_cols].add_suffix('_arm')\n",
    "\n",
    "results_df_annotated['tf_id'] = results_df_annotated['job_name'].apply(lambda x: x.split('_')[2].upper())\n",
    "results_df_annotated['arm_id'] = results_df_annotated['job_name'].apply(lambda x: x.split('_')[0].upper())\n",
    "\n",
    "results_df_annotated = pd.merge(results_df_annotated, tf_iupred_data, left_on='tf_id', right_on='Entry_tf', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, arm_data, left_on='arm_id', right_on='Entry_arm', how='left')\n",
    "\n",
    "# remove rows where tf or arm not in dataset\n",
    "results_df_annotated = results_df_annotated[results_df_annotated['Entry_tf'].notna() & results_df_annotated['Entry_arm'].notna()]\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total number of modelled pairs: {len(results_df)}\")\n",
    "print(f\"Total rows in merged_df: {len(results_df_annotated)}\")\n",
    "print(f\"Rows with annotated data (STRING): {results_df_annotated['combined_score'].notna().sum()}\")\n",
    "print(f\"Rows with annotated data (IntAct): {results_df_annotated['intact_score'].notna().sum()}\")\n",
    "\n",
    "\n",
    "# convert all STRING scores from 0-1000 to 0-1 (linear conversion)\n",
    "STRING_COLS = ['experimental', 'database', 'textmining', 'combined_score']\n",
    "for col in STRING_COLS:\n",
    "    results_df_annotated[col] = results_df_annotated[col] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_df_annotated[results_df_annotated['ranking_score'] < 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c008bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = '/home/markus/MPI_local/HPC_results/'\n",
    "results_df_annotated['model_path'] = results_df_annotated['job_name'].apply(lambda jn: path_prefix + jn.lower() + '_model.cif')\n",
    "results_df_annotated['model_path'].to_csv(PDB2NET_PREFIX + 'pipeline1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_pdb2net\n",
    "\n",
    "importlib.reload(functions_pdb2net)\n",
    "\n",
    "from functions_pdb2net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a617c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated_1 = results_df_annotated[results_df_annotated['job_name'] == \"O60287_1-2271_P11161_1-476\".lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d644d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = annotate_interface_tf(results_df_annotated, \"/home/markus/MPI_local/data/PDB2Net/pipeline1\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60366514",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RESIDUES_MIN_OVERLAP = 5\n",
    "results_df_annotated['diso_mediating'] = False\n",
    "for ind, row in results_df_annotated.iterrows():\n",
    "\ttry:\n",
    "\t\tresults_df_annotated.at[ind, 'diso_mediating'] = check_overlap_intf_diso(row['interface_tf'], row['disordered_regions_mask_tf'], NUM_RESIDUES_MIN_OVERLAP)\n",
    "\texcept:\n",
    "\t\tprint(\"----\")\n",
    "\t\tprint(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_struct = pd.DataFrame()\n",
    "%store -r up_ids_structure_ds\n",
    "\n",
    "# remove pairs in structure dataset\n",
    "up_ids_structure_ds_sorted = []\n",
    "for up_pair in up_ids_structure_ds:\n",
    "    up_pair_clean = [x.upper() for x in up_pair if pd.notna(x)]\n",
    "    up_pair_clean = tuple(sorted(up_pair_clean))\n",
    "    up_ids_structure_ds_sorted.append(up_pair_clean)\n",
    "\n",
    "for _, row in results_df_annotated.iterrows():\n",
    "    pair_tuple = eval(row['pair_id'])\n",
    "    if pair_tuple not in up_ids_structure_ds_sorted:\n",
    "        no_struct = pd.concat([no_struct, row.to_frame().T], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"removed {row['pair_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'interaction dataset' with modeled structures\n",
    "int_ds_results = no_struct[no_struct['combined_score'].notna() | no_struct['intact_score'].notna()]\n",
    "\n",
    "print(len(int_ds_results))\n",
    "print(len(int_ds_results[int_ds_results['diso_mediating'] == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_results_only_IntAct = int_ds_results[(int_ds_results[INTACT_SCORE_COLUMN].notna()) & ~(int_ds_results[STRING_SCORE_COLUMN].notna())]\n",
    "int_ds_results_only_STRING = int_ds_results[~(int_ds_results[INTACT_SCORE_COLUMN].notna()) & (int_ds_results[STRING_SCORE_COLUMN].notna())]\n",
    "int_ds_results_STRING_and_INTACT = int_ds_results[(int_ds_results[INTACT_SCORE_COLUMN].notna()) & (int_ds_results[STRING_SCORE_COLUMN].notna())]\n",
    "print(f\"Length of int_ds_results_only_IntAct: {len(int_ds_results_only_IntAct)}\")\n",
    "print(f\"Length of int_ds_results_only_STRING: {len(int_ds_results_only_STRING)}\")\n",
    "print(f\"Length of int_ds_results_STRING_and_INTACT: {len(int_ds_results_STRING_and_INTACT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for the three interaction datasets\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Filter out values below 0 for each dataset by setting them to 0\n",
    "int_ds_results_only_IntAct_adjusted = int_ds_results_only_IntAct.copy()\n",
    "int_ds_results_only_IntAct_adjusted.loc[int_ds_results_only_IntAct_adjusted['ranking_score'] < 0, 'ranking_score'] = 0\n",
    "\n",
    "int_ds_results_only_STRING_adjusted = int_ds_results_only_STRING.copy()\n",
    "int_ds_results_only_STRING_adjusted.loc[int_ds_results_only_STRING_adjusted['ranking_score'] < 0, 'ranking_score'] = 0\n",
    "\n",
    "int_ds_results_STRING_and_INTACT_adjusted = int_ds_results_STRING_and_INTACT.copy()\n",
    "int_ds_results_STRING_and_INTACT_adjusted.loc[int_ds_results_STRING_and_INTACT_adjusted['ranking_score'] < 0, 'ranking_score'] = 0\n",
    "\n",
    "# Prepare data for boxplot\n",
    "data_to_plot = [\n",
    "    int_ds_results_only_IntAct_adjusted['ranking_score'],\n",
    "    int_ds_results_only_STRING_adjusted['ranking_score'],\n",
    "    int_ds_results_STRING_and_INTACT_adjusted['ranking_score']\n",
    "]\n",
    "\n",
    "labels = ['IntAct only', 'STRING only', 'STRING ∩ IntAct']\n",
    "\n",
    "# Create boxplot with seaborn deep palette\n",
    "colors = sns.color_palette(\"deep\", 3)\n",
    "box_plot = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "\n",
    "# Apply colors\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Ranking score')\n",
    "ax.set_title('Distribution of ranking scores [0,1.5] for different interaction sources')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0,1.6)\n",
    "\n",
    "\n",
    "# Calculate medians\n",
    "medians = [data.median() for data in data_to_plot]\n",
    "\n",
    "# Add sample sizes and medians to the plot\n",
    "for i, (data, median) in enumerate(zip(data_to_plot, medians)):\n",
    "    ax.text(i+1, 1.3, f'n={len(data)}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "    ax.text(i+1, 1.25, f'median: {median:.3f}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/interact_box1.png\",dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "print(f\"IntAct only: n={len(int_ds_results_only_IntAct_adjusted)}, median={int_ds_results_only_IntAct_adjusted['ranking_score'].median():.3f}\")\n",
    "print(f\"STRING only: n={len(int_ds_results_only_STRING_adjusted)}, median={int_ds_results_only_STRING_adjusted['ranking_score'].median():.3f}\")\n",
    "print(f\"STRING & IntAct: n={len(int_ds_results_STRING_and_INTACT_adjusted)}, median={int_ds_results_STRING_and_INTACT_adjusted['ranking_score'].median():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43cc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered boxplots with score thresholds\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Apply score filters\n",
    "int_ds_results_only_IntAct_high = int_ds_results_only_IntAct_adjusted[\n",
    "    (int_ds_results_only_IntAct[INTACT_SCORE_COLUMN] > 0.4)\n",
    "]\n",
    "\n",
    "int_ds_results_only_STRING_high = int_ds_results_only_STRING_adjusted[\n",
    "    (int_ds_results_only_STRING[STRING_SCORE_COLUMN] > 0.4)\n",
    "]\n",
    "\n",
    "int_ds_results_both_high = int_ds_results_STRING_and_INTACT_adjusted[\n",
    "    (int_ds_results_STRING_and_INTACT[INTACT_SCORE_COLUMN] > 0.4) & \n",
    "    (int_ds_results_STRING_and_INTACT[STRING_SCORE_COLUMN] > 0.4)\n",
    "]\n",
    "\n",
    "int_ds_results_either_high = int_ds_results[\n",
    "    ((int_ds_results[INTACT_SCORE_COLUMN] > 0.4) | \n",
    "     (int_ds_results[STRING_SCORE_COLUMN] > 0.4)) &\n",
    "    (int_ds_results['ranking_score'] >= 0)\n",
    "]\n",
    "\n",
    "# Prepare data for boxplot\n",
    "data_to_plot_filtered = [\n",
    "    int_ds_results_only_IntAct_high['ranking_score'],\n",
    "    int_ds_results_only_STRING_high['ranking_score'],\n",
    "    # int_ds_results_both_high['ranking_score'],\n",
    "    int_ds_results_either_high['ranking_score']\n",
    "]\n",
    "\n",
    "labels_filtered = ['IntAct only\\n(>0.4)', 'STRING only\\n(>0.4)', '(STRING > 0.4) | (IntAct > 0.4)']\n",
    "\n",
    "# Create boxplot with seaborn deep palette\n",
    "colors_filtered = sns.color_palette(\"deep\")\n",
    "# Make the third (rightmost) color purple\n",
    "colors_filtered = [colors_filtered[0], colors_filtered[1], colors_filtered[5]]  # Use index 3 for purple\n",
    "box_plot_filtered = ax.boxplot(data_to_plot_filtered, labels=labels_filtered, patch_artist=True)\n",
    "\n",
    "# Apply colors\n",
    "for patch, color in zip(box_plot_filtered['boxes'], colors_filtered):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Ranking score')\n",
    "ax.set_title('Distribution of ranking scores [0,1.5] for high-confidence interactions')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(False,1.6)\n",
    "\n",
    "# Calculate medians\n",
    "medians_filtered = [data.median() for data in data_to_plot_filtered]\n",
    "\n",
    "# Add sample sizes and medians to the plot\n",
    "for i, (data, median) in enumerate(zip(data_to_plot_filtered, medians_filtered)):\n",
    "    ax.text(i+1, 1.3, f'n={len(data)}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "    ax.text(i+1, 1.25, f'median: {median:.3f}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/interact_box2.png\",dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for filtered datasets\n",
    "print(\"Summary statistics (filtered >0.4):\")\n",
    "print(f\"IntAct only: n={len(int_ds_results_only_IntAct_high)}, median={int_ds_results_only_IntAct_high['ranking_score'].median():.3f}\")\n",
    "print(f\"STRING only: n={len(int_ds_results_only_STRING_high)}, median={int_ds_results_only_STRING_high['ranking_score'].median():.3f}\")\n",
    "print(f\"Both >0.4: n={len(int_ds_results_both_high)}, median={int_ds_results_both_high['ranking_score'].median():.3f}\")\n",
    "print(f\"Either >0.4: n={len(int_ds_results_either_high)}, median={int_ds_results_either_high['ranking_score'].median():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c0f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CUTOFF = 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "int_ds_results_plot = int_ds_results[int_ds_results['ranking_score'] >= 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.1\n",
    "bins = np.arange(0, max(int_ds_results_plot['ranking_score']) + 0.025, 0.01)\n",
    "hist, bin_edges = np.histogram(int_ds_results_plot['ranking_score'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] >= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Ranking score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of ranking scores [0,1.5] in interaction dataset (STRING ∪ IntAct)')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = int_ds_results['ranking_score'].median()\n",
    "n_samples = len(int_ds_results)\n",
    "n_above_cutoff = len(int_ds_results[int_ds_results['ranking_score'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(int_ds_results[(int_ds_results['ranking_score'] < METRIC_CUTOFF) & (int_ds_results['ranking_score'] >= 0)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.4f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/interact_hist.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_results_exp_co = int_ds_results[(int_ds_results[STRING_SCORE_COLUMN] > 0.4) |  (int_ds_results[INTACT_SCORE_COLUMN] > 0.4)]\n",
    "\n",
    "int_ds_results_exp_co_plot = int_ds_results_exp_co[int_ds_results_exp_co['ranking_score'] >= 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.025\n",
    "bins = np.arange(0, max(int_ds_results_exp_co_plot['ranking_score']) + 0.025, 0.01)\n",
    "hist, bin_edges = np.histogram(int_ds_results_exp_co_plot['ranking_score'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] >= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Ranking score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of ranking scores [0,1.5] in filtered interaction dataset')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = int_ds_results_exp_co['ranking_score'].median()\n",
    "n_samples = len(int_ds_results_exp_co)\n",
    "n_above_cutoff = len(int_ds_results_exp_co[int_ds_results_exp_co['ranking_score'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(int_ds_results_exp_co[(int_ds_results_exp_co['ranking_score'] < METRIC_CUTOFF) & (int_ds_results_exp_co['ranking_score'] >= 0)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.2f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/interact_hist_exp_cutoff\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'prediction dataset' with modeled structures\n",
    "pred_ds_results = no_struct[~no_struct['combined_score'].notna()] # any one of the STRING scores is good for this filter\n",
    "pred_ds_results = pred_ds_results[~pred_ds_results['intact_score'].notna()]\n",
    "\n",
    "print(len(pred_ds))\n",
    "print(len(pred_ds_results))\n",
    "print(len(pred_ds_results[pred_ds_results['diso_mediating'] == True]))\n",
    "pred_ds_results_diso_med = pred_ds_results[pred_ds_results['diso_mediating'] == True]\n",
    "# pred_ds_results = pred_ds_results[pred_ds_results['diso_mediating'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084dddf",
   "metadata": {},
   "source": [
    "#### prediciton dataset plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "pred_ds_results_plot = pred_ds_results[(pred_ds_results['ranking_score'] >= 0)]\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.025\n",
    "bins = np.arange(0, max(pred_ds_results_plot['ranking_score']) + 0.025, 0.025)\n",
    "hist, bin_edges = np.histogram(pred_ds_results_plot['ranking_score'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] >= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Ranking score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = pred_ds_results['ranking_score'].median()\n",
    "n_samples = len(pred_ds_results)\n",
    "n_above_cutoff = len(pred_ds_results[pred_ds_results['ranking_score'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(pred_ds_results[(pred_ds_results['ranking_score'] < METRIC_CUTOFF)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.4f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/predict_hist.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CUTOFF = 0.63\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "pred_ds_results_diso_med_plot = pred_ds_results_diso_med.copy()\n",
    "pred_ds_results_diso_med_plot.loc[pred_ds_results_diso_med_plot['ranking_score'] < 0, 'ranking_score'] = 0\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.025\n",
    "bins = np.arange(0, 1, 0.01)\n",
    "hist, bin_edges = np.histogram(pred_ds_results_diso_med_plot['ranking_score'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] >= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Ranking score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = pred_ds_results_diso_med['ranking_score'].median()\n",
    "n_samples = len(pred_ds_results_diso_med)\n",
    "n_above_cutoff = len(pred_ds_results_diso_med[pred_ds_results_diso_med['ranking_score'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(pred_ds_results_diso_med[(pred_ds_results_diso_med['ranking_score'] < METRIC_CUTOFF)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.4f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/predict_hist_interface_rs.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CUTOFF = 3.9\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.025\n",
    "bins = np.arange(0, 30, 0.1)\n",
    "hist, bin_edges = np.histogram(pred_ds_results_diso_med_plot['interface_pae_min'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] <= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Interface PAE min')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = pred_ds_results_diso_med['interface_pae_min'].median()\n",
    "n_samples = len(pred_ds_results_diso_med)\n",
    "n_above_cutoff = len(pred_ds_results_diso_med[pred_ds_results_diso_med['interface_pae_min'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(pred_ds_results_diso_med[(pred_ds_results_diso_med['interface_pae_min'] < METRIC_CUTOFF)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.4f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/predict_hist_interface_ipaemin.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface_pae_max\n",
    "\n",
    "METRIC_CUTOFF = 3.6\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a histogram with custom colors and fixed bin size of 0.025\n",
    "bins = np.arange(0, 30, 0.1)\n",
    "hist, bin_edges = np.histogram(pred_ds_results_diso_med_plot['interface_pae_max'], bins=bins)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "for i in range(len(hist)):\n",
    "    color = 'green' if bin_centers[i] <= METRIC_CUTOFF else 'red'\n",
    "    plt.bar(bin_centers[i], hist[i], width=(bin_edges[1] - bin_edges[0]), \n",
    "            color=color, alpha=0.7, align='center')\n",
    "\n",
    "plt.xlabel('Interface PAE max')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add median and sample size to legend with cutoff information\n",
    "median_value = pred_ds_results_diso_med['interface_pae_max'].median()\n",
    "n_samples = len(pred_ds_results_diso_med)\n",
    "n_above_cutoff = len(pred_ds_results_diso_med[pred_ds_results_diso_med['interface_pae_max'] >= METRIC_CUTOFF])\n",
    "n_below_cutoff = len(pred_ds_results_diso_med[(pred_ds_results_diso_med['interface_pae_max'] < METRIC_CUTOFF)])\n",
    "\n",
    "# Calculate percentages\n",
    "pct_above = (n_above_cutoff / n_samples) * 100\n",
    "pct_below = (n_below_cutoff / n_samples) * 100\n",
    "\n",
    "plt.axvline(x=median_value, color='blue', linestyle='--', label=f'Median: {median_value:.4f}')\n",
    "plt.axvline(x=METRIC_CUTOFF, color='black', linestyle='-', alpha=0.5, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "plt.legend(title=f'n = {n_samples}\\n≥ {METRIC_CUTOFF}: {pct_above:.1f}% ({n_above_cutoff})\\n< {METRIC_CUTOFF}: {pct_below:.1f}% ({n_below_cutoff})')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/predict_hist_interface_ipaemax.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ds_results_diso_med_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define adjustable cutoffs for the three metrics\n",
    "RANKING_SCORE_CUTOFF = 0.63  # >= 0.6 is good\n",
    "INTERFACE_PAE_MIN_CUTOFF = 3.9  # <= 5.7 is good  \n",
    "INTERFACE_PAE_MAX_CUTOFF = 3.6  # <= 7.5 is good\n",
    "\n",
    "# Calculate how many criteria each row meets\n",
    "def count_criteria_met(row):\n",
    "    criteria_met = 0\n",
    "    \n",
    "    # Check ranking score criterion\n",
    "    if row['ranking_score'] >= RANKING_SCORE_CUTOFF:\n",
    "        criteria_met += 1\n",
    "    \n",
    "    # Check interface PAE min criterion\n",
    "    if row['interface_pae_min'] <= INTERFACE_PAE_MIN_CUTOFF:\n",
    "        criteria_met += 1\n",
    "        \n",
    "    # Check interface PAE max criterion  \n",
    "    if row['interface_pae_max'] <= INTERFACE_PAE_MAX_CUTOFF:\n",
    "        criteria_met += 1\n",
    "        \n",
    "    return criteria_met\n",
    "\n",
    "# Apply the function to count criteria met for each row\n",
    "pred_ds_results_diso_med_plot['criteria_met'] = pred_ds_results_diso_med_plot.apply(count_criteria_met, axis=1)\n",
    "\n",
    "# Count how many rows meet 0, 1, 2, or 3 criteria\n",
    "criteria_counts = pred_ds_results_diso_med_plot['criteria_met'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create histogram\n",
    "bars = plt.bar(criteria_counts.index, criteria_counts.values, \n",
    "               color=['red', 'orange', 'lightgreen', 'darkgreen'], \n",
    "               alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, count in zip(bars, criteria_counts.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(criteria_counts.values)*0.01,\n",
    "             f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Number of Cutoff Criteria Met')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.title(f'Distribution of Cutoff Criteria Met\\n(RS≥{RANKING_SCORE_CUTOFF}, iPAE_min≤{INTERFACE_PAE_MIN_CUTOFF}, iPAE_max≤{INTERFACE_PAE_MAX_CUTOFF})')\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "legend_labels = [\n",
    "    f'0 criteria (n={criteria_counts.get(0, 0)})',\n",
    "    f'1 criterion (n={criteria_counts.get(1, 0)})', \n",
    "    f'2 criteria (n={criteria_counts.get(2, 0)})',\n",
    "    f'3 criteria (n={criteria_counts.get(3, 0)})'\n",
    "]\n",
    "plt.legend(bars, legend_labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/predict_criteria_met.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "total_rows = len(pred_ds_results_diso_med_plot)\n",
    "print(\"Summary of criteria met:\")\n",
    "for i in range(4):\n",
    "    count = criteria_counts.get(i, 0)\n",
    "    percentage = (count / total_rows) * 100 if total_rows > 0 else 0\n",
    "    print(f\"{i} criteria: {count} rows ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows: {total_rows}\")\n",
    "print(f\"Rows meeting at least 2 criteria: {criteria_counts.get(2, 0) + criteria_counts.get(3, 0)} ({((criteria_counts.get(2, 0) + criteria_counts.get(3, 0)) / total_rows * 100):.1f}%)\")\n",
    "print(f\"Rows meeting all 3 criteria: {criteria_counts.get(3, 0)} ({(criteria_counts.get(3, 0) / total_rows * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f690bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows that meet all 3 criteria\n",
    "high_confidence_predictions = pred_ds_results_diso_med_plot[pred_ds_results_diso_med_plot['criteria_met'] == 3]\n",
    "\n",
    "print(f\"Rows meeting all 3 criteria (n={len(high_confidence_predictions)}):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Print with relevant columns\n",
    "columns_to_show = ['tf_id', 'arm_id', 'ranking_score', 'interface_pae_min', 'interface_pae_max', 'iptm', 'ptm']\n",
    "\n",
    "for idx, row in high_confidence_predictions.iterrows():\n",
    "    print(f\"{row['tf_id']}-{row['arm_id']}\")\n",
    "    # print(f\"TF: {row['tf_id']}, ARM: {row['arm_id']}\")\n",
    "    # print(f\"  Ranking Score: {row['ranking_score']:.3f}\")\n",
    "    # print(f\"  Interface PAE min: {row['interface_pae_min']:.2f}\")\n",
    "    # print(f\"  Interface PAE max: {row['interface_pae_max']:.2f}\")\n",
    "    # print(f\"  ipTM: {row['iptm']:.3f}, pTM: {row['ptm']:.3f}\")\n",
    "    # print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort prediction dataset by ranking score (ascending) and print pair ID and ranking score\n",
    "pred_ds_results_sorted = pred_ds_results.sort_values('ranking_score', ascending=False)\n",
    "\n",
    "print(\"Pair ID\\t\\t\\t\\tRanking Score\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx, row in pred_ds_results_sorted.head(10).iterrows():\n",
    "    pair_id = row['job_name']\n",
    "    ranking_score = row['ranking_score'] if pd.notna(row['ranking_score']) else 'N/A'\n",
    "    \n",
    "    print(f\"{pair_id}\\t\\t{ranking_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot comparison between pred_ds_results and pred_ds_results_diso_med\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Filter out negative ranking scores for both datasets\n",
    "pred_ds_results_filtered = pred_ds_results[pred_ds_results['ranking_score'] >= 0]\n",
    "pred_ds_results_diso_med_filtered = pred_ds_results_diso_med[pred_ds_results_diso_med['ranking_score'] >= 0]\n",
    "\n",
    "# Prepare data for boxplot\n",
    "data_to_plot = [\n",
    "    pred_ds_results_filtered['ranking_score'],\n",
    "    pred_ds_results_diso_med_filtered['ranking_score']\n",
    "]\n",
    "\n",
    "labels = ['All predictions', 'Disorder-mediated']\n",
    "\n",
    "# Create boxplot with seaborn deep palette (blue and orange)\n",
    "colors = sns.color_palette(\"deep\", 2)  # Gets blue and orange from deep palette\n",
    "box_plot = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "\n",
    "# Apply colors (blue and orange)\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('Ranking score')\n",
    "ax.set_title('Distribution of ranking scores in prediction dataset')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add METRIC_CUTOFF as vertical line\n",
    "ax.axhline(y=METRIC_CUTOFF, color='black', linestyle='--', alpha=0.7, label=f'Threshold: {METRIC_CUTOFF}')\n",
    "\n",
    "# Calculate medians and sample sizes\n",
    "medians = [data.median() for data in data_to_plot]\n",
    "sample_sizes = [len(data) for data in data_to_plot]\n",
    "\n",
    "# Add sample sizes and medians to the plot\n",
    "for i, (data, median, n) in enumerate(zip(data_to_plot, medians, sample_sizes)):\n",
    "    ax.text(i+1, ax.get_ylim()[1] * 0.9, f'n={n}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "    ax.text(i+1, ax.get_ylim()[1] * 0.85, f'median: {median:.3f}', \n",
    "            ha='center', va='top', fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/markus/Desktop/Thesis/pred_comparison_boxplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "print(f\"All predictions: n={len(pred_ds_results_filtered)}, median={pred_ds_results_filtered['ranking_score'].median():.3f}\")\n",
    "print(f\"Disorder-mediated: n={len(pred_ds_results_diso_med_filtered)}, median={pred_ds_results_diso_med_filtered['ranking_score'].median():.3f}\")\n",
    "\n",
    "# Print counts above/below cutoff\n",
    "n_above_all = len(pred_ds_results_filtered[pred_ds_results_filtered['ranking_score'] >= METRIC_CUTOFF])\n",
    "n_above_diso = len(pred_ds_results_diso_med_filtered[pred_ds_results_diso_med_filtered['ranking_score'] >= METRIC_CUTOFF])\n",
    "pct_above_all = (n_above_all / len(pred_ds_results_filtered)) * 100\n",
    "pct_above_diso = (n_above_diso / len(pred_ds_results_diso_med_filtered)) * 100\n",
    "\n",
    "print(f\"Above {METRIC_CUTOFF} threshold:\")\n",
    "print(f\"All predictions: {n_above_all} ({pct_above_all:.1f}%)\")\n",
    "print(f\"Disorder-mediated: {n_above_diso} ({pct_above_diso:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# STRING_SCORE_COLUMN = 'experimental'\n",
    "# INTACT_SCORE_COLUMN = 'intact_score'\n",
    "\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'iptm', STRING_SCORE_COLUMN, ax=axes[0], scatter_threshold=2)\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', STRING_SCORE_COLUMN, ax=axes[1], scatter_threshold=2)\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2], scatter_threshold=2)\n",
    "\n",
    "# fig.suptitle('AlphaFold Metrics vs STRING score, NAs dropped', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "# create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "# fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52841c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = create_overlaid_AF_violin_plot(\n",
    "#     dataset1=results_df_annotated,\n",
    "#     dataset1_name='all pairs', \n",
    "#     dataset2=report_2_pairs,\n",
    "#     dataset2_name='report 2',\n",
    "#     figsize=(12, 6)\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# co = 0.9\n",
    "# string_high_scoring = results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN])\n",
    "# string_high_scoring = string_high_scoring[string_high_scoring[STRING_SCORE_COLUMN] > co]\n",
    "\n",
    "# fig, ax = create_overlaid_AF_violin_plot(\n",
    "#     dataset1=string_high_scoring,\n",
    "#     dataset1_name=f\"STRING high scoring {co}\", \n",
    "#     dataset2=results_df_annotated,\n",
    "#     dataset2_name='all pairs',\n",
    "#     figsize=(12, 6)\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = create_overlaid_AF_violin_plot(\n",
    "#     dataset1=results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]),\n",
    "#     dataset1_name='STRING', \n",
    "#     dataset2=results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]),\n",
    "#     dataset2_name='IntAct',\n",
    "#     figsize=(12, 6)\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = create_overlaid_AF_violin_plot(\n",
    "#     dataset1=results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]),\n",
    "#     dataset1_name='STRING', \n",
    "#     dataset2=results_df_annotated,\n",
    "#     dataset2_name='all pairs',\n",
    "#     figsize=(12, 6)\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = create_overlaid_AF_violin_plot(\n",
    "#     dataset1=results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]),\n",
    "#     dataset1_name='IntAct', \n",
    "#     dataset2=results_df_annotated,\n",
    "#     dataset2_name='all pairs',\n",
    "#     figsize=(12, 6)\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32896a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXME: should I include duplicate values?\n",
    "# prediction_IntAct = results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN].notna()]\n",
    "# prediction_IntAct_plot = prediction_IntAct[prediction_IntAct['ranking_score'] >= 0]\n",
    "\n",
    "# prediction_STRING = results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN].notna()]\n",
    "# prediction_STRING_plot = prediction_STRING[prediction_STRING['ranking_score'] >= 0]\n",
    "\n",
    "# print(len(prediction_IntAct[prediction_IntAct['ranking_score']>=0.65]))\n",
    "# print(len(prediction_IntAct[prediction_IntAct['ranking_score']<0.65]))\n",
    "# print(len(prediction_STRING[prediction_STRING['ranking_score']>=0.65]))\n",
    "# print(len(prediction_STRING[prediction_STRING['ranking_score']<0.65]))\n",
    "\n",
    "\n",
    "# fig, ax = create_double_violin_plot(prediction_IntAct_plot, 'IntAct', prediction_STRING_plot, 'STRING (experimental)', 'ranking_score', 'ranking_score', (8,8))\n",
    "# plt.axhline(y=0.65, color='red', linestyle='--', alpha=0.7, label='y=0.65')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a732c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_IntAct_cutoff = prediction_IntAct[prediction_IntAct[INTACT_SCORE_COLUMN] > 0.4]\n",
    "# prediction_IntAct_cutoff_plot = prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score'] >= 0]\n",
    "\n",
    "# prediction_STRING_cutoff = prediction_STRING[prediction_STRING[STRING_SCORE_COLUMN] > 0.4]\n",
    "# prediction_STRING_cutoff_plot = prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0711e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score']>=0.65]))\n",
    "# print(len(prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score']<0.65]))\n",
    "# print(len(prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score']>=0.65]))\n",
    "# print(len(prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score']<0.65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7891a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = create_double_violin_plot(prediction_IntAct_cutoff_plot, 'IntAct', prediction_STRING_cutoff_plot, 'STRING (experimental)', 'ranking_score', 'ranking_score', (8,8))\n",
    "# plt.axhline(y=0.65, color='red', linestyle='--', alpha=0.7, label='y=0.65')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_set = results_df_annotated[(~results_df_annotated[STRING_SCORE_COLUMN].notna()) & (~results_df_annotated[INTACT_SCORE_COLUMN].notna())]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# below_cutoff = prediction_set[(prediction_set['ranking_score'] < 0.65) & (prediction_set['ranking_score'] > 0)]['ranking_score']\n",
    "# above_cutoff = prediction_set[prediction_set['ranking_score'] >= 0.65]['ranking_score']\n",
    "\n",
    "# plt.hist(below_cutoff, bins=18, alpha=0.7, color='red', label=f'Below 0.65 (n={len(below_cutoff)})')\n",
    "# plt.hist(above_cutoff, bins=17, alpha=0.7, color='green', label=f'Above/Equal 0.65 (n={len(above_cutoff)})')\n",
    "\n",
    "# plt.axvline(x=0.65, color='black', linestyle='--', linewidth=2, label='Cutoff = 0.65')\n",
    "# plt.xlabel('Ranking Score')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Ranking Scores in Prediction Set')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb91f86",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88025a",
   "metadata": {},
   "source": [
    "creating AUC curves with STRING / IntAct alone does not really make sense. My understanding is that both include only **positive** interaction candidates and assign scores to the certainty. So even a low score means a relatively high probability of interaction because the candidate is in the DB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
