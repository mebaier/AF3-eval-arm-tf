{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "## General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.physical.links.detailed.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "# PROTEOME_PATH = '/home/markus/MPI_local/data/full_UP/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "IUPRED3_PATH = '../../iupred3'\n",
    "\n",
    "AF_TOKEN_LIMIT = 5120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uniprot = pd.read_csv(PROTEOME_PATH, low_memory=False, sep='\\t')\n",
    "uniprot_filtered = all_uniprot[all_uniprot['Reviewed'] == 'reviewed']\n",
    "# uniprot_filtered = all_uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filter lists\n",
    "from filter_data import arm_accs_ipr, arm_accs_pfam\n",
    "\n",
    "# strip version number since it is not included in the uniprot annotation\n",
    "arm_accs_pfam_stripped = [acc.split(\".\")[0] for acc in arm_accs_pfam]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = uniprot_filtered['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(uniprot_filtered[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = uniprot_filtered['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(uniprot_filtered[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = uniprot_filtered['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam_stripped))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(uniprot_filtered[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = uniprot_filtered[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "# TODO: use caching\n",
    "# TODO: adjust ensemble mapping if using other proteome?\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = uniprot_filtered[uniprot_filtered['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))\n",
    "\n",
    "# 4m 1.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IUPRED3_THRESHOLD = 0.5\n",
    "MIN_LENGTH_DISORDERED_REGION = 20\n",
    "IUPRED_CACHE_DIR = '/home/markus/MPI_local/production1/IUPred3'\n",
    "\n",
    "tf_proteins_curated_ds = add_iupred3(tf_proteins_curated_ds, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f\"Number of transcription factors with at least one disordered region (IUPred3, threshold={IUPRED3_THRESHOLD}, min length={MIN_LENGTH_DISORDERED_REGION}): {len(tf_proteins_curated_ds_IUPred3_diso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_over_token_limit = len(all_pairs[all_pairs['Length_arm'] + all_pairs['Length_tf'] > AF_TOKEN_LIMIT])\n",
    "print(f\"Pairs over token limit: {num_pairs_over_token_limit} ({(num_pairs_over_token_limit/len(all_pairs))*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_STRING = len(all_pairs_w_STRING)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {num_all_pairs_w_STRING} ({(num_all_pairs_w_STRING/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_intact import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned = read_clean_intact('../../data/IntAct/human/human.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_cleaned, on='pair_id', how='inner')\n",
    "\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_IntAct = len(all_pairs_w_IntAct)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {num_all_pairs_w_IntAct} ({(num_all_pairs_w_IntAct/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_intersect_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "\n",
    "print(len(all_pairs_intersect_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_union_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='outer')\n",
    "\n",
    "print(len(all_pairs_union_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd617a",
   "metadata": {},
   "source": [
    "### write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in armadillo_proteins.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/arm_all_uniprot_rev_fasta', row['Reviewed'])\n",
    "# for idx, row in tf_proteins_curated_ds_IUPred3_diso.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/tf_all_uniprot_rev_fasta', row['Reviewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armadillo_proteins.to_csv('../../armadillo_proteins.csv', index=False)\n",
    "# tf_proteins_curated_ds_IUPred3_diso.to_csv('../../transcription_factors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fd92d",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### create job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6646c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "### STRING\n",
    "# note that the order is important. The category (100,200) is very large so it comes last to fill up the remaining jobs\n",
    "# categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (100,200), (0,100), (200,300)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS, STRING_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### IntAct\n",
    "# categories = [(0.1,0.2), (0.9,1), (0.8,0.9), (0.7,0.8), (0.6,0.7), (0.5,0.6), (0.4,0.5), (0.2,0.3), (0.3,0.4)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_IntAct, BATCH_SIZE, categories, BATCH_DIRS, INTACT_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### all pairs\n",
    "new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "### ID list:\n",
    "id_list_good = [\n",
    "    (\"Q13285\", \"A0A2R8YCH5\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"Q9H3D4\", \"A0A8I5KU01\"),\n",
    "    (\"Q8NHM5\", \"A1YPR0\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q6SJ96\", \"O14981\"),\n",
    "    (\"Q9NRY4\", \"O00750\"),\n",
    "    (\"Q6ZRS2\", \"A0A8V8TQN3\")\n",
    "]\n",
    "\n",
    "# id_list_complex = [\n",
    "#     (\"Q03181\", \"Q9H3U1\"),\n",
    "#     (\"P04637\", \"A0A994J4J0\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"Q9UBG7\", \"A0A8I5KU01\"),\n",
    "#     (\"P19838\", \"A0A1W2PRG6\")\n",
    "# ]\n",
    "\n",
    "missing = [('Q13285', 'Q6BTZ4'), ('P04637', 'Q9VL06'), ('P04637', 'Q9VL06'), ('Q9UIF8', 'Q54U63'), ('Q15047', 'Q54U63'), ('Q15047', 'Q5R881'), ('Q9H3D4', 'Q9VL06'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('Q6ZRS2', 'P38811'), ('Q6ZRS2', 'P38811')]\n",
    "# new_af_jobs = create_job_batch_id_list(all_pairs, missing, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/AF_job_batches/batch_51')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143d563",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a644",
   "metadata": {},
   "source": [
    "### Negatomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a83e",
   "metadata": {},
   "source": [
    "#### IntAct Negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intact_negative = pd.read_csv('../../data/IntAct/human/human_negative.txt', sep='\\t')\n",
    "# intact_negative.drop(['Alias(es) interactor A', \n",
    "#                      'Alias(es) interactor B', \n",
    "#                      'Interaction detection method(s)',\n",
    "#                      'Publication 1st author(s)',\n",
    "#                      'Publication Identifier(s)',\n",
    "#                      'Taxid interactor A',\n",
    "#                      'Taxid interactor B',\n",
    "#                      'Biological role(s) interactor A',\n",
    "#                      'Biological role(s) interactor B',\n",
    "#                      'Experimental role(s) interactor A',\n",
    "#                      'Experimental role(s) interactor B',\n",
    "#                      'Type(s) interactor A',\n",
    "#                      'Type(s) interactor B',\n",
    "#                      'Xref(s) interactor A',\n",
    "#                      'Xref(s) interactor B',\n",
    "#                      'Interaction Xref(s)',\n",
    "#                      'Annotation(s) interactor A',\n",
    "#                      'Annotation(s) interactor B',\n",
    "#                      'Interaction annotation(s)',\n",
    "#                      'Host organism(s)',\n",
    "#                      'Interaction parameter(s)',\n",
    "#                      'Creation date',\n",
    "#                      'Update date',\n",
    "#                      'Checksum(s) interactor A',\n",
    "#                      'Checksum(s) interactor B',\n",
    "#                      'Interaction Checksum(s)',\n",
    "#                      'Feature(s) interactor A',\n",
    "#                      'Feature(s) interactor B',\n",
    "#                      'Stoichiometry(s) interactor A',\n",
    "#                      'Stoichiometry(s) interactor B',\n",
    "#                      'Identification method participant A',\n",
    "#                      'Identification method participant B',\n",
    "#                      'Expansion method(s)'\n",
    "#                      ], axis=1, inplace=True)\n",
    "# intact_negative.loc[:, 'intact_score'] = intact_negative.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "# intact_negative['pair_id'] = intact_negative.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', '').split('-')[0], row['ID(s) interactor B'].replace('uniprotkb:', '').split('-')[0]]))), axis=1)\n",
    "# intact_negative = intact_negative.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "# all_pairs_intact_negative = pd.merge(all_pairs, intact_negative, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_intact_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4c239",
   "metadata": {},
   "source": [
    "#### Blohm negatome2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatome2 = pd.read_csv('../../data/negatome2.0/combined.txt', sep='\\t', names=['ID_1', 'ID_2'])\n",
    "# negatome2['pair_id'] = negatome2.apply(lambda row: str(tuple(sorted([row['ID_1'].split('-')[0], row['ID_2'].split('-')[0]]))), axis=1)\n",
    "# all_pairs_negatome2 = pd.merge(all_pairs, negatome2, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_negatome2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63893e",
   "metadata": {},
   "source": [
    "#### Stelzl 2005 negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stelzl_neg = pd.read_csv('../../data/16169070_neg.mitab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Read in HPC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df_uc = pd.DataFrame(data=find_summary_files(HPC_RESULT_DIR))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df_uc)}\")\n",
    "\n",
    "results_df_uc['pair_id'] = results_df_uc.apply(create_pair_id, axis=1)\n",
    "\n",
    "print(f\"jobs before cleaning: {len(results_df_uc)}\")\n",
    "results_df = clean_results(results_df_uc)\n",
    "print(f\"jobs after cleaning: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, intact_cleaned, on='pair_id', how='left')\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total number of modelled pairs: {len(results_df)}\")\n",
    "print(f\"Total rows in merged_df: {len(results_df_annotated)}\")\n",
    "print(f\"Rows with annotated data (STRING): {results_df_annotated['combined_score'].notna().sum()}\")\n",
    "print(f\"Rows with annotated data (IntAct): {results_df_annotated['intact_score'].notna().sum()}\")\n",
    "\n",
    "\n",
    "# convert all STRING scores from 0-1000 to 0-1 (linear conversion)\n",
    "STRING_COLS = ['experimental', 'database', 'textmining', 'combined_score']\n",
    "for col in STRING_COLS:\n",
    "    results_df_annotated[col] = results_df_annotated[col] / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6969fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs treated as 0', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments), NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments); NAs included', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'iptm', INTACT_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'ptm', INTACT_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', 'iptm', INTACT_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by IntAct Score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated['avg_STRING_IntAct'] = (results_df_annotated[STRING_SCORE_COLUMN] + results_df_annotated[INTACT_SCORE_COLUMN]) / 2 \n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'iptm', 'avg_STRING_IntAct', 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'ptm', 'avg_STRING_IntAct', 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ptm', 'iptm', 'avg_STRING_IntAct', 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by average of STRING and IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb91f86",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88025a",
   "metadata": {},
   "source": [
    "creating AUC curves with STRING / IntAct alone does not really make sense. My understanding is that both include only **positive** interaction candidates and assign scores to the certainty. So even a low score means a relatively high probability of interaction because the candidate is in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def plot_roc(df: pd.DataFrame, pos: Set[str], neg: Set[str], param_name: str, min_val: float, max_val: float, \n",
    "             sampling: int = 1000, direction: str = 'up', plot_title: str = '', ax=None) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Calculate and plot the ROC curve based on a specified parameter.\n",
    "    \n",
    "    This function evaluates the performance of a binary classifier by varying a threshold parameter\n",
    "    and calculating the True Positive Rate (TPR) and False Positive Rate (FPR) at each threshold.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the parameter to evaluate and pair_id column\n",
    "        pos (Set[str]): Set of positive example pair_ids (ground truth positive cases)\n",
    "        neg (Set[str]): Set of negative example pair_ids (ground truth negative cases)\n",
    "        param_name (str): Name of the column in df to use as the classification parameter\n",
    "        min_val (float): Minimum threshold value to evaluate\n",
    "        max_val (float): Maximum threshold value to evaluate\n",
    "        sampling (int, optional): Number of threshold points to sample between min and max. Defaults to 1000.\n",
    "        direction (str, optional): Direction of classification - 'up' means values >= threshold are positive,\n",
    "                                  'down' means values < threshold are positive. Defaults to 'up'.\n",
    "        plot_title (str, optional): Title for the ROC curve plot. If empty, a default title is used. Defaults to ''.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If None, creates new figure.\n",
    "                                   \n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Lists of FPR and TPR values that make up the ROC curve\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        # Print the size of positive and negative sets for verification    \n",
    "        print(f\"Number of positive examples: {len(pos)}\")\n",
    "        print(f\"Number of negative examples: {len(neg)}\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "\n",
    "    # Calculate step size based on range and sampling\n",
    "    step = (max_val - min_val) / sampling\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"max_val must be greater than min_val\")\n",
    "    \n",
    "    # Lists to store TPR and FPR values\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    \n",
    "    for i in range(sampling + 1):\n",
    "        sep_val = min_val + (i * step)\n",
    "        \n",
    "        if direction == 'up':\n",
    "            calc_pos = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "        elif direction == 'down':\n",
    "            calc_pos = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "        else:\n",
    "            raise ValueError(\"direction must be either 'up' or 'down'\")\n",
    "        \n",
    "        # Calculate confusion matrix values\n",
    "        TP_num = len(calc_pos.intersection(pos))\n",
    "        FP_num = len(calc_pos.intersection(neg))\n",
    "        TN_num = len(calc_neg.intersection(neg))\n",
    "        FN_num = len(calc_neg.intersection(pos))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        TPR = TP_num / max(TP_num + FN_num, 1)\n",
    "        FPR = FP_num / max(FP_num + TN_num, 1)\n",
    "        \n",
    "        tpr_list.append(TPR)\n",
    "        fpr_list.append(FPR)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    ax.plot(fpr_list, tpr_list, 'b-', linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2)  # Diagonal line representing random guess\n",
    "    \n",
    "    auc_value = sklearn.metrics.auc(fpr_list, tpr_list)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    \n",
    "    if plot_title:\n",
    "        title = plot_title\n",
    "    else:\n",
    "        title = f'ROC Curve for {param_name}'\n",
    "    \n",
    "    ax.set_title(f'{title}\\nAUC = {auc_value:.3f}', fontsize=12)\n",
    "    \n",
    "    # Add grid and improve appearance\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fpr_list, tpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRING_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# string_combined_pos = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] >= STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# string_combined_neg = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] < STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(string_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(string_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (STRING cutoff >= {STRING_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152712a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTACT_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# intact_combined_pos = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] >= INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# intact_combined_neg = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] < INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(intact_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(intact_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (IntAct cutoff >= {INTACT_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
