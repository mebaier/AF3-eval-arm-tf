{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "## General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional, Set\n",
    "\n",
    "def contains_any_annotation(cell_value: Any, annotations_list: List[str]) -> bool:\n",
    "    \"\"\"Check if any of the annotations are in the column value.\n",
    "\n",
    "    Args:\n",
    "        cell_value (Any): Cell value from DataFrame column to check\n",
    "        annotations_list (List[str]): List of annotation strings to check for\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any annotation is found in the cell_value, False otherwise\n",
    "    \"\"\"\n",
    "    if pd.isna(cell_value):\n",
    "        return False\n",
    "    for annotation in annotations_list:\n",
    "        if annotation in cell_value:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.physical.links.detailed.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "# PROTEOME_PATH = '/home/markus/MPI_local/data/full_PDB/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "IUPRED3_PATH = '../../iupred3'\n",
    "\n",
    "AF_TOKEN_LIMIT = 5120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uniprot = pd.read_csv(PROTEOME_PATH, low_memory=False, sep='\\t')\n",
    "uniprot_filtered = all_uniprot[all_uniprot['Reviewed'] == 'reviewed']\n",
    "# uniprot_filtered = all_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Armadillos\n",
    "## all the interpro IDs that have the word \"armadillo\" in the description\n",
    "arm_accs_ipr = [\n",
    "    'IPR013636',\n",
    "    'IPR024574',\n",
    "    'IPR041322',\n",
    "    'IPR055241',\n",
    "    'IPR056252',\n",
    "    'IPR016617',\n",
    "    'IPR031524',\n",
    "    'IPR038739',\n",
    "    'IPR038905',\n",
    "    'IPR039868',\n",
    "    'IPR040268',\n",
    "    'IPR042462',\n",
    "    'IPR042834',\n",
    "    'IPR043379',\n",
    "    'IPR044282',\n",
    "    'IPR051303',\n",
    "    'IPR052441',\n",
    "    'IPR011989',\n",
    "    'IPR016024',\n",
    "    'IPR000225',\n",
    "    'IPR041209',\n",
    "    'IPR049152',\n",
    "    'IPR006911'\n",
    "]\n",
    "\n",
    "# all pfam accessions that have \"armadillo\" in the description\n",
    "arm_accs_pfam = [\n",
    "    'PF00514.29',\n",
    "    'PF17822.6',\n",
    "    'PF08427.15',\n",
    "    'PF15767.10',\n",
    "    'PF22915.1',\n",
    "    'PF04826.19',\n",
    "    'PF23295.1',\n",
    "    'PF16629.10',\n",
    "    'PF18770.7',\n",
    "    'PF21052.3',\n",
    "    'PF11841.14',\n",
    "    'PF14726.11',\n",
    "    'PF18581.7'\n",
    "]\n",
    "\n",
    "# strip version number since it is not included in the uniprot annotation\n",
    "for i in range(len(arm_accs_pfam)):\n",
    "    arm_accs_pfam[i] = arm_accs_pfam[i].split(\".\")[0]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = uniprot_filtered['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(uniprot_filtered[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = uniprot_filtered['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(uniprot_filtered[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = uniprot_filtered['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(uniprot_filtered[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = uniprot_filtered[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "armadillo_proteins['Entry'].to_csv('armadillo_proteins_entries.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = uniprot_filtered[uniprot_filtered['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d244af",
   "metadata": {},
   "source": [
    "### Filter disordered transcription factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37cf2f",
   "metadata": {},
   "source": [
    "#### AIUPred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecd2c5",
   "metadata": {},
   "source": [
    "##### Annotaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c22ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API requests (only necessary once, see cache file AIUPRED_PATH)\n",
    "\n",
    "# url = 'https://aiupred.elte.hu/rest_api'\n",
    "\n",
    "# AIUPred_data = []\n",
    "# c = 0\n",
    "# for acc in tf_proteins_curated_ds['Entry'].tolist():\n",
    "#     data = {'accession': acc, 'smoothing': 'default'}\n",
    "#     response = requests.get(url, params=data)\n",
    "#     if response.status_code == 200:\n",
    "#         AIUPred_data.append(json.loads(response.text))\n",
    "#         c += 1\n",
    "#         if c % 100 == 0:\n",
    "#             print(c)\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch data for accession {acc}: {response.status_code}\")\n",
    "        \n",
    "# with open('AIUPred_data.json', 'a') as json_file:\n",
    "#     json.dump(AIUPred_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cache file\n",
    "with open(AIUPRED_PATH, 'r') as file:\n",
    "    AIUPred_df = pd.DataFrame(json.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56f6c0",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def find_subranges(data: List[float], threshold: float, min_length: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Find continuous subranges in data where values exceed the threshold for at least min_length positions.\n",
    "\n",
    "    Args:\n",
    "        data (List[float]): List of numerical values to analyze\n",
    "        threshold (float): Minimum value to be considered part of a subrange\n",
    "        min_length (int): Minimum length a subrange must have to be included in results\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: List of tuples containing start and end indices of subranges\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    start = None\n",
    "\n",
    "    for i, value in enumerate(data):\n",
    "        if value >= threshold:\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            # End of a potential subrange\n",
    "            if start is not None:\n",
    "                if i - start >= min_length:\n",
    "                    subranges.append((start, i - 1))\n",
    "                start = None\n",
    "    \n",
    "    # Check if we ended with an ongoing subrange\n",
    "    if start is not None:\n",
    "        if len(data) - start >= min_length:\n",
    "            subranges.append((start, len(data) - 1))\n",
    "\n",
    "    return subranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ab1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert find_subranges([1,1,2,4,5,6,2,3,1], 2, 3) == [(2,7)]\n",
    "assert find_subranges([1,1,2,4,5,6,2,3,1], 2, 20) == []\n",
    "assert find_subranges([], 2, 20) == []\n",
    "assert find_subranges([1,2,2,2,3,3,3,3,2,3,3,2,1,3,3,3], 3, 3) == [(4,7), (13,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH_DISORDERED_REGION = 20\n",
    "AIUPRED_THRESHOLD = 0.9\n",
    "\n",
    "# AIUPred_df['ind_disordered_regions'] = AIUPred_df['AIUPred'].apply(lambda x: find_subranges(x, AIUPRED_THRESHOLD, MIN_LENGTH_DISORDERED_REGION))\n",
    "# AIUPred_df['num_disordered_regions'] = AIUPred_df['ind_disordered_regions'].apply(len)\n",
    "# tf_proteins_curated_ds_AIUpred = tf_proteins_curated_ds.merge(AIUPred_df, left_on='Entry', right_on='accession', how='inner')\n",
    "# tf_proteins_curated_ds_AIUpred_diso = tf_proteins_curated_ds_AIUpred[tf_proteins_curated_ds_AIUpred['num_disordered_regions'] > 0]\n",
    "\n",
    "# print(f'Num proteins before disorder filter: {len(tf_proteins_curated_ds_AIUpred)}')\n",
    "# print(f'Num proteins after disorder filter: {len(tf_proteins_curated_ds_AIUpred_diso)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(IUPRED3_PATH)\n",
    "import iupred3_lib\n",
    "\n",
    "# sequence = tf_proteins_curated_ds['Sequence'].iloc[0]\n",
    "# print(sequence)\n",
    "# iupred3_result = iupred3_lib.iupred(sequence, 'long', smoothing='no')\n",
    "# print(iupred3_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: caching\n",
    "IUPRED3_THRESHOLD = 0.5\n",
    "\n",
    "tf_proteins_curated_ds['iupred3'] = tf_proteins_curated_ds['Sequence'].apply(lambda x: iupred3_lib.iupred(x, 'long', smoothing='no')[0])\n",
    "\n",
    "tf_proteins_curated_ds['num_disordered_regions'] = tf_proteins_curated_ds['iupred3'].apply(lambda x: len(find_subranges(x, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION)))\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f\"Number of transcription factors with at least one disordered region (IUPred3, threshold={IUPRED3_THRESHOLD}, min length={MIN_LENGTH_DISORDERED_REGION}): {len(tf_proteins_curated_ds_IUPred3_diso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_pairs(arm_df: pd.DataFrame, tf_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe with all possible pairs from the cartesian product of the two dataframes arm_df and tf_df.\n",
    "    \n",
    "    This function performs a full cartesian join between the armadillo proteins dataframe and the transcription factor proteins\n",
    "    dataframe, generating all possible combinations between them.\n",
    "\n",
    "    Args:\n",
    "        arm_df (pd.DataFrame): DataFrame containing armadillo proteins\n",
    "        tf_df (pd.DataFrame): DataFrame containing transcription factor proteins\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all possible pairs between armadillo and transcription factor proteins\n",
    "        with a unique pair_id column for each combination\n",
    "    \"\"\"\n",
    "    # Create a key for cross join\n",
    "    arm_df_temp = arm_df.copy()\n",
    "    tf_df_temp = tf_df.copy()\n",
    "    \n",
    "    arm_df_temp['key'] = 1\n",
    "    tf_df_temp['key'] = 1\n",
    "    \n",
    "    # Perform a cross join using the dummy key\n",
    "    pairs_df = pd.merge(arm_df_temp, tf_df_temp, on='key', suffixes=('_arm', '_tf'))\n",
    "    \n",
    "    # Drop the dummy key column\n",
    "    pairs_df = pairs_df.drop('key', axis=1)\n",
    "    \n",
    "    # Create pair_id column for consistency with other functions in the pipeline\n",
    "    pairs_df['pair_id'] = pairs_df.apply(lambda row: str(tuple(sorted([row['Entry_arm'].upper(), row['Entry_tf'].upper()]))), axis=1)\n",
    "    \n",
    "    print(f\"Created {len(pairs_df)} possible protein pairs between {len(arm_df)} armadillo proteins and {len(tf_df)} transcription factors\")\n",
    "    \n",
    "    return pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_over_token_limit = len(all_pairs[all_pairs['Length_arm'] + all_pairs['Length_tf'] > AF_TOKEN_LIMIT])\n",
    "print(f\"Pairs over token limit: {num_pairs_over_token_limit} ({(num_pairs_over_token_limit/len(all_pairs))*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_STRING = len(all_pairs_w_STRING)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {num_all_pairs_w_STRING} ({(num_all_pairs_w_STRING/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66782a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_df = pd.read_csv('../../data/IntAct/human/human.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# clean intact dataset:\n",
    "# create new column 'intact_score' that has the score from 'Confidence value(s)' parsed\n",
    "# drop unnecessary columns\n",
    "\n",
    "def intact_score_filter(x: str) -> float:\n",
    "    \"\"\"Parse IntAct miscore from confidence value string.\n",
    "    \n",
    "    Args:\n",
    "        x (str): Confidence value string containing intact-miscore\n",
    "        \n",
    "    Returns:\n",
    "        float: Extracted IntAct miscore, or 0.0 if not found\n",
    "    \"\"\"\n",
    "    match = re.search(r\"(?<=intact-miscore:)\\d\\.\\d*\", x)\n",
    "    return float(match.group()) if match else 0.0\n",
    "\n",
    "print(len(intact_df))\n",
    "intact_cleaned = intact_df.copy()\n",
    "intact_cleaned.drop(['Alias(es) interactor A', \n",
    "                     'Alias(es) interactor B', \n",
    "                     'Interaction detection method(s)',\n",
    "                     'Publication 1st author(s)',\n",
    "                     'Publication Identifier(s)',\n",
    "                     'Taxid interactor A',\n",
    "                     'Taxid interactor B',\n",
    "                     'Biological role(s) interactor A',\n",
    "                     'Biological role(s) interactor B',\n",
    "                     'Experimental role(s) interactor A',\n",
    "                     'Experimental role(s) interactor B',\n",
    "                     'Type(s) interactor A',\n",
    "                     'Type(s) interactor B',\n",
    "                     'Xref(s) interactor A',\n",
    "                     'Xref(s) interactor B',\n",
    "                     'Interaction Xref(s)',\n",
    "                     'Annotation(s) interactor A',\n",
    "                     'Annotation(s) interactor B',\n",
    "                     'Interaction annotation(s)',\n",
    "                     'Host organism(s)',\n",
    "                     'Interaction parameter(s)',\n",
    "                     'Creation date',\n",
    "                     'Update date',\n",
    "                     'Checksum(s) interactor A',\n",
    "                     'Checksum(s) interactor B',\n",
    "                     'Interaction Checksum(s)',\n",
    "                     'Feature(s) interactor A',\n",
    "                     'Feature(s) interactor B',\n",
    "                     'Stoichiometry(s) interactor A',\n",
    "                     'Stoichiometry(s) interactor B',\n",
    "                     'Identification method participant A',\n",
    "                     'Identification method participant B',\n",
    "                     'Expansion method(s)'\n",
    "                     ], axis=1, inplace=True)\n",
    "# intact_cleaned = intact_df[intact_df['Interaction type(s)'].apply(lambda x: 'direct interaction' in x)]\n",
    "intact_cleaned.loc[:, 'intact_score'] = intact_cleaned.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "print(len(intact_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f796f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned['pair_id'] = intact_cleaned.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', ''), row['ID(s) interactor B'].replace('uniprotkb:', '')]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned = intact_cleaned.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "print(f\"Number of unique pairs after deduplication: {len(intact_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_cleaned, on='pair_id', how='inner')\n",
    "\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_IntAct = len(all_pairs_w_IntAct)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {num_all_pairs_w_IntAct} ({(num_all_pairs_w_IntAct/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_intersect_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "\n",
    "print(len(all_pairs_intersect_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_union_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='outer')\n",
    "\n",
    "print(len(all_pairs_union_STRING_IntAct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd617a",
   "metadata": {},
   "source": [
    "### Create Structure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_to_fasta(id: str, seq: str, path: str, comment: str = '') -> None:\n",
    "    \"\"\"Create a FASTA file with the given ID and sequence.\n",
    "    \n",
    "    This function creates a new FASTA file with the specified ID as the filename\n",
    "    (with .fasta extension) and writes the sequence in standard FASTA format.\n",
    "    \n",
    "    Args:\n",
    "        id (str): Protein ID to use as both the filename and the FASTA header\n",
    "        seq (str): Amino acid sequence to write to the file\n",
    "        path (str): Directory path where the FASTA file should be created\n",
    "        comment (str, optional): Optional comment to add to the FASTA header. Defaults to ''.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Raises:\n",
    "        OSError: If the directory cannot be created or file cannot be written\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    filename = f\"{id}.fasta\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "    \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    header = f\">{id}\"\n",
    "    if comment:\n",
    "        header += f'|{comment}\\n'\n",
    "    else:\n",
    "        header += '\\n'\n",
    "    \n",
    "    with open(full_path, 'w') as f:\n",
    "        f.write(header)\n",
    "        f.write(f\"{seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in armadillo_proteins.iterrows():\n",
    "    print_to_fasta(row['Entry'], row['Sequence'], '../../production1/arm_all_uniprot_rev_fasta', row['Reviewed'])\n",
    "# for idx, row in tf_proteins_curated_ds_IUPred3_diso.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/tf_all_uniprot_rev_fasta', row['Reviewed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fd92d",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55461481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Tuple\n",
    "import math, random\n",
    "import os\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf6053",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_af_jobs_to_individual_files(af_jobs: List[Dict[str, Any]], output_dir: str) -> None:\n",
    "    \"\"\"Write each AlphaFold job to an individual file.\n",
    "    \n",
    "    Each job is saved as a JSON file in the specified output directory with the job's name as the filename.\n",
    "    \n",
    "    Args:\n",
    "        af_jobs (List[Dict[str, Any]]): List of AlphaFold job dictionaries\n",
    "        output_dir (str): Directory where job files will be saved\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Raises:\n",
    "        SystemExit: If output directory already exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"ERROR: output directory '{output_dir}' already exists!\")\n",
    "        print(\"Aborting.\")\n",
    "        return\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    for job in af_jobs:\n",
    "        file_name = f\"{job['name']}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump([job], f, indent=2)  # use [] so AF parser knows it's in alphafoldserver dialect\n",
    "\n",
    "def sort_rec(obj: Union[List[Any], Dict[str, Any], Any]) -> Any:\n",
    "    \"\"\"Sort a list or dictionary recursively.\n",
    "    \n",
    "    This function is used to create comparable job representations by sorting all nested structures.\n",
    "    \n",
    "    Args:\n",
    "        obj (Union[List[Any], Dict[str, Any], Any]): Object to sort recursively\n",
    "\n",
    "    Returns:\n",
    "        Any: Sorted object with all nested structures sorted\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return sorted((k, sort_rec(v)) for k, v in obj.items())\n",
    "    if isinstance(obj, list):\n",
    "        return sorted(sort_rec(x) for x in obj)\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def get_comparable_job(job_data: Dict[str, Any], deep_copy: bool = True) -> Any:\n",
    "    \"\"\"Create a comparable representation of the job by removing the name field and sorting the other fields.\n",
    "    \n",
    "    This function creates a standardized representation of an AlphaFold job that can be compared\n",
    "    to other jobs to detect duplicates, regardless of the job name or field order.\n",
    "    \n",
    "    Args:\n",
    "        job_data (Dict[str, Any]): AlphaFold job dictionary\n",
    "        deep_copy (bool, optional): Whether to create a deep copy of the job data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Any: Comparable representation of the job\n",
    "    \"\"\"\n",
    "    if deep_copy:\n",
    "        # Create a deep copy to avoid modifying the original\n",
    "        comparable = copy.deepcopy(job_data)\n",
    "    else:\n",
    "        comparable = job_data\n",
    "    if 'name' in comparable:\n",
    "        del comparable['name']\n",
    "    return sort_rec(comparable)\n",
    "\n",
    "def collect_created_jobs(results_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect all jobs in a directory (all .json files are considered jobs).\n",
    "    \n",
    "    IMPORTANT: one file is considered to have one job!\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Directory containing AlphaFold job files\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of AlphaFold job dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        OSError: If directory cannot be accessed\n",
    "    \"\"\"\n",
    "    collected_jobs = []\n",
    "    \n",
    "    # Go through all .json files in the results directory (not recursively)\n",
    "    for file_name in os.listdir(results_dir):\n",
    "        if file_name.endswith('.json') and os.path.isfile(os.path.join(results_dir, file_name)):\n",
    "            try:\n",
    "                with open(os.path.join(results_dir, file_name), 'r') as f:\n",
    "                    collected_jobs += json.load(f)\n",
    "                    \n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "                continue\n",
    "    return collected_jobs\n",
    "\n",
    "def create_alphafold_job(job_name: str, sequence1: str, sequence2: str, dialect: str = 'alphafoldserver') -> Dict[str, Any]:\n",
    "    \"\"\"Create a standardized AlphaFold job dictionary from input parameters.\n",
    "\n",
    "    Args:\n",
    "        job_name (str): Name of the job, typically in the format \"protein1_start-end_protein2_start-end\"\n",
    "        sequence1 (str): Amino acid sequence of the first protein\n",
    "        sequence2 (str): Amino acid sequence of the second protein\n",
    "        dialect (str, optional): The dialect to use for AlphaFold. Defaults to 'alphafoldserver'.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary representing the AlphaFold job in the specified format\n",
    "    \"\"\"\n",
    "    job = {\n",
    "        'name': job_name,\n",
    "        'modelSeeds': [],\n",
    "        'sequences': [\n",
    "            {\n",
    "                'proteinChain': {\n",
    "                    'sequence': sequence1,\n",
    "                    'count': 1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'proteinChain': {\n",
    "                    'sequence': sequence2,\n",
    "                    'count': 1\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'dialect': dialect,\n",
    "        'version': 1,\n",
    "    }\n",
    "    return job\n",
    "\n",
    "def create_job_batch_scoreCategories(pair_df: pd.DataFrame, batch_size: int, categories: List[Tuple[float, float]], \n",
    "                                job_dirs: List[str], column_name: str, token_limit: int = 5120) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a new batch of batch_size jobs from pair_df.\n",
    "    \n",
    "    Don't create jobs that have been created previously.\n",
    "    For each category (range of scores) in categories create \n",
    "    batch_size/len(categories) new jobs randomly sampled from all possible jobs in the category.\n",
    "    If a category does not have enough possible jobs to fill the limit, redistribute to the other\n",
    "    categories. Filter jobs that are too large.\n",
    "    \n",
    "    Note: categories should be specified in a way so the category with the largest number of possible jobs comes at the end of the array.\n",
    "    \n",
    "    Args:\n",
    "        pair_df (pd.DataFrame): DataFrame containing protein pairs with sequences and other information\n",
    "        batch_size (int): Total number of jobs to create across all categories\n",
    "        categories (List[Tuple[float, float]]): List of tuples (min_score, max_score) defining score ranges for each category\n",
    "        job_dirs (List[str]): List of directories to search for existing jobs to avoid duplicates\n",
    "        column_name (str): Column name in pair_df for the score to filter on\n",
    "        token_limit (int, optional): Maximum total sequence length for a job. Defaults to 5120.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of newly created AlphaFold job dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If required columns are missing from pair_df\n",
    "    \"\"\"\n",
    "    new_jobs = []\n",
    "    prev_jobs = []\n",
    "    for dir in job_dirs:\n",
    "        prev_jobs += collect_created_jobs(dir)\n",
    "        \n",
    "    for i in range(len(prev_jobs)):\n",
    "        prev_jobs[i] = get_comparable_job(prev_jobs[i], deep_copy=False)\n",
    "    \n",
    "    total_created = 0\n",
    "    num_categories = len(categories)\n",
    "    category_counts = {}  # Dictionary to track jobs created in each category\n",
    "    \n",
    "    for category in categories:\n",
    "        # Stop if we've already created enough jobs\n",
    "        if total_created >= batch_size:\n",
    "            break\n",
    "            \n",
    "        # dynamically adjust the quota for the next category to account for categories that don't fill their quota\n",
    "        cat_quota = math.floor((batch_size-total_created)/num_categories)\n",
    "        \n",
    "        num_categories -= 1\n",
    "        \n",
    "        min_score = category[0]\n",
    "        max_score = category[1]\n",
    "        created_in_category = 0\n",
    "        possible_pairs = pair_df[(pair_df[column_name] >= min_score) & (pair_df[column_name] <= max_score)]\n",
    "        possible_ind = possible_pairs.index.tolist()\n",
    "        \n",
    "        category_key = f\"{min_score}-{max_score}\"\n",
    "        category_counts[category_key] = 0\n",
    "        \n",
    "        while created_in_category < cat_quota and len(possible_ind) > 0:\n",
    "            ind = random.choice(possible_ind)\n",
    "            possible_ind.remove(ind)\n",
    "            \n",
    "            row = pair_df.iloc[ind]\n",
    "            armadillo_entry = row['Entry_arm']\n",
    "            tf_entry = row['Entry_tf']\n",
    "            armadillo_sequence = row['Sequence_arm']\n",
    "            tf_sequence = row['Sequence_tf']\n",
    "            \n",
    "            if len(tf_sequence) + len(armadillo_sequence) > token_limit:\n",
    "                continue\n",
    "            \n",
    "            # Calculate indices for the sequences\n",
    "            armadillo_x, armadillo_y = 1, len(armadillo_sequence)\n",
    "            tf_x, tf_y = 1, len(tf_sequence)\n",
    "\n",
    "            # Generate job name in the specified format\n",
    "            job_name = f\"{armadillo_entry}_{armadillo_x}-{armadillo_y}_{tf_entry}_{tf_x}-{tf_y}\"\n",
    "            \n",
    "            # Create job using the helper function\n",
    "            job = create_alphafold_job(job_name, armadillo_sequence, tf_sequence)\n",
    "            \n",
    "            # check if job was already created earlier\n",
    "            job_comparable = get_comparable_job(job)\n",
    "            if not any(existing_comparable == job_comparable for existing_comparable in prev_jobs):\n",
    "                # no duplicate job found\n",
    "                created_in_category += 1\n",
    "                total_created += 1\n",
    "                category_counts[category_key] += 1\n",
    "                prev_jobs.append(get_comparable_job(job))\n",
    "                new_jobs.append(job)\n",
    "                \n",
    "                # Ensure we don't exceed batch_size\n",
    "                if total_created >= batch_size:\n",
    "                    break\n",
    "        \n",
    "    # Print the number of jobs created in each category\n",
    "    print(f\"Created {len(new_jobs)} new jobs total.\")\n",
    "    print(\"Jobs created per category:\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  Score range {category}: {count} jobs\")\n",
    "    \n",
    "    return new_jobs\n",
    "\n",
    "def create_job_batch_all_pairs(pair_df: pd.DataFrame, batch_size: int, \n",
    "                               job_dirs: List[str], token_limit: int = 5120) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a new batch of AlphaFold jobs by randomly sampling from all protein pairs.\n",
    "    \n",
    "    This function creates a specified number of new AlphaFold jobs by randomly selecting protein pairs\n",
    "    from the input DataFrame. It avoids creating duplicate jobs by checking against existing jobs\n",
    "    in the specified directories and filters out pairs that exceed the token limit.\n",
    "    \n",
    "    Args:\n",
    "        pair_df (pd.DataFrame): DataFrame containing protein pairs with columns:\n",
    "                               - 'Entry_arm': Armadillo protein entry ID\n",
    "                               - 'Entry_tf': Transcription factor protein entry ID  \n",
    "                               - 'Sequence_arm': Armadillo protein sequence\n",
    "                               - 'Sequence_tf': Transcription factor protein sequence\n",
    "        batch_size (int): Total number of new jobs to create\n",
    "        job_dirs (List[str]): List of directories to search for existing jobs to avoid duplicates\n",
    "        token_limit (int, optional): Maximum total sequence length allowed for a job. \n",
    "                                   Pairs with combined sequence length exceeding this limit\n",
    "                                   will be skipped. Defaults to 5120.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of newly created AlphaFold job dictionaries in alphafoldserver format.\n",
    "                             Each job dictionary contains:\n",
    "                             - 'name': Job name in format \"proteinA_start-end_proteinB_start-end\"\n",
    "                             - 'sequences': List of protein chain specifications\n",
    "                             - 'dialect': Set to 'alphafoldserver'\n",
    "                             - 'version': Set to 1\n",
    "                             - 'modelSeeds': Empty list\n",
    "    \n",
    "    Note:\n",
    "        - The function randomly samples pairs without replacement until the batch size is reached\n",
    "        - Pairs exceeding the token limit are automatically skipped\n",
    "        - Duplicate jobs (based on sequence content) are avoided by comparing against existing jobs\n",
    "        - Job names follow the format: \"{armadillo_entry}_1-{seq_length}_{tf_entry}_1-{seq_length}\"\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If required columns are missing from pair_df\n",
    "        IndexError: If pair_df is empty or insufficient pairs available\n",
    "    \"\"\"\n",
    "    new_jobs = []\n",
    "    prev_jobs = []\n",
    "    for dir in job_dirs:\n",
    "        prev_jobs += collect_created_jobs(dir)\n",
    "        \n",
    "    for i in range(len(prev_jobs)):\n",
    "        prev_jobs[i] = get_comparable_job(prev_jobs[i], deep_copy=False)\n",
    "    \n",
    "    possible_ind = pair_df.index.tolist()   \n",
    "    \n",
    "    total_created = 0\n",
    "    while total_created < batch_size:\n",
    "        \n",
    "        ind = random.choice(possible_ind)\n",
    "        possible_ind.remove(ind)\n",
    "            \n",
    "        row = pair_df.iloc[ind]\n",
    "        armadillo_entry = row['Entry_arm']\n",
    "        tf_entry = row['Entry_tf']\n",
    "        armadillo_sequence = row['Sequence_arm']\n",
    "        tf_sequence = row['Sequence_tf']\n",
    "            \n",
    "        if len(tf_sequence) + len(armadillo_sequence) > token_limit:\n",
    "            continue\n",
    "            \n",
    "        # Calculate indices for the sequences\n",
    "        armadillo_x, armadillo_y = 1, len(armadillo_sequence)\n",
    "        tf_x, tf_y = 1, len(tf_sequence)\n",
    "\n",
    "        # Generate job name in the specified format\n",
    "        job_name = f\"{armadillo_entry}_{armadillo_x}-{armadillo_y}_{tf_entry}_{tf_x}-{tf_y}\"\n",
    "        \n",
    "        # Create job using the helper function\n",
    "        job = create_alphafold_job(job_name, armadillo_sequence, tf_sequence)\n",
    "        \n",
    "        # check if job was already created earlier\n",
    "        job_comparable = get_comparable_job(job)\n",
    "        if not any(existing_comparable == job_comparable for existing_comparable in prev_jobs):\n",
    "            # no duplicate job found\n",
    "            total_created += 1\n",
    "            prev_jobs.append(get_comparable_job(job))\n",
    "            new_jobs.append(job)\n",
    "        \n",
    "    # Print the number of jobs created in each category\n",
    "    print(f\"Created {len(new_jobs)} new jobs total.\")\n",
    "    \n",
    "    return new_jobs\n",
    "\n",
    "def create_job_batch_id_list(pair_df: pd.DataFrame, id_list: List[Tuple[str, str]], \n",
    "                               job_dirs: List[str], token_limit: int = 5120) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create a batch of AlphaFold jobs from a specific list of protein ID pairs.\n",
    "    \n",
    "    This function creates AlphaFold jobs for specific protein pairs identified by their UniProt IDs.\n",
    "    It avoids creating duplicate jobs by checking against existing jobs in the specified directories.\n",
    "    \n",
    "    Args:\n",
    "        pair_df (pd.DataFrame): DataFrame containing protein pairs with columns:\n",
    "                               - 'Entry_arm': Armadillo protein entry ID\n",
    "                               - 'Entry_tf': Transcription factor protein entry ID  \n",
    "                               - 'Sequence_arm': Armadillo protein sequence\n",
    "                               - 'Sequence_tf': Transcription factor protein sequence\n",
    "        id_list (List[Tuple[str, str]]): List of tuples containing protein ID pairs to create jobs for\n",
    "        job_dirs (List[str]): List of directories to search for existing jobs to avoid duplicates\n",
    "        token_limit (int, optional): Maximum total sequence length allowed for a job. \n",
    "                                   Pairs with combined sequence length exceeding this limit\n",
    "                                   will be skipped. Defaults to 5120.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of newly created AlphaFold job dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If no matching row is found for a pair or multiple matching rows are found\n",
    "        KeyError: If required columns are missing from pair_df\n",
    "    \"\"\"\n",
    "    new_jobs = []\n",
    "    prev_jobs = []\n",
    "    for dir in job_dirs:\n",
    "        prev_jobs += collect_created_jobs(dir)\n",
    "        \n",
    "    for i in range(len(prev_jobs)):\n",
    "        prev_jobs[i] = get_comparable_job(prev_jobs[i], deep_copy=False)\n",
    "        \n",
    "    total_created = 0\n",
    "    \n",
    "    for (id_1, id_2) in id_list:\n",
    "        \n",
    "        row_matches = pair_df[((pair_df['Entry_arm'] == id_1) & (pair_df['Entry_tf'] == id_2)) | \n",
    "                              ((pair_df['Entry_arm'] == id_2) & (pair_df['Entry_tf'] == id_1))]\n",
    "        if len(row_matches) == 0:\n",
    "            raise Exception(f\"No matching row found for pair ({id_1}, {id_2}) in pair_df.\")\n",
    "        elif len(row_matches) > 1:\n",
    "            raise Exception(f\"Multiple matching rows found for pair ({id_1}, {id_2}) in pair_df.\")\n",
    "        \n",
    "        row = row_matches.iloc[0]\n",
    "        armadillo_entry = row['Entry_arm']\n",
    "        tf_entry = row['Entry_tf']\n",
    "        armadillo_sequence = row['Sequence_arm']\n",
    "        tf_sequence = row['Sequence_tf']\n",
    "            \n",
    "        if len(tf_sequence) + len(armadillo_sequence) > token_limit:\n",
    "            print(f\"Skipping because of token limit: {armadillo_entry}-{tf_entry}\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate indices for the sequences\n",
    "        armadillo_x, armadillo_y = 1, len(armadillo_sequence)\n",
    "        tf_x, tf_y = 1, len(tf_sequence)\n",
    "\n",
    "        # Generate job name in the specified format\n",
    "        job_name = f\"{armadillo_entry}_{armadillo_x}-{armadillo_y}_{tf_entry}_{tf_x}-{tf_y}\"\n",
    "        \n",
    "        # Create job using the helper function\n",
    "        job = create_alphafold_job(job_name, armadillo_sequence, tf_sequence)\n",
    "        \n",
    "        # check if job was already created earlier\n",
    "        job_comparable = get_comparable_job(job)\n",
    "        if not any(existing_comparable == job_comparable for existing_comparable in prev_jobs):\n",
    "            # no duplicate job found\n",
    "            total_created += 1\n",
    "            prev_jobs.append(get_comparable_job(job))\n",
    "            new_jobs.append(job)\n",
    "        \n",
    "    # Print the number of jobs created in each category\n",
    "    print(f\"Created {len(new_jobs)} new jobs total.\")\n",
    "    \n",
    "    return new_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### create job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "### STRING\n",
    "# note that the order is important. The category (100,200) is very large so it comes last to fill up the remaining jobs\n",
    "categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (100,200), (0,100), (200,300)]\n",
    "new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS, STRING_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### IntAct\n",
    "# categories = [(0.1,0.2), (0.9,1), (0.8,0.9), (0.7,0.8), (0.6,0.7), (0.5,0.6), (0.4,0.5), (0.2,0.3), (0.3,0.4)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_IntAct, BATCH_SIZE, categories, BATCH_DIRS, INTACT_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### all pairs\n",
    "# new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "### ID list:\n",
    "# id_list_good = [\n",
    "#     (\"Q13285\", \"A0A2R8YCH5\"),\n",
    "#     (\"P04637\", \"A0A8I5KU01\"),\n",
    "#     (\"P04637\", \"A0A8I5KU01\"),\n",
    "#     (\"Q9H3D4\", \"A0A8I5KU01\"),\n",
    "#     (\"Q8NHM5\", \"A1YPR0\"),\n",
    "#     (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "#     (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"Q9NRY4\", \"O00750\"),\n",
    "#     (\"Q6ZRS2\", \"A0A8V8TQN3\")\n",
    "# ]\n",
    "\n",
    "# id_list_complex = [\n",
    "#     (\"Q03181\", \"Q9H3U1\"),\n",
    "#     (\"P04637\", \"A0A994J4J0\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"Q9UBG7\", \"A0A8I5KU01\"),\n",
    "#     (\"P19838\", \"A0A1W2PRG6\")\n",
    "# ]\n",
    "\n",
    "# new_af_jobs = create_job_batch_id_list(all_pairs, id_list_complex, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/AF_job_batches/batch_43')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b2c0c",
   "metadata": {},
   "source": [
    "### Verify result completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hpc_results(hpc_results_dir: str, required_files_template: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Verify that all required files are present in each subfolder of the HPC_results directory.\n",
    "    \n",
    "    This function checks each job folder in the HPC results directory to ensure that all expected\n",
    "    output files have been generated correctly.\n",
    "\n",
    "    Args:\n",
    "        hpc_results_dir (str): Path to the HPC_results directory.\n",
    "        required_files_template (List[str]): List of required file names with 'JOB_NAME' as a placeholder.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary with job names as keys and a list of missing files as values.\n",
    "    \"\"\"\n",
    "    missing_files_report = {}\n",
    "\n",
    "    for job_folder in os.listdir(hpc_results_dir):\n",
    "        job_path = os.path.join(hpc_results_dir, job_folder)\n",
    "\n",
    "        if not os.path.isdir(job_path):\n",
    "            continue\n",
    "\n",
    "        # Replace 'JOB_NAME' in the template with the actual job name\n",
    "        required_files = [file.replace(\"JOB_NAME\", job_folder) for file in required_files_template]\n",
    "\n",
    "        missing_files = [file for file in required_files if not os.path.exists(os.path.join(job_path, file))]\n",
    "\n",
    "        if missing_files:\n",
    "            missing_files_report[job_folder] = missing_files\n",
    "\n",
    "    return missing_files_report\n",
    "\n",
    "def missing_files_report() -> None:\n",
    "    \"\"\"Generate a report of missing files in the HPC results directory.\n",
    "    \n",
    "    This function checks for all required output files from AlphaFold jobs and reports any missing files.\n",
    "    \"\"\"\n",
    "    required_files_template = [\n",
    "        \"JOB_NAME_confidences.json\",\n",
    "        \"JOB_NAME_data.json\",\n",
    "        \"JOB_NAME_model.cif\",\n",
    "        \"JOB_NAME_summary_confidences.json\",\n",
    "        \"ranking_scores.csv\",\n",
    "        \"seed-1_sample-0\",\n",
    "        \"seed-1_sample-1\",\n",
    "        \"seed-1_sample-2\",\n",
    "        \"seed-1_sample-3\",\n",
    "        \"seed-1_sample-4\",\n",
    "    ]\n",
    "    report = verify_hpc_results(HPC_RESULT_DIR, required_files_template)\n",
    "    if report:\n",
    "        print(\"Missing files detected:\")\n",
    "        for job, files in report.items():\n",
    "            print(f\"Job: {job}, Missing Files: {files}\")\n",
    "    else:\n",
    "        print(\"All files are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed083c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_files_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143d563",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a644",
   "metadata": {},
   "source": [
    "### Negatomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a83e",
   "metadata": {},
   "source": [
    "#### IntAct Negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intact_negative = pd.read_csv('../../data/IntAct/human/human_negative.txt', sep='\\t')\n",
    "# intact_negative.drop(['Alias(es) interactor A', \n",
    "#                      'Alias(es) interactor B', \n",
    "#                      'Interaction detection method(s)',\n",
    "#                      'Publication 1st author(s)',\n",
    "#                      'Publication Identifier(s)',\n",
    "#                      'Taxid interactor A',\n",
    "#                      'Taxid interactor B',\n",
    "#                      'Biological role(s) interactor A',\n",
    "#                      'Biological role(s) interactor B',\n",
    "#                      'Experimental role(s) interactor A',\n",
    "#                      'Experimental role(s) interactor B',\n",
    "#                      'Type(s) interactor A',\n",
    "#                      'Type(s) interactor B',\n",
    "#                      'Xref(s) interactor A',\n",
    "#                      'Xref(s) interactor B',\n",
    "#                      'Interaction Xref(s)',\n",
    "#                      'Annotation(s) interactor A',\n",
    "#                      'Annotation(s) interactor B',\n",
    "#                      'Interaction annotation(s)',\n",
    "#                      'Host organism(s)',\n",
    "#                      'Interaction parameter(s)',\n",
    "#                      'Creation date',\n",
    "#                      'Update date',\n",
    "#                      'Checksum(s) interactor A',\n",
    "#                      'Checksum(s) interactor B',\n",
    "#                      'Interaction Checksum(s)',\n",
    "#                      'Feature(s) interactor A',\n",
    "#                      'Feature(s) interactor B',\n",
    "#                      'Stoichiometry(s) interactor A',\n",
    "#                      'Stoichiometry(s) interactor B',\n",
    "#                      'Identification method participant A',\n",
    "#                      'Identification method participant B',\n",
    "#                      'Expansion method(s)'\n",
    "#                      ], axis=1, inplace=True)\n",
    "# intact_negative.loc[:, 'intact_score'] = intact_negative.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "# intact_negative['pair_id'] = intact_negative.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', '').split('-')[0], row['ID(s) interactor B'].replace('uniprotkb:', '').split('-')[0]]))), axis=1)\n",
    "# intact_negative = intact_negative.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "# all_pairs_intact_negative = pd.merge(all_pairs, intact_negative, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_intact_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4c239",
   "metadata": {},
   "source": [
    "#### Blohm negatome2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatome2 = pd.read_csv('../../data/negatome2.0/combined.txt', sep='\\t', names=['ID_1', 'ID_2'])\n",
    "# negatome2['pair_id'] = negatome2.apply(lambda row: str(tuple(sorted([row['ID_1'].split('-')[0], row['ID_2'].split('-')[0]]))), axis=1)\n",
    "# all_pairs_negatome2 = pd.merge(all_pairs, negatome2, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_negatome2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63893e",
   "metadata": {},
   "source": [
    "#### Stelzl 2005 negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stelzl_neg = pd.read_csv('../../data/16169070_neg.mitab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Read in HPC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df_uc = pd.DataFrame(data=find_summary_files(HPC_RESULT_DIR))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df_uc)}\")\n",
    "\n",
    "results_df_uc['pair_id'] = results_df_uc.apply(create_pair_id, axis=1)\n",
    "\n",
    "print(f\"jobs before cleaning: {len(results_df_uc)}\")\n",
    "results_df = clean_results(results_df_uc)\n",
    "print(f\"jobs after cleaning: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, intact_cleaned, on='pair_id', how='left')\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total number of modelled pairs: {len(results_df)}\")\n",
    "print(f\"Total rows in merged_df: {len(results_df_annotated)}\")\n",
    "print(f\"Rows with annotated data (STRING): {results_df_annotated['combined_score'].notna().sum()}\")\n",
    "print(f\"Rows with annotated data (IntAct): {results_df_annotated['intact_score'].notna().sum()}\")\n",
    "\n",
    "\n",
    "# convert all STRING scores from 0-1000 to 0-1 (linear conversion)\n",
    "STRING_COLS = ['experimental', 'database', 'textmining', 'combined_score']\n",
    "for col in STRING_COLS:\n",
    "    results_df_annotated[col] = results_df_annotated[col] / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_plot_colour(df: pd.DataFrame, x_metric: str, y_metric: str, color_metric: str, \n",
    "                        title: str = '', cmap: str = 'viridis', alpha: float = 0.7, size: int = 50, ax=None) -> None:\n",
    "    \"\"\"Create a scatter plot with two metrics and color by a third metric.\n",
    "    \n",
    "    This function creates a scatter plot between two specified metrics with points colored\n",
    "    by a third metric.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics to plot\n",
    "        x_metric (str): Name of the column to plot on x-axis\n",
    "        y_metric (str): Name of the column to plot on y-axis\n",
    "        color_metric (str): Name of the column to use for point colors\n",
    "        title (str, optional): Custom title for the plot. Defaults to ''.\n",
    "        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n",
    "        alpha (float, optional): Transparency of points. Defaults to 0.7.\n",
    "        size (int, optional): Point size. Defaults to 50.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If None, creates new figure.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        x=df[x_metric], \n",
    "        y=df[y_metric], \n",
    "        c=df[color_metric],\n",
    "        cmap=cmap,\n",
    "        alpha=alpha,\n",
    "        s=size,\n",
    "        edgecolors='w'  # White edge to make points stand out\n",
    "    )\n",
    "    \n",
    "    if title == '':\n",
    "        title = f'{y_metric} vs {x_metric}\\nColored by {color_metric}'\n",
    "\n",
    "    # Add a color bar to show the scale of the color metric\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(color_metric, fontsize=10)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_metric, fontsize=12)\n",
    "    ax.set_ylabel(y_metric, fontsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add number of datapoints as a text box in the upper left corner\n",
    "    num_points = len(df)\n",
    "    ax.annotate(f'Datapoints: {num_points}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36814cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_plot(df: pd.DataFrame, x_metric: str, y_metric: str,\n",
    "                        title: str = '', alpha: float = 0.7, size: int = 50, ax=None) -> None:\n",
    "    \"\"\"Create a scatter plot with two metrics.\n",
    "    \n",
    "    This function creates a scatter plot between two specified metrics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics to plot\n",
    "        x_metric (str): Name of the column to plot on x-axis\n",
    "        y_metric (str): Name of the column to plot on y-axis\n",
    "        title (str, optional): Custom title for the plot. Defaults to ''.\n",
    "        alpha (float, optional): Transparency of points. Defaults to 0.7.\n",
    "        size (int, optional): Point size. Defaults to 50.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If None, creates new figure.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        x=df[x_metric], \n",
    "        y=df[y_metric], \n",
    "        alpha=alpha,\n",
    "        s=size,\n",
    "        edgecolors='w'  # White edge to make points stand out\n",
    "    )\n",
    "    \n",
    "    if title == '':\n",
    "        title = f'{y_metric} vs {x_metric}'\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_metric, fontsize=12)\n",
    "    ax.set_ylabel(y_metric, fontsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add number of datapoints as a text box in the upper left corner\n",
    "    num_points = len(df)\n",
    "    ax.annotate(f'Datapoints: {num_points}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6969fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs treated as 0', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments), NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments); NAs included', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'iptm', INTACT_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'ptm', INTACT_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', 'iptm', INTACT_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by IntAct Score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated['avg_STRING_IntAct'] = (results_df_annotated[STRING_SCORE_COLUMN] + results_df_annotated[INTACT_SCORE_COLUMN]) / 2 \n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'iptm', 'avg_STRING_IntAct', 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'ptm', 'avg_STRING_IntAct', 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ptm', 'iptm', 'avg_STRING_IntAct', 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by average of STRING and IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb91f86",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88025a",
   "metadata": {},
   "source": [
    "creating AUC curves with STRING / IntAct alone does not really make sense. My understanding is that both include only **positive** interaction candidates and assign scores to the certainty. So even a low score means a relatively high probability of interaction because the candidate is in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def plot_roc(df: pd.DataFrame, pos: Set[str], neg: Set[str], param_name: str, min_val: float, max_val: float, \n",
    "             sampling: int = 1000, direction: str = 'up', plot_title: str = '', ax=None) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Calculate and plot the ROC curve based on a specified parameter.\n",
    "    \n",
    "    This function evaluates the performance of a binary classifier by varying a threshold parameter\n",
    "    and calculating the True Positive Rate (TPR) and False Positive Rate (FPR) at each threshold.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the parameter to evaluate and pair_id column\n",
    "        pos (Set[str]): Set of positive example pair_ids (ground truth positive cases)\n",
    "        neg (Set[str]): Set of negative example pair_ids (ground truth negative cases)\n",
    "        param_name (str): Name of the column in df to use as the classification parameter\n",
    "        min_val (float): Minimum threshold value to evaluate\n",
    "        max_val (float): Maximum threshold value to evaluate\n",
    "        sampling (int, optional): Number of threshold points to sample between min and max. Defaults to 1000.\n",
    "        direction (str, optional): Direction of classification - 'up' means values >= threshold are positive,\n",
    "                                  'down' means values < threshold are positive. Defaults to 'up'.\n",
    "        plot_title (str, optional): Title for the ROC curve plot. If empty, a default title is used. Defaults to ''.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If None, creates new figure.\n",
    "                                   \n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Lists of FPR and TPR values that make up the ROC curve\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        # Print the size of positive and negative sets for verification    \n",
    "        print(f\"Number of positive examples: {len(pos)}\")\n",
    "        print(f\"Number of negative examples: {len(neg)}\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "\n",
    "    # Calculate step size based on range and sampling\n",
    "    step = (max_val - min_val) / sampling\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"max_val must be greater than min_val\")\n",
    "    \n",
    "    # Lists to store TPR and FPR values\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    \n",
    "    for i in range(sampling + 1):\n",
    "        sep_val = min_val + (i * step)\n",
    "        \n",
    "        if direction == 'up':\n",
    "            calc_pos = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "        elif direction == 'down':\n",
    "            calc_pos = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "        else:\n",
    "            raise ValueError(\"direction must be either 'up' or 'down'\")\n",
    "        \n",
    "        # Calculate confusion matrix values\n",
    "        TP_num = len(calc_pos.intersection(pos))\n",
    "        FP_num = len(calc_pos.intersection(neg))\n",
    "        TN_num = len(calc_neg.intersection(neg))\n",
    "        FN_num = len(calc_neg.intersection(pos))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        TPR = TP_num / max(TP_num + FN_num, 1)\n",
    "        FPR = FP_num / max(FP_num + TN_num, 1)\n",
    "        \n",
    "        tpr_list.append(TPR)\n",
    "        fpr_list.append(FPR)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    ax.plot(fpr_list, tpr_list, 'b-', linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2)  # Diagonal line representing random guess\n",
    "    \n",
    "    auc_value = sklearn.metrics.auc(fpr_list, tpr_list)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    \n",
    "    if plot_title:\n",
    "        title = plot_title\n",
    "    else:\n",
    "        title = f'ROC Curve for {param_name}'\n",
    "    \n",
    "    ax.set_title(f'{title}\\nAUC = {auc_value:.3f}', fontsize=12)\n",
    "    \n",
    "    # Add grid and improve appearance\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fpr_list, tpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRING_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# string_combined_pos = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] >= STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# string_combined_neg = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] < STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(string_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(string_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (STRING cutoff >= {STRING_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152712a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTACT_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# intact_combined_pos = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] >= INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# intact_combined_neg = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] < INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(intact_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(intact_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (IntAct cutoff >= {INTACT_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
