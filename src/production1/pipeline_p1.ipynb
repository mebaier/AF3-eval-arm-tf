{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "## General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.physical.links.detailed.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "# PROTEOME_PATH = '/home/markus/MPI_local/data/full_UP/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uniprot = pd.read_csv(PROTEOME_PATH, low_memory=False, sep='\\t')\n",
    "uniprot_filtered = all_uniprot[all_uniprot['Reviewed'] == 'reviewed']\n",
    "# uniprot_filtered = all_uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip version number since it is not included in the uniprot annotation\n",
    "arm_accs_pfam_stripped = [acc.split(\".\")[0] for acc in arm_accs_pfam]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = uniprot_filtered['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(uniprot_filtered[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = uniprot_filtered['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(uniprot_filtered[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = uniprot_filtered['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam_stripped))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(uniprot_filtered[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = uniprot_filtered[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "# TODO: use caching\n",
    "# TODO: adjust ensemble mapping if using other proteome?\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = uniprot_filtered[uniprot_filtered['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))\n",
    "\n",
    "# 4m 1.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds = add_iupred3(tf_proteins_curated_ds, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f\"Number of transcription factors with at least one disordered region (IUPred3, threshold={IUPRED3_THRESHOLD}, min length={MIN_LENGTH_DISORDERED_REGION}): {len(tf_proteins_curated_ds_IUPred3_diso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_over_token_limit = len(all_pairs[all_pairs['Length_arm'] + all_pairs['Length_tf'] > AF_TOKEN_LIMIT])\n",
    "print(f\"Pairs over token limit: {num_pairs_over_token_limit} ({(num_pairs_over_token_limit/len(all_pairs))*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_STRING = len(all_pairs_w_STRING)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {num_all_pairs_w_STRING} ({(num_all_pairs_w_STRING/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_intact import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned = read_clean_intact('../../data/IntAct/human/human.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_cleaned, on='pair_id', how='inner')\n",
    "\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_IntAct = len(all_pairs_w_IntAct)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {num_all_pairs_w_IntAct} ({(num_all_pairs_w_IntAct/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_intersect_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "\n",
    "print(len(all_pairs_intersect_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_union_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='outer')\n",
    "\n",
    "print(len(all_pairs_union_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd617a",
   "metadata": {},
   "source": [
    "### write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in armadillo_proteins.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/arm_all_uniprot_rev_fasta', row['Reviewed'])\n",
    "# for idx, row in tf_proteins_curated_ds_IUPred3_diso.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/tf_all_uniprot_rev_fasta', row['Reviewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armadillo_proteins.to_csv('../../armadillo_proteins.csv', index=False)\n",
    "# tf_proteins_curated_ds_IUPred3_diso.to_csv('../../transcription_factors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f20999",
   "metadata": {},
   "source": [
    "## 1.1 PDB reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff831cb6",
   "metadata": {},
   "source": [
    "### imports an functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import download_functions\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(download_functions)\n",
    "importlib.reload(functions_plotting)\n",
    "\n",
    "# Step 2: Re-import everything you need\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from download_functions import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_report_arm_filter(pdb_report: pd.DataFrame, armadillo_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter entries that have at least one ARM, add column isARM = True|False\n",
    "    keep_pdbs = set()\n",
    "    armadillo_entries = armadillo_proteins['Entry'].tolist()\n",
    "    pdb_report['isARM'] = False\n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in armadillo_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isARM'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_tf_filter(pdb_report: pd.DataFrame, tf_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep_pdbs = set()\n",
    "    tf_entries = tf_proteins['Entry'].tolist()\n",
    "    pdb_report['isDisoTF'] = False\n",
    "    \n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in tf_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isDisoTF'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_disorder_filter(pdb_report: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter for entries that have at least one protein that is not ARm and has a disordered region\n",
    "    keep_pdbs = set()\n",
    "\n",
    "    for _, row in pdb_report.iterrows():\n",
    "        if row['isARM'] == False and row['num_disordered_regions'] > 0:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_AF_metrics(report_df: pd.DataFrame, results_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Annotate a report DataFrame with AlphaFold metrics from job results.\n",
    "    \n",
    "    This function takes a report DataFrame and annotates it with AlphaFold metrics\n",
    "    (iptm, ptm, ranking_score) by matching entries with completed jobs in the results directory.\n",
    "    \n",
    "    Args:\n",
    "        report_df (pd.DataFrame): DataFrame to annotate, should contain columns needed to construct job names\n",
    "        results_dir (str): Directory containing AlphaFold job results\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Report DataFrame with added af_iptm, af_ptm, af_ranking_score columns\n",
    "    \"\"\"\n",
    "    report_df['job_name'] = report_df['job_name'].apply(lambda x: str(x).lower() if not pd.isna(x) else x)\n",
    "    results_df = pd.DataFrame(data=find_summary_files(results_dir))\n",
    "    results_df = clean_results(results_df)\n",
    "    \n",
    "    report_df = report_df.merge(results_df, on='job_name')\n",
    "    \n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d065de",
   "metadata": {},
   "source": [
    "### report 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_1 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/1/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760715e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entry IDs from both datasets\n",
    "pdb_entry_ids = set(pdb_report_1['Accession Code(s)'].dropna())\n",
    "tf_entry_ids = set(tf_proteins_curated_ds_IUPred3_diso['Entry'])\n",
    "\n",
    "# Find intersection\n",
    "common_entries = pdb_entry_ids.intersection(tf_entry_ids)\n",
    "\n",
    "print(f\"Number of Entry IDs in pdb_report_1_two_seq: {len(pdb_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs in tf_proteins_curated_ds: {len(tf_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs that appear in both datasets: {len(common_entries)}\")\n",
    "print(f\"Percentage of PDB entries that are also TFs: {len(common_entries)/len(pdb_entry_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only entries where at least one Uniprot ID is in list of disordered TFs\n",
    "pdb_report_1 = pdb_report_tf_filter(pdb_report_1, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_1 = pdb_report_arm_filter(pdb_report_1, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "# pdb_report_1 = add_iupred3(pdb_report_1, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08220f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_1['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_1_seq2 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pairs where ARM and disordered TF is the same protein\n",
    "keep_pdbs = set()\n",
    "\n",
    "for ind, row in pdb_report_1.iterrows():\n",
    "    if not (row['isARM'] == True and row['isDisoTF'] == True):\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "\n",
    "pdb_report_1 = pdb_report_1[pdb_report_1['Entry ID'].isin(keep_pdbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "# pdb_report_1 = pdb_report_disorder_filter(pdb_report_1_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577268e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_1['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f620f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_appearing_3 = entry_counts[entry_counts == 3].index.tolist()\n",
    "pdb_report_1_seq3 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68903f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs by separating into arm and tf half\n",
    "report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "# report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True)]\n",
    "# report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False)]\n",
    "\n",
    "report_1_pairs = pd.merge(left=report_1_tf, right=report_1_arm, on='Entry ID', suffixes=['_tf', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_1_pairs = report_1_pairs.rename(columns={'Entry ID': 'pdb_id'})\n",
    "\n",
    "# print_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "\n",
    "report_1_pairs = append_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d904a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_1_pairs_sorted = report_1_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_1_pairs_sorted.iterrows():\n",
    "    pdb_id = row['pdb_id']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_tf'] + \" \" + row['Release Date_arm']  # Using tf release date\n",
    "    job_name = row.get('job_name', 'N/A')  # Use get() in case column doesn't exist\n",
    "\n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\\t\\t{job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabafded",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1_pairs = annotate_AF_metrics(report_1_pairs, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_1_pairs, 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ptm', 'dockq_score', ax=axes[1],corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2b91d",
   "metadata": {},
   "source": [
    "### report 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_2 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/2/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_2 = pdb_report_arm_filter(pdb_report_2, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "pdb_report_2 = add_iupred3(pdb_report_2, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_2['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_2_seq2 = pdb_report_2[pdb_report_2['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d383e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "pdb_report_2_seq2 = pdb_report_disorder_filter(pdb_report_2_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39830797",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_2_seq2['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_arm = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == True) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_2_diso = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == False) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "\n",
    "report_2_pairs = pd.merge(left=report_2_diso, right=report_2_arm, on='Entry ID', suffixes=['_diso', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_2_pairs.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "report_2_pairs = append_dockq(report_2_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "report_2_pairs.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74224994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_2_pairs_sorted = report_2_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_2_pairs_sorted.iterrows():\n",
    "    pdb_id = row['Entry ID']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_diso']\n",
    "    \n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs = annotate_AF_metrics(report_2_pairs, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4560582",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs['in_training_set'] = report_2_pairs['Release Date_diso'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot_colour(report_2_pairs, 'iptm', 'dockq_score', 'in_training_set', ax=axes[0])\n",
    "create_scatter_plot_colour(report_2_pairs, 'ptm', 'dockq_score', 'in_training_set', ax=axes[1])\n",
    "create_scatter_plot(report_2_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71de96",
   "metadata": {},
   "source": [
    "### job creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "\n",
    "# IMPORTANT: check for duplicates that were modelled in production1! They may need to be downloaded extra\n",
    "\n",
    "job_list = create_job_list_from_filtered_report(pdb_report_2_seq2)\n",
    "new_af_jobs = create_job_batch_sequences_dict(job_list, BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98359aaa",
   "metadata": {},
   "source": [
    "## 1.2 Hadeer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83082c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadeer_df = pd.read_csv('/home/markus/MPI_local/downloads/transcription_factors_pdbs.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def check_interface(pdb_id, chain_X, chain_Y, data_dir, min_atoms=10, max_distance=5) -> bool:\n",
    "    \"\"\"check if in the specified pdb there is an interface between chain_A and chain_B\n",
    "    using Gregors data\n",
    "\n",
    "    Args:\n",
    "        pdb_id (_type_): _description_\n",
    "        chain_A (_type_): _description_\n",
    "        chain_B (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        bool: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    # find files matching pattern and check for atom pairs within 5A between the two chains\n",
    "    files = list(Path(data_dir).rglob(f\"{str.upper(pdb_id)}_detailed_interactions.csv\"))\n",
    "\n",
    "    if len(files) > 1:\n",
    "        print(f\"ERROR: for {pdb_id}, multiple files were found.\")\n",
    "        return False\n",
    "    elif len(files) == 0:\n",
    "        print(f\"ERROR: for {pdb_id}, no files were found.\")\n",
    "        return False\n",
    "        \n",
    "    df = pd.read_csv(files[0], sep=',')\n",
    "    \n",
    "    atom_counter = 0\n",
    "    for _,row in df.iterrows():\n",
    "        if (row['Chain_A'] == chain_X and row['Chain_B'] == chain_Y) or (row['Chain_A'] == chain_Y and row['Chain_B'] == chain_X):\n",
    "            if row['Distance'] <= max_distance:\n",
    "                atom_counter += 1\n",
    "\n",
    "    return atom_counter >= min_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba571061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadeer_df.drop(columns=['Protein.names', 'Gene.Names', 'Organism', 'Length', 'InterPro', 'Pfam', 'DisProt', 'STRING', 'IntAct', 'Ensembl', 'Repeat', 'HGNC'])\n",
    "hadeer_df.rename(columns={'chain': 'chain_tf', 'Entry': 'Entry_tf'}, inplace=True)\n",
    "\n",
    "# add for each pdb the other chains and other pdbs\n",
    "# find out which chain is ARM\n",
    "# check interaction between chain_tf and chain_arm\n",
    "\n",
    "hadeer_df['chains_arm'] = None\n",
    "hadeer_df['Entrys_arm'] = None\n",
    "\n",
    "for ind,row in hadeer_df.iterrows():\n",
    "\n",
    "    pdb_id = row['pdb']\n",
    "    if pd.isna(pdb_id):\n",
    "        continue\n",
    "\n",
    "    # normalize pdb id and ensure chain is present\n",
    "    pdb_id = str(pdb_id).upper().strip()\n",
    "    chain_tf = row['chain_tf']\n",
    "    if pd.isna(chain_tf):\n",
    "        continue\n",
    "\n",
    "    all_pdbs = get_pdb_chains_to_uniprot(pdb_id, '../../production1/mapping_cache')\n",
    "    \n",
    "    arm_pairs = []\n",
    "    \n",
    "    for chain, id in all_pdbs.items():\n",
    "        if id in armadillo_proteins['Entry'].tolist():\n",
    "            arm_pairs.append((chain, id))\n",
    "            \n",
    "    \n",
    "    if len(arm_pairs) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        hadeer_df.at[ind, 'chains_arm'] = [pair[0] for pair in arm_pairs]\n",
    "        hadeer_df.at[ind, 'Entrys_arm'] = [pair[1] for pair in arm_pairs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define interface as having at least n atoms within 5 A of each other \n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "# Check interface between TF chain and ARM chain\n",
    "hadeer_df['arm_tf_interface'] = False  # Initialize with False\n",
    "valid_rows = hadeer_df[hadeer_df['pdb'].notna() & hadeer_df['chain_tf'].notna() & hadeer_df['chains_arm'].notna()]\n",
    "\n",
    "for ind in valid_rows.index:\n",
    "    pdb_id = str(valid_rows.at[ind, 'pdb']).upper().strip()\n",
    "    chain_tf = valid_rows.at[ind, 'chain_tf']\n",
    "    chains_arm = valid_rows.at[ind, 'chains_arm']\n",
    "    \n",
    "    for chain_arm in chains_arm:\n",
    "        if check_interface(pdb_id, chain_tf, chain_arm, '/home/markus/MPI_local/data/PDB2Net', INTERFACE_MIN_ATOMS):\n",
    "            hadeer_df.at[ind, 'arm_tf_interface'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique()))\n",
    "display(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e803956",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "new_af_jobs = create_job_batch_from_PDB_IDs(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique().tolist(), BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### create job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'\n",
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "### STRING\n",
    "# note that the order is important. The category (100,200) is very large so it comes last to fill up the remaining jobs\n",
    "# categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (100,200), (0,100), (200,300)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS, STRING_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### IntAct\n",
    "# categories = [(0.1,0.2), (0.9,1), (0.8,0.9), (0.7,0.8), (0.6,0.7), (0.5,0.6), (0.4,0.5), (0.2,0.3), (0.3,0.4)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_IntAct, BATCH_SIZE, categories, BATCH_DIRS, INTACT_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### all pairs\n",
    "new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "### ID list:\n",
    "id_list_good = [\n",
    "    (\"Q13285\", \"A0A2R8YCH5\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"Q9H3D4\", \"A0A8I5KU01\"),\n",
    "    (\"Q8NHM5\", \"A1YPR0\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q6SJ96\", \"O14981\"),\n",
    "    (\"Q9NRY4\", \"O00750\"),\n",
    "    (\"Q6ZRS2\", \"A0A8V8TQN3\")\n",
    "]\n",
    "\n",
    "# id_list_complex = [\n",
    "#     (\"Q03181\", \"Q9H3U1\"),\n",
    "#     (\"P04637\", \"A0A994J4J0\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"Q9UBG7\", \"A0A8I5KU01\"),\n",
    "#     (\"P19838\", \"A0A1W2PRG6\")\n",
    "# ]\n",
    "\n",
    "# missing = [('Q13285', 'Q6BTZ4'), ('P04637', 'Q9VL06'), ('P04637', 'Q9VL06'), ('Q9UIF8', 'Q54U63'), ('Q15047', 'Q54U63'), ('Q15047', 'Q5R881'), ('Q9H3D4', 'Q9VL06'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('Q6ZRS2', 'P38811'), ('Q6ZRS2', 'P38811')]\n",
    "# new_af_jobs = create_job_batch_id_list(all_pairs, missing, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/AF_job_batches/batch_55')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143d563",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a644",
   "metadata": {},
   "source": [
    "### Negatomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a83e",
   "metadata": {},
   "source": [
    "#### IntAct Negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intact_negative = pd.read_csv('../../data/IntAct/human/human_negative.txt', sep='\\t')\n",
    "# intact_negative.drop(['Alias(es) interactor A', \n",
    "#                      'Alias(es) interactor B', \n",
    "#                      'Interaction detection method(s)',\n",
    "#                      'Publication 1st author(s)',\n",
    "#                      'Publication Identifier(s)',\n",
    "#                      'Taxid interactor A',\n",
    "#                      'Taxid interactor B',\n",
    "#                      'Biological role(s) interactor A',\n",
    "#                      'Biological role(s) interactor B',\n",
    "#                      'Experimental role(s) interactor A',\n",
    "#                      'Experimental role(s) interactor B',\n",
    "#                      'Type(s) interactor A',\n",
    "#                      'Type(s) interactor B',\n",
    "#                      'Xref(s) interactor A',\n",
    "#                      'Xref(s) interactor B',\n",
    "#                      'Interaction Xref(s)',\n",
    "#                      'Annotation(s) interactor A',\n",
    "#                      'Annotation(s) interactor B',\n",
    "#                      'Interaction annotation(s)',\n",
    "#                      'Host organism(s)',\n",
    "#                      'Interaction parameter(s)',\n",
    "#                      'Creation date',\n",
    "#                      'Update date',\n",
    "#                      'Checksum(s) interactor A',\n",
    "#                      'Checksum(s) interactor B',\n",
    "#                      'Interaction Checksum(s)',\n",
    "#                      'Feature(s) interactor A',\n",
    "#                      'Feature(s) interactor B',\n",
    "#                      'Stoichiometry(s) interactor A',\n",
    "#                      'Stoichiometry(s) interactor B',\n",
    "#                      'Identification method participant A',\n",
    "#                      'Identification method participant B',\n",
    "#                      'Expansion method(s)'\n",
    "#                      ], axis=1, inplace=True)\n",
    "# intact_negative.loc[:, 'intact_score'] = intact_negative.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "# intact_negative['pair_id'] = intact_negative.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', '').split('-')[0], row['ID(s) interactor B'].replace('uniprotkb:', '').split('-')[0]]))), axis=1)\n",
    "# intact_negative = intact_negative.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "# all_pairs_intact_negative = pd.merge(all_pairs, intact_negative, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_intact_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4c239",
   "metadata": {},
   "source": [
    "#### Blohm negatome2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatome2 = pd.read_csv('../../data/negatome2.0/combined.txt', sep='\\t', names=['ID_1', 'ID_2'])\n",
    "# negatome2['pair_id'] = negatome2.apply(lambda row: str(tuple(sorted([row['ID_1'].split('-')[0], row['ID_2'].split('-')[0]]))), axis=1)\n",
    "# all_pairs_negatome2 = pd.merge(all_pairs, negatome2, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_negatome2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63893e",
   "metadata": {},
   "source": [
    "#### Stelzl 2005 negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stelzl_neg = pd.read_csv('../../data/16169070_neg.mitab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Read in HPC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df_uc = pd.DataFrame(data=find_summary_files(HPC_RESULT_DIR))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df_uc)}\")\n",
    "\n",
    "results_df_uc['pair_id'] = results_df_uc.apply(create_pair_id, axis=1)\n",
    "\n",
    "print(f\"jobs before cleaning: {len(results_df_uc)}\")\n",
    "results_df = clean_results(results_df_uc)\n",
    "print(f\"jobs after cleaning: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, intact_cleaned, on='pair_id', how='left')\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total number of modelled pairs: {len(results_df)}\")\n",
    "print(f\"Total rows in merged_df: {len(results_df_annotated)}\")\n",
    "print(f\"Rows with annotated data (STRING): {results_df_annotated['combined_score'].notna().sum()}\")\n",
    "print(f\"Rows with annotated data (IntAct): {results_df_annotated['intact_score'].notna().sum()}\")\n",
    "\n",
    "\n",
    "# convert all STRING scores from 0-1000 to 0-1 (linear conversion)\n",
    "STRING_COLS = ['experimental', 'database', 'textmining', 'combined_score']\n",
    "for col in STRING_COLS:\n",
    "    results_df_annotated[col] = results_df_annotated[col] / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6969fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'iptm', STRING_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ptm', STRING_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.fillna({STRING_SCORE_COLUMN: 0}), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs treated as 0', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments), NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'iptm', STRING_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ranking_score', 'ptm', STRING_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated, 'ptm', 'iptm', STRING_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by STRING Score (experiments); NAs included', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_scatter_plot(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'iptm', INTACT_SCORE_COLUMN, 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', 'ptm', INTACT_SCORE_COLUMN, 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', 'iptm', INTACT_SCORE_COLUMN, 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by IntAct Score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated['avg_STRING_IntAct'] = (results_df_annotated[STRING_SCORE_COLUMN] + results_df_annotated[INTACT_SCORE_COLUMN]) / 2 \n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'iptm', 'avg_STRING_IntAct', 'ranking_score vs iptm', ax=axes[0])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ranking_score', 'ptm', 'avg_STRING_IntAct', 'ranking_score vs ptm', ax=axes[1])\n",
    "create_scatter_plot_colour(results_df_annotated.dropna(subset=['avg_STRING_IntAct']), 'ptm', 'iptm', 'avg_STRING_IntAct', 'iptm vs ptm', ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics Colored by average of STRING and IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb91f86",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88025a",
   "metadata": {},
   "source": [
    "creating AUC curves with STRING / IntAct alone does not really make sense. My understanding is that both include only **positive** interaction candidates and assign scores to the certainty. So even a low score means a relatively high probability of interaction because the candidate is in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def plot_roc(df: pd.DataFrame, pos: Set[str], neg: Set[str], param_name: str, min_val: float, max_val: float, \n",
    "             sampling: int = 1000, direction: str = 'up', plot_title: str = '', ax=None) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Calculate and plot the ROC curve based on a specified parameter.\n",
    "    \n",
    "    This function evaluates the performance of a binary classifier by varying a threshold parameter\n",
    "    and calculating the True Positive Rate (TPR) and False Positive Rate (FPR) at each threshold.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the parameter to evaluate and pair_id column\n",
    "        pos (Set[str]): Set of positive example pair_ids (ground truth positive cases)\n",
    "        neg (Set[str]): Set of negative example pair_ids (ground truth negative cases)\n",
    "        param_name (str): Name of the column in df to use as the classification parameter\n",
    "        min_val (float): Minimum threshold value to evaluate\n",
    "        max_val (float): Maximum threshold value to evaluate\n",
    "        sampling (int, optional): Number of threshold points to sample between min and max. Defaults to 1000.\n",
    "        direction (str, optional): Direction of classification - 'up' means values >= threshold are positive,\n",
    "                                  'down' means values < threshold are positive. Defaults to 'up'.\n",
    "        plot_title (str, optional): Title for the ROC curve plot. If empty, a default title is used. Defaults to ''.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If None, creates new figure.\n",
    "                                   \n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Lists of FPR and TPR values that make up the ROC curve\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        # Print the size of positive and negative sets for verification    \n",
    "        print(f\"Number of positive examples: {len(pos)}\")\n",
    "        print(f\"Number of negative examples: {len(neg)}\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "\n",
    "    # Calculate step size based on range and sampling\n",
    "    step = (max_val - min_val) / sampling\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"max_val must be greater than min_val\")\n",
    "    \n",
    "    # Lists to store TPR and FPR values\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    \n",
    "    for i in range(sampling + 1):\n",
    "        sep_val = min_val + (i * step)\n",
    "        \n",
    "        if direction == 'up':\n",
    "            calc_pos = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "        elif direction == 'down':\n",
    "            calc_pos = set(df[df[param_name] < sep_val]['pair_id'].tolist())\n",
    "            calc_neg = set(df[df[param_name] >= sep_val]['pair_id'].tolist())\n",
    "        else:\n",
    "            raise ValueError(\"direction must be either 'up' or 'down'\")\n",
    "        \n",
    "        # Calculate confusion matrix values\n",
    "        TP_num = len(calc_pos.intersection(pos))\n",
    "        FP_num = len(calc_pos.intersection(neg))\n",
    "        TN_num = len(calc_neg.intersection(neg))\n",
    "        FN_num = len(calc_neg.intersection(pos))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        TPR = TP_num / max(TP_num + FN_num, 1)\n",
    "        FPR = FP_num / max(FP_num + TN_num, 1)\n",
    "        \n",
    "        tpr_list.append(TPR)\n",
    "        fpr_list.append(FPR)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    ax.plot(fpr_list, tpr_list, 'b-', linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2)  # Diagonal line representing random guess\n",
    "    \n",
    "    auc_value = sklearn.metrics.auc(fpr_list, tpr_list)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    \n",
    "    if plot_title:\n",
    "        title = plot_title\n",
    "    else:\n",
    "        title = f'ROC Curve for {param_name}'\n",
    "    \n",
    "    ax.set_title(f'{title}\\nAUC = {auc_value:.3f}', fontsize=12)\n",
    "    \n",
    "    # Add grid and improve appearance\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fpr_list, tpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRING_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# string_combined_pos = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] >= STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# string_combined_neg = set(results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN] < STRING_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(string_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(string_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, string_combined_pos, string_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (STRING cutoff >= {STRING_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152712a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTACT_SCORE_CUTOFF = 0.4\n",
    "# # comparisons like >=, < with NAs don't pass the filter\n",
    "# intact_combined_pos = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] >= INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "# intact_combined_neg = set(results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN] < INTACT_SCORE_CUTOFF]['pair_id'].to_list())\n",
    "\n",
    "# print(f\"Number of positive examples: {len(intact_combined_pos)}\")\n",
    "# print(f\"Number of negative examples: {len(intact_combined_neg)}\")\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ranking_score', 0, 1, plot_title='Ranking Score', ax=axes[0])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'iptm', 0, 1, plot_title='iPTM', ax=axes[1])\n",
    "# plot_roc(results_df_annotated, intact_combined_pos, intact_combined_neg, 'ptm', 0, 1, plot_title='PTM', ax=axes[2])\n",
    "\n",
    "# fig.suptitle(f'ROC Curves for AlphaFold Metrics (IntAct cutoff >= {INTACT_SCORE_CUTOFF})', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04960152",
   "metadata": {},
   "source": [
    "## 4. Structure Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from functions_blastp import *\n",
    "from download_functions import *\n",
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f955aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/*.csv')\n",
    "# review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/intersect_df - set3.csv')\n",
    "struct_ds = pd.concat([pd.read_csv(f) for f in review_files], ignore_index=True)\n",
    "struct_ds = struct_ds.drop_duplicates(subset=['pdb_id', 'query_tf', 'query_arm', 'chain_tf', 'chain_arm'])\n",
    "struct_ds['pair_id'] = struct_ds.apply(lambda row: str(tuple(sorted([row['query_tf'].split('|')[0].upper(), row['query_arm'].split('|')[0].upper()]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_ids = struct_ds['pdb_id'].drop_duplicates().to_list()\n",
    "for pdb_id in pdb_ids:\n",
    "    sequences = download_pdb_sequence(pdb_id)\n",
    "    if len(sequences) == 0:\n",
    "        print(f\"Empty sequence list: {pdb_id}\")\n",
    "        continue\n",
    "    job = create_alphafold_job_ms(pdb_id, sequences)\n",
    "    file_path = os.path.join('/home/markus/MPI_local/production1/PDB_modelling/', f\"{pdb_id}.json\")\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(job, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ds = struct_ds.drop(columns=[\n",
    "    \"chain_tf\", \"chain_arm\", \"%identity_tf\", \"%identity_arm\",\n",
    "    \"evalue_tf\", \"bit score_tf\", \"evalue_arm\", \"subject_tf\",\n",
    "    \"bit score_arm\", \"alignment length_tf\", \"mismatches_tf\", \"gap opens_tf\", \"q. start_tf\",\n",
    "    \"q. end_tf\", \"s. start_tf\", \"s. end_tf\", \"subject_arm\", \"alignment length_arm\",\n",
    "    \"mismatches_arm\", \"gap opens_arm\", \"q. start_arm\", \"q. end_arm\", \"s. start_arm\", \"s. end_arm\", \"Unnamed: 12\", \"Unnamed: 5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dec79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLAST_IDENTITY_CUTOFF: int|bool = False\n",
    "BLAST_SCORE_CUTOFF: int|bool = False\n",
    "BLAST_EVALUE_CUTOFF: float|bool = 0.00001\n",
    "BLAST_COVERAGE_CUTOFF: float|bool = 0.5\n",
    "TF_OUTPUT_DIR = \"/home/markus/MPI_local/production1/blastp_results/tf_blastp_no_e_lim_new_fmt\"\n",
    "ARM_OUTPUT_DIR = \"/home/markus/MPI_local/production1/blastp_results/tf_blastp_no_e_lim_new_fmt\"\n",
    "\n",
    "\n",
    "columns: List[str] = [\n",
    "    \"query\", \"subject\", \"%identity\", \"alignment length\", \"mismatches\", \"gap opens\",\n",
    "    \"q. start\", \"q. end\", \"s. start\", \"s. end\", \"evalue\", \"bit score\", \"% query coverage per subject\", \"% query coverage per hsp\", \"% query coverage per uniq subject\"\n",
    "]\n",
    "\n",
    "tf_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(TF_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(tf_blast_df))\n",
    "arm_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(ARM_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(arm_blast_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ds = struct_ds.merge(arm_blast_df.add_suffix('_arm'), how='left', left_on='query_arm', right_on='uniprot_id_arm')\n",
    "struct_ds = struct_ds.merge(tf_blast_df.add_suffix('_tf'), how='left', left_on='query_tf', right_on='uniprot_id_tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(struct_ds))\n",
    "struct_ds = struct_ds[struct_ds['pair_id'].isin(all_pairs['pair_id'])]\n",
    "print(len(struct_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_structs = all_pairs_struct_ds_annotated[all_pairs_struct_ds_annotated['comment'].str.contains('looks good|in complex', case=False, na=False)]\n",
    "\n",
    "# Get unique PDB IDs from reviews_df\n",
    "unique_pdb_ids = good_structs['pdb_id'].unique()\n",
    "print(f\"Found {len(unique_pdb_ids)} unique PDB IDs to download\")\n",
    "\n",
    "# Define download directory\n",
    "pdb_download_dir = \"/home/markus/MPI_local/data/PDB\"\n",
    "\n",
    "# Download each PDB structure (function handles duplicate checking)\n",
    "downloaded_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for pdb_id in unique_pdb_ids:\n",
    "    if pd.isna(pdb_id):  # Skip NaN values\n",
    "        continue\n",
    "        \n",
    "    result = download_pdb_structure(pdb_id, pdb_download_dir)\n",
    "    if result:\n",
    "        downloaded_count += 1\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Successfully processed: {downloaded_count}\")\n",
    "print(f\"Failed downloads: {failed_count}\")\n",
    "print(f\"Total processed: {len([pdb_id for pdb_id in unique_pdb_ids if not pd.isna(pdb_id)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "\n",
    "# Calculate DockQ scores using the function from functions_filtering.py\n",
    "good_structs = append_dockq(good_structs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
