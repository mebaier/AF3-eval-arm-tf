{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d8d8ed",
   "metadata": {},
   "source": [
    "## General Description\n",
    "\n",
    "1. Create Datasets\n",
    "2. Create AF jobs\n",
    "3. analyze AF output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67130045",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 1. Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecf960",
   "metadata": {},
   "source": [
    "### Library imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4c98",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PATH = '/home/markus/MPI_local/data/STRING/9606.protein.physical.links.detailed.v12.0.txt_processed.csv'\n",
    "PROTEOME_PATH = '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv'\n",
    "# PROTEOME_PATH = '/home/markus/MPI_local/data/full_UP/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "TF_DATASET_PATH = '/home/markus/MPI_local/data/human_TFs/DatabaseExtract_v_1.01.csv'\n",
    "ENSEMBL_MAPPING_PATH = '/home/markus/MPI_local/data/Ensembl_mapping/Homo_sapiens.GRCh38.114.uniprot.tsv/hps/nobackup/flicek/ensembl/production/release_dumps/release-114/ftp_dumps/vertebrates/tsv/homo_sapiens/Homo_sapiens.GRCh38.114.uniprot.tsv'\n",
    "AIUPRED_PATH = '/home/markus/MPI_local/data/AIUPred/AIUPred_data.json'\n",
    "DISPROT_PATH = '/home/markus/MPI_local/data/DisProt/DisProt_release_2024_12 with_ambiguous_evidences.tsv'\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7005bb",
   "metadata": {},
   "source": [
    "### Read in Proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uniprot = pd.read_csv(PROTEOME_PATH, low_memory=False, sep='\\t')\n",
    "uniprot_filtered = all_uniprot[all_uniprot['Reviewed'] == 'reviewed']\n",
    "# uniprot_filtered = all_uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517d11f",
   "metadata": {},
   "source": [
    "### create Armadillo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8086705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip version number since it is not included in the uniprot annotation\n",
    "arm_accs_pfam_stripped = [acc.split(\".\")[0] for acc in arm_accs_pfam]\n",
    "    \n",
    "# find proteins with \"ARM\" in their Repeat column\n",
    "repeat_mask_arm = uniprot_filtered['Repeat'].apply(lambda x: \"ARM\" in str(x) if pd.notna(x) else False)\n",
    "print(f\"Proteins with 'ARM' in Repeat column: {len(uniprot_filtered[repeat_mask_arm])}\")\n",
    "\n",
    "# Filter rows where InterPro column contains any interpro_annotations\n",
    "interpro_mask_arm = uniprot_filtered['InterPro'].apply(lambda x: contains_any_annotation(x, arm_accs_ipr))\n",
    "print(f\"Proteins with specified InterPro annotation: {len(uniprot_filtered[interpro_mask_arm])}\")\n",
    "\n",
    "# Filter rows where Pfam column contains any pfam_annotations\n",
    "pfam_mask_arm = uniprot_filtered['Pfam'].apply(lambda x: contains_any_annotation(x, arm_accs_pfam_stripped))\n",
    "print(f\"Proteins with specified Pfam annotation: {len(uniprot_filtered[pfam_mask_arm])}\")\n",
    "\n",
    "# apply filters using OR\n",
    "armadillo_proteins = uniprot_filtered[interpro_mask_arm | pfam_mask_arm | repeat_mask_arm]\n",
    "\n",
    "print(f\"Found {len(armadillo_proteins)} proteins with armadillo domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74d7f1",
   "metadata": {},
   "source": [
    "### Create Transcription Factor dataset (UniProtFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eea99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def str_tf(x):\n",
    "#     return \"transcription factor\" in x.lower()\n",
    "\n",
    "# def str_t(x):\n",
    "#     return \"transcription\" in x.lower()\n",
    "\n",
    "# # IPR accessions containing \"transcription\"\n",
    "# IPR_entries = pd.read_csv(\"../entry.list\", sep=\"\\t\")\n",
    "# tf_accs_ipr = IPR_entries[IPR_entries['ENTRY_NAME'].apply(lambda x: str_t(x))][\"ENTRY_AC\"].tolist()\n",
    "\n",
    "# # PFAM accessions containing \"transcription factor\"\n",
    "# PFAM_entries = pd.read_csv(\"../data/pfam_parsed_data.csv\", sep=\",\")\n",
    "# tf_accs_pfam = PFAM_entries[PFAM_entries['DE'].apply(lambda x: str_t(x))][\"AC\"].tolist()\n",
    "\n",
    "# # strip version number since it is not included in the uniprot annotation\n",
    "# for i in range(len(tf_accs_pfam)):\n",
    "#     tf_accs_pfam[i] = tf_accs_pfam[i].split(\".\")[0]\n",
    "\n",
    "# interpro_mask_tf = reviewed_proteins['InterPro'].apply(lambda x: contains_any_annotation(x, tf_accs_ipr))\n",
    "# print(f\"Proteins with specified InterPro annotation: {len(reviewed_proteins[interpro_mask_tf])}\")\n",
    "\n",
    "# pfam_mask_tf = reviewed_proteins['Pfam'].apply(lambda x: contains_any_annotation(x, tf_accs_pfam))\n",
    "# print(f\"Proteins with specified Pfam annotation: {len(reviewed_proteins[pfam_mask_tf])}\")\n",
    "\n",
    "# txt_mask_tf = reviewed_proteins['Protein names'].apply(lambda x: str_tf(x))\n",
    "# print(f\"Proteins with 'Transcription factor' in the name: {len(reviewed_proteins[txt_mask_tf])}\")\n",
    "\n",
    "\n",
    "# # Combine filters with OR operation\n",
    "# tf_proteins_uniprot_ds = reviewed_proteins[interpro_mask_tf | pfam_mask_tf | txt_mask_tf]\n",
    "\n",
    "# print(f\"Found {len(tf_proteins_uniprot_ds)} proteins with transcription factor annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18c690",
   "metadata": {},
   "source": [
    "### Use existing Transcription Factor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs = pd.read_csv(TF_DATASET_PATH)\n",
    "human_TFs = human_TFs[human_TFs['Is TF?'] == 'Yes']\n",
    "len(human_TFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_mapping = pd.read_csv(ENSEMBL_MAPPING_PATH, sep='\\t')\n",
    "ensembl_mapping_swissProt = ensembl_mapping[ensembl_mapping['db_name'] == 'Uniprot/SWISSPROT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_TFs_gids = human_TFs['Ensembl ID'].tolist()\n",
    "\n",
    "# Use swiss prot accessions to prevent duplicates\n",
    "# TODO: use caching\n",
    "# TODO: adjust ensemble mapping if using other proteome?\n",
    "human_TF_uniprot_accs = ensembl_mapping_swissProt[ensembl_mapping_swissProt['gene_stable_id'].apply(lambda x: any((id == x) for id in human_TFs_gids))]['xref'].tolist()\n",
    "print(len(human_TF_uniprot_accs))\n",
    "\n",
    "tf_proteins_curated_ds = uniprot_filtered[uniprot_filtered['Entry'].apply(lambda x: any((id in x) for id in human_TF_uniprot_accs))]\n",
    "print(len(tf_proteins_curated_ds))\n",
    "\n",
    "# 4m 1.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228b97",
   "metadata": {},
   "source": [
    "#### IUPred 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds = add_iupred3(tf_proteins_curated_ds, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)\n",
    "\n",
    "tf_proteins_curated_ds_IUPred3_diso = tf_proteins_curated_ds[tf_proteins_curated_ds['num_disordered_regions'] > 0]\n",
    "\n",
    "print(f\"Number of transcription factors with at least one disordered region (IUPred3, threshold={IUPRED3_THRESHOLD}, min length={MIN_LENGTH_DISORDERED_REGION}): {len(tf_proteins_curated_ds_IUPred3_diso)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291adc6",
   "metadata": {},
   "source": [
    "#### Disprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disprot_df = pd.read_csv(DISPROT_PATH, sep='\\t')\n",
    "\n",
    "# make format the same as in uniprot columns\n",
    "disprot_df['disprot_id'] = disprot_df['disprot_id'].apply(lambda x: x + ';')\n",
    "\n",
    "tf_disprot_ids = tf_proteins_curated_ds['DisProt'].dropna().tolist()\n",
    "\n",
    "disprot_tfs = disprot_df[disprot_df['disprot_id'].apply(lambda x: x in tf_disprot_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_proteins_curated_ds_disprot = tf_proteins_curated_ds.merge(disprot_df, how='left', left_on='DisProt', right_on='disprot_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f148f",
   "metadata": {},
   "source": [
    "### Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = create_all_pairs(armadillo_proteins, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_over_token_limit = len(all_pairs[all_pairs['Length_arm'] + all_pairs['Length_tf'] > AF_TOKEN_LIMIT])\n",
    "print(f\"Pairs over token limit: {num_pairs_over_token_limit} ({(num_pairs_over_token_limit/len(all_pairs))*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f4723",
   "metadata": {},
   "source": [
    "### STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the STRING file\n",
    "# note that the file is a STRING database dump preprocessed with the scripts in /src/STRING \n",
    "# it should contain columns p1_Uniprot, p2_Uniprot and pair_id\n",
    "string_df = pd.read_csv(STRING_PATH, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4effdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the all_pairs df with the STRING scores\n",
    "# IMPORTANT: drop rows that don't have a matching STRING entry\n",
    "all_pairs_w_STRING = pd.merge(all_pairs, string_df, on='pair_id', how='inner')\n",
    "\n",
    "# print number of unmatched pairs\n",
    "unmatched_pairs = all_pairs[~all_pairs['pair_id'].isin(all_pairs_w_STRING['pair_id'])]\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_STRING = len(all_pairs_w_STRING)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with STRING data: {num_all_pairs_w_STRING} ({(num_all_pairs_w_STRING/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32f64e",
   "metadata": {},
   "source": [
    "### IntAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_intact import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_cleaned = read_clean_intact('../../data/IntAct/human/human.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_w_IntAct = pd.merge(all_pairs, intact_cleaned, on='pair_id', how='inner')\n",
    "\n",
    "num_all_pairs = len(all_pairs)\n",
    "num_all_pairs_w_IntAct = len(all_pairs_w_IntAct)\n",
    "print(f\"Number of pairs in all_pairs: {num_all_pairs}\")\n",
    "print(f\"Number of pairs successfully merged with IntAct data: {num_all_pairs_w_IntAct} ({(num_all_pairs_w_IntAct/num_all_pairs)*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_intersect_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='inner')\n",
    "\n",
    "print(len(all_pairs_intersect_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_union_STRING_IntAct = pd.merge(all_pairs_w_STRING, all_pairs_w_IntAct, on='pair_id', how='outer')\n",
    "\n",
    "print(len(all_pairs_union_STRING_IntAct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd617a",
   "metadata": {},
   "source": [
    "### write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in armadillo_proteins.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/arm_all_uniprot_rev_fasta', row['Reviewed'])\n",
    "# for idx, row in tf_proteins_curated_ds_IUPred3_diso.iterrows():\n",
    "#     print_to_fasta(row['Entry'], row['Sequence'], '../../production1/tf_all_uniprot_rev_fasta', row['Reviewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# armadillo_proteins.to_csv('../../armadillo_proteins.csv', index=False)\n",
    "# tf_proteins_curated_ds_IUPred3_diso.to_csv('../../transcription_factors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f20999",
   "metadata": {},
   "source": [
    "## 1.1 PDB reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff831cb6",
   "metadata": {},
   "source": [
    "### imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "\n",
    "# Step 2: Re-import everything you need\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_report_arm_filter(pdb_report: pd.DataFrame, armadillo_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter entries that have at least one ARM, add column isARM = True|False\n",
    "    keep_pdbs = set()\n",
    "    armadillo_entries = armadillo_proteins['Entry'].tolist()\n",
    "    pdb_report['isARM'] = False\n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in armadillo_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isARM'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_tf_filter(pdb_report: pd.DataFrame, tf_proteins: pd.DataFrame) -> pd.DataFrame:\n",
    "    keep_pdbs = set()\n",
    "    tf_entries = tf_proteins['Entry'].tolist()\n",
    "    pdb_report['isDisoTF'] = False\n",
    "    \n",
    "\n",
    "    for ind, row in pdb_report.iterrows():\n",
    "        if pd.notna(row['Accession Code(s)']) and row['Accession Code(s)'] in tf_entries:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            pdb_report.at[ind, 'isDisoTF'] = True\n",
    "    \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report\n",
    "\n",
    "def pdb_report_disorder_filter(pdb_report: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter for entries that have at least one protein that is not ARm and has a disordered region\n",
    "    keep_pdbs = set()\n",
    "\n",
    "    for _, row in pdb_report.iterrows():\n",
    "        if row['isARM'] == False and row['num_disordered_regions'] > 0:\n",
    "            keep_pdbs.add(row['Entry ID'])\n",
    "            \n",
    "    pdb_report = pdb_report[pdb_report['Entry ID'].isin(keep_pdbs)]\n",
    "    return pdb_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d065de",
   "metadata": {},
   "source": [
    "### report 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_1 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/1/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760715e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entry IDs from both datasets\n",
    "pdb_entry_ids = set(pdb_report_1['Accession Code(s)'].dropna())\n",
    "tf_entry_ids = set(tf_proteins_curated_ds_IUPred3_diso['Entry'])\n",
    "\n",
    "# Find intersection\n",
    "common_entries = pdb_entry_ids.intersection(tf_entry_ids)\n",
    "\n",
    "print(f\"Number of Entry IDs in pdb_report_1_two_seq: {len(pdb_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs in tf_proteins_curated_ds: {len(tf_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs that appear in both datasets: {len(common_entries)}\")\n",
    "print(f\"Percentage of PDB entries that are also TFs: {len(common_entries)/len(pdb_entry_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only entries where at least one Uniprot ID is in list of disordered TFs\n",
    "pdb_report_1 = pdb_report_tf_filter(pdb_report_1, tf_proteins_curated_ds_IUPred3_diso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_1 = pdb_report_arm_filter(pdb_report_1, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "# pdb_report_1 = add_iupred3(pdb_report_1, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08220f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_1['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_1_seq2 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pairs where ARM and disordered TF is the same protein\n",
    "keep_pdbs = set()\n",
    "\n",
    "for ind, row in pdb_report_1.iterrows():\n",
    "    if not (row['isARM'] == True and row['isDisoTF'] == True):\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "\n",
    "pdb_report_1 = pdb_report_1[pdb_report_1['Entry ID'].isin(keep_pdbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "# pdb_report_1 = pdb_report_disorder_filter(pdb_report_1_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577268e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_1['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f620f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_appearing_3 = entry_counts[entry_counts == 3].index.tolist()\n",
    "pdb_report_1_seq3 = pdb_report_1[pdb_report_1['Entry ID'].isin(entries_appearing_3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68903f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs by separating into arm and tf half\n",
    "report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False) & (pdb_report_1_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "# report_1_arm = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == True)]\n",
    "# report_1_tf = pdb_report_1_seq2[(pdb_report_1_seq2['isARM'] == False)]\n",
    "\n",
    "report_1_pairs = pd.merge(left=report_1_tf, right=report_1_arm, on='Entry ID', suffixes=['_tf', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_1_pairs = report_1_pairs.rename(columns={'Entry ID': 'pdb_id'})\n",
    "\n",
    "# print_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "\n",
    "report_1_pairs, no_model = append_dockq(report_1_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d904a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_1_pairs_sorted = report_1_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_1_pairs_sorted.iterrows():\n",
    "    pdb_id = row['pdb_id']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_tf'] + \" \" + row['Release Date_arm']  # Using tf release date\n",
    "    job_name = row.get('job_name', 'N/A')  # Use get() in case column doesn't exist\n",
    "\n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\\t\\t{job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabafded",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1_pairs = annotate_AF_metrics(report_1_pairs, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_1_pairs, 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ptm', 'dockq_score', ax=axes[1],corr=True)\n",
    "create_scatter_plot(report_1_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2b91d",
   "metadata": {},
   "source": [
    "### report 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e16ba",
   "metadata": {},
   "source": [
    "one ARM and one disordered protein (see PDB report),\n",
    "only two proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_2 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/2/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f061688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entry IDs from both datasets\n",
    "pdb_entry_ids = set(pdb_report_2['Accession Code(s)'].dropna())\n",
    "tf_entry_ids = set(tf_proteins_curated_ds_IUPred3_diso['Entry'])\n",
    "\n",
    "# Find intersection\n",
    "common_entries = pdb_entry_ids.intersection(tf_entry_ids)\n",
    "\n",
    "print(f\"Number of Entry IDs in pdb_report_2: {len(pdb_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs in tf_proteins_curated_ds: {len(tf_entry_ids)}\")\n",
    "print(f\"Number of Entry IDs that appear in both datasets: {len(common_entries)}\")\n",
    "print(f\"Percentage of PDB entries that are also TFs: {len(common_entries)/len(pdb_entry_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_2 = pdb_report_arm_filter(pdb_report_2, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "pdb_report_2 = add_iupred3(pdb_report_2, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some PDBs that have entries for seemingly non-existent chains. I don't know why. Therefore, I remove all rows that have an empty 'Sequence' field\n",
    "pdb_report_2 = pdb_report_2[pdb_report_2['Sequence'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice (redundant in this case)\n",
    "print(len(pdb_report_2))\n",
    "entry_counts = pdb_report_2['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_2_seq2 = pdb_report_2[pdb_report_2['Entry ID'].isin(entries_appearing_twice)]\n",
    "print(len(pdb_report_2_seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d383e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now filter for disordered regions\n",
    "pdb_report_2_seq2 = pdb_report_disorder_filter(pdb_report_2_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39830797",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_2_seq2['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_arm = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == True) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_2_diso = pdb_report_2_seq2[(pdb_report_2_seq2['isARM'] == False) & (pdb_report_2_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "\n",
    "report_2_pairs = pd.merge(left=report_2_diso, right=report_2_arm, on='Entry ID', suffixes=['_diso', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_2_pairs.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "report_2_pairs, no_model = append_dockq_single_interface(report_2_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "report_2_pairs.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c04bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f65f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74224994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_2_pairs_sorted = report_2_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_2_pairs_sorted.iterrows():\n",
    "    pdb_id = row['Entry ID']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_diso']\n",
    "    length = len(row['Sequence_arm']) + len(row['Sequence_diso'])\n",
    "    \n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\\t\\t{length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs = annotate_AF_metrics(report_2_pairs, '/home/markus/MPI_local/HPC_results_full')\n",
    "report_2_pairs['in_training_set'] = report_2_pairs['Release Date_diso'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into directory for PDB2Net processing\n",
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = PDB2NET_PREFIX + 'report2/'\n",
    "report_2_pairs['file_path'] = report_2_pairs['Entry ID'].apply(lambda id: path_prefix + id.lower() + '.cif')\n",
    "\n",
    "report_2_pairs.drop_duplicates(subset=['file_path'], inplace=False)['file_path'].to_csv(PDB2NET_PREFIX + 'report2.csv', index=False)\n",
    "\n",
    "# download pdb structures for pdb2net\n",
    "download_pdb_structures(set(report_2_pairs['Entry ID'].tolist()), path_prefix, 'cif', '/home/markus/MPI_local/data/PDB', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a354137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "# define interface as having at least INTERFACE_MIN_ATOMS atoms within INTERFACE_MAX_DISTANCE A of each other \n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "INTERFACE_MAX_DISTANCE = 5 # higher not possible => change PDB2Net data\n",
    "# Check interface between TF chain and ARM chain\n",
    "report_2_pairs['chain_interface'] = False  # Initialize with False\n",
    "valid_rows = report_2_pairs[report_2_pairs['Entry ID'].notna() & report_2_pairs['Asym ID_diso'].notna() & report_2_pairs['Asym ID_arm'].notna()]\n",
    "\n",
    "for ind in valid_rows.index:\n",
    "    pdb_id = str(valid_rows.at[ind, 'Entry ID']).upper().strip()\n",
    "    chain_tf = valid_rows.at[ind, 'Asym ID_diso']\n",
    "    chains_arm = valid_rows.at[ind, 'Asym ID_arm']\n",
    "    \n",
    "    for chain_arm in chains_arm:\n",
    "        if check_interface(pdb_id, chain_tf, chain_arm, '/home/markus/MPI_local/data/PDB2Net/report2/2025-08-31_10-24-32', INTERFACE_MIN_ATOMS, INTERFACE_MAX_DISTANCE):\n",
    "            report_2_pairs.at[ind, 'chain_interface'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "report_2_pairs_interface = report_2_pairs[report_2_pairs['chain_interface'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_2_pairs, 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ptm', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('AF metrics vs DockQ score for interaction disordered-ARM (yellow = in training set)', fontsize = 18)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'iptm', 'dockq_score', 'in_training_set', ax=axes[0], corr=True)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'ptm', 'dockq_score', 'in_training_set', ax=axes[1], corr=True)\n",
    "create_scatter_plot_colour(report_2_pairs_interface, 'ranking_score', 'dockq_score', 'in_training_set', ax=axes[2], corr=True)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.49]))\n",
    "print(len(report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('AF metrics vs DockQ score for interaction disordered-ARM (yellow = in training set)', fontsize = 18)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'iptm', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'ptm', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False], 'ranking_score', 'dockq_score', ax=axes[2], corr=True)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e897a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2_pairs_interface_no_tranining = report_2_pairs_interface[report_2_pairs_interface['in_training_set'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(report_2_pairs_interface_no_tranining[report_2_pairs_interface_no_tranining['dockq_score'] >= 0.49]))\n",
    "print(len(report_2_pairs_interface_no_tranining[report_2_pairs_interface_no_tranining['dockq_score'] >= 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(report_2_pairs_interface, report_2_pairs_interface[report_2_pairs_interface['dockq_score'] >= 0.49], report_2_pairs_interface[report_2_pairs_interface['dockq_score'] < 0.49], 'ranking_score', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot_colour(report_2_pairs, 'iptm', 'dockq_score', 'chain_interface', ax=axes[0])\n",
    "create_scatter_plot_colour(report_2_pairs, 'ptm', 'dockq_score', 'chain_interface', ax=axes[1])\n",
    "create_scatter_plot_colour(report_2_pairs, 'ranking_score', 'dockq_score', 'chain_interface', ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fc6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(report_2_pairs[report_2_pairs['in_training_set'] == True], 'ranking_score', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(report_2_pairs[report_2_pairs['in_training_set'] == False], 'ranking_score', 'dockq_score', ax=axes[1], corr=True)\n",
    "create_scatter_plot(report_2_pairs, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40735318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=report_2_pairs[report_2_pairs['in_training_set'] == False],\n",
    "    dataset1_name='report 2 not in training', \n",
    "    dataset2=report_2_pairs[report_2_pairs['in_training_set'] == True],\n",
    "    dataset2_name='report 2 in training',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_double_violin_plot(\n",
    "    dataset1=report_2_pairs[report_2_pairs['in_training_set'] == False],\n",
    "    dataset1_name='not in training',\n",
    "    metric1='dockq_score',\n",
    "    dataset2=report_2_pairs[report_2_pairs['in_training_set'] == True],\n",
    "    dataset2_name='in training',\n",
    "    metric2='dockq_score'\n",
    ")\n",
    "fig.suptitle('Disordered-ARM: dockq score', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_2_pairs['Entry ID'].to_csv('report_2_entry_ids.csv', index=False)\n",
    "download_pdb_structures(set(report_2_pairs['Entry ID'].tolist()), '/home/markus/PDB2Net/in/rep2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14e271",
   "metadata": {},
   "source": [
    "### report 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_report_3 = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/3/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter entries that have at least one ARM, add column isARM = True|False\n",
    "pdb_report_3 = pdb_report_arm_filter(pdb_report_3, armadillo_proteins)\n",
    "# annotate with iupred3\n",
    "pdb_report_3 = add_iupred3(pdb_report_3, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries that appear exactly twice => arm interacting with tf (since one is arm and the other must be tf)\n",
    "entry_counts = pdb_report_3['Entry ID'].value_counts()\n",
    "entries_appearing_twice = entry_counts[entry_counts == 2].index.tolist()\n",
    "pdb_report_3_seq2 = pdb_report_3[pdb_report_3['Entry ID'].isin(entries_appearing_twice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all pairs that have one ARM partner => the other protein must be the TF (candidate), since that was in the original search criteria\n",
    "# now filter for disordered regions\n",
    "pdb_report_3_seq2 = pdb_report_disorder_filter(pdb_report_3_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88289bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_structures(set(pdb_report_3_seq2['Entry ID'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec179587",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3_arm = pdb_report_3_seq2[(pdb_report_3_seq2['isARM'] == True) & (pdb_report_3_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "report_3_diso = pdb_report_3_seq2[(pdb_report_3_seq2['isARM'] == False) & (pdb_report_3_seq2['Total Number of polymer Entity Instances (Chains) per Entity'] == 1)]\n",
    "\n",
    "report_3_pairs = pd.merge(left=report_3_diso, right=report_3_arm, on='Entry ID', suffixes=['_diso', '_arm'])\n",
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "report_3_pairs.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "report_3_pairs, no_model = append_dockq(report_3_pairs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot, 'pdb', PDB_CACHE, DOCKQ_CACHE)\n",
    "report_3_pairs.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by dockq column and print the requested information\n",
    "report_3_pairs_sorted = report_3_pairs.sort_values('dockq_score', ascending=False)\n",
    "\n",
    "print(\"PDB_ID\\t\\tDockQ\\t\\tRelease Date\\t\\tJob Name\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in report_3_pairs_sorted.iterrows():\n",
    "    pdb_id = row['Entry ID']\n",
    "    dockq = row['dockq_score'] if pd.notna(row['dockq_score']) else 'N/A'\n",
    "    release_date = row['Release Date_diso']\n",
    "    \n",
    "    print(f\"{pdb_id}\\t\\t{dockq}\\t\\t{release_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f166453",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3_pairs = annotate_AF_metrics(report_3_pairs, '/home/markus/MPI_local/HPC_results_full')\n",
    "report_3_pairs['in_training_set'] = report_3_pairs['Release Date_diso'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71de96",
   "metadata": {},
   "source": [
    "### job creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "# IMPORTANT: check for duplicates that were modelled in production1! They may need to be downloaded extra\n",
    "\n",
    "\n",
    "BATCH_DIRS = []\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "new_af_jobs = create_job_batch_from_PDB_IDs(list(set(pdb_report_2_seq2['Entry ID'].tolist())), BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98359aaa",
   "metadata": {},
   "source": [
    "## 1.2 Hadeer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83082c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadeer_df = pd.read_csv('/home/markus/MPI_local/downloads/transcription_factors_pdbs.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba571061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadeer_df.drop(columns=['Protein.names', 'Gene.Names', 'Organism', 'Length', 'InterPro', 'Pfam', 'DisProt', 'STRING', 'IntAct', 'Ensembl', 'Repeat', 'HGNC'])\n",
    "hadeer_df.rename(columns={'chain': 'chain_tf', 'Entry': 'Entry_tf'}, inplace=True)\n",
    "\n",
    "# add for each pdb the other chains and other pdbs\n",
    "# find out which chain is ARM\n",
    "# check interaction between chain_tf and chain_arm\n",
    "\n",
    "hadeer_df['chains_arm'] = None\n",
    "hadeer_df['Entrys_arm'] = None\n",
    "\n",
    "for ind,row in hadeer_df.iterrows():\n",
    "\n",
    "    pdb_id = row['pdb']\n",
    "    if pd.isna(pdb_id):\n",
    "        continue\n",
    "\n",
    "    # normalize pdb id and ensure chain is present\n",
    "    pdb_id = str(pdb_id).upper().strip()\n",
    "    chain_tf = row['chain_tf']\n",
    "    if pd.isna(chain_tf):\n",
    "        continue\n",
    "\n",
    "    all_pdbs = get_pdb_chains_to_uniprot(pdb_id, '../../production1/mapping_cache')\n",
    "    \n",
    "    arm_pairs = []\n",
    "    \n",
    "    for chain, id in all_pdbs.items():\n",
    "        if id in armadillo_proteins['Entry'].tolist():\n",
    "            arm_pairs.append((chain, id))\n",
    "            \n",
    "    \n",
    "    if len(arm_pairs) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        hadeer_df.at[ind, 'chains_arm'] = [pair[0] for pair in arm_pairs]\n",
    "        hadeer_df.at[ind, 'Entrys_arm'] = [pair[1] for pair in arm_pairs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define interface as having at least n atoms within 5 A of each other \n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "# Check interface between TF chain and ARM chain\n",
    "hadeer_df['arm_tf_interface'] = False  # Initialize with False\n",
    "valid_rows = hadeer_df[hadeer_df['pdb'].notna() & hadeer_df['chain_tf'].notna() & hadeer_df['chains_arm'].notna()]\n",
    "\n",
    "for ind in valid_rows.index:\n",
    "    pdb_id = str(valid_rows.at[ind, 'pdb']).upper().strip()\n",
    "    chain_tf = valid_rows.at[ind, 'chain_tf']\n",
    "    chains_arm = valid_rows.at[ind, 'chains_arm']\n",
    "    \n",
    "    for chain_arm in chains_arm:\n",
    "        if check_interface(pdb_id, chain_tf, chain_arm, '/home/markus/MPI_local/data/PDB2Net', INTERFACE_MIN_ATOMS):\n",
    "            hadeer_df.at[ind, 'arm_tf_interface'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique()))\n",
    "display(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e803956",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "new_af_jobs = create_job_batch_from_PDB_IDs(hadeer_df[hadeer_df['arm_tf_interface'] == True]['pdb'].unique().tolist(), BATCH_DIRS, 12000)\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a4133",
   "metadata": {},
   "source": [
    "## 2. Create AF job files\n",
    "- create job files for alphafold\n",
    "- don't create duplicate jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c2d1f",
   "metadata": {},
   "source": [
    "### create job files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'\n",
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DIRS = [os.path.join('../../production1/AF_job_batches/', d) for d in os.listdir('../../production1/AF_job_batches') if os.path.isdir(os.path.join('../../production1/AF_job_batches', d)) and 'batch' in d]\n",
    "BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "### STRING\n",
    "# note that the order is important. The category (100,200) is very large so it comes last to fill up the remaining jobs\n",
    "# categories = [(900,1000), (800,900), (700,800), (600,700), (500,600), (400,500), (300,400), (100,200), (0,100), (200,300)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_STRING, BATCH_SIZE, categories, BATCH_DIRS, STRING_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### IntAct\n",
    "# categories = [(0.1,0.2), (0.9,1), (0.8,0.9), (0.7,0.8), (0.6,0.7), (0.5,0.6), (0.4,0.5), (0.2,0.3), (0.3,0.4)]\n",
    "# new_af_jobs = create_job_batch_scoreCategories(all_pairs_w_IntAct, BATCH_SIZE, categories, BATCH_DIRS, INTACT_SCORE_COLUMN, AF_TOKEN_LIMIT)\n",
    "\n",
    "### all pairs\n",
    "# new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "new_af_jobs = create_job_batch_all_pairs(all_pairs, BATCH_SIZE, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "### ID list:\n",
    "id_list_good = [\n",
    "    (\"Q13285\", \"A0A2R8YCH5\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"P04637\", \"A0A8I5KU01\"),\n",
    "    (\"Q9H3D4\", \"A0A8I5KU01\"),\n",
    "    (\"Q8NHM5\", \"A1YPR0\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q9UJU2\", \"A0A2R8YCH5\"),\n",
    "    (\"Q6SJ96\", \"O14981\"),\n",
    "    (\"Q9NRY4\", \"O00750\"),\n",
    "    (\"Q6ZRS2\", \"A0A8V8TQN3\")\n",
    "]\n",
    "\n",
    "# id_list_complex = [\n",
    "#     (\"Q03181\", \"Q9H3U1\"),\n",
    "#     (\"P04637\", \"A0A994J4J0\"),\n",
    "#     (\"Q6SJ96\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"P49450\", \"O14981\"),\n",
    "#     (\"Q9UBG7\", \"A0A8I5KU01\"),\n",
    "#     (\"P19838\", \"A0A1W2PRG6\")\n",
    "# ]\n",
    "\n",
    "# missing = [('Q13285', 'Q6BTZ4'), ('P04637', 'Q9VL06'), ('P04637', 'Q9VL06'), ('Q9UIF8', 'Q54U63'), ('Q15047', 'Q54U63'), ('Q15047', 'Q5R881'), ('Q9H3D4', 'Q9VL06'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('P49450', 'Q4WJI7'), ('Q6ZRS2', 'P38811'), ('Q6ZRS2', 'P38811')]\n",
    "# new_af_jobs = create_job_batch_id_list(all_pairs, missing, BATCH_DIRS, AF_TOKEN_LIMIT)\n",
    "\n",
    "\n",
    "write_af_jobs_to_individual_files(new_af_jobs, '../../production1/AF_job_batches/batch_56')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff92c07",
   "metadata": {},
   "source": [
    "## 3. Analyze AF results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "\n",
    "# Step 2: Re-import everything you need\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33db9f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_RESULT_DIR = \"/home/markus/MPI_local/HPC_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373a644",
   "metadata": {},
   "source": [
    "### Negatomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4a83e",
   "metadata": {},
   "source": [
    "#### IntAct Negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intact_negative = pd.read_csv('../../data/IntAct/human/human_negative.txt', sep='\\t')\n",
    "# intact_negative.drop(['Alias(es) interactor A', \n",
    "#                      'Alias(es) interactor B', \n",
    "#                      'Interaction detection method(s)',\n",
    "#                      'Publication 1st author(s)',\n",
    "#                      'Publication Identifier(s)',\n",
    "#                      'Taxid interactor A',\n",
    "#                      'Taxid interactor B',\n",
    "#                      'Biological role(s) interactor A',\n",
    "#                      'Biological role(s) interactor B',\n",
    "#                      'Experimental role(s) interactor A',\n",
    "#                      'Experimental role(s) interactor B',\n",
    "#                      'Type(s) interactor A',\n",
    "#                      'Type(s) interactor B',\n",
    "#                      'Xref(s) interactor A',\n",
    "#                      'Xref(s) interactor B',\n",
    "#                      'Interaction Xref(s)',\n",
    "#                      'Annotation(s) interactor A',\n",
    "#                      'Annotation(s) interactor B',\n",
    "#                      'Interaction annotation(s)',\n",
    "#                      'Host organism(s)',\n",
    "#                      'Interaction parameter(s)',\n",
    "#                      'Creation date',\n",
    "#                      'Update date',\n",
    "#                      'Checksum(s) interactor A',\n",
    "#                      'Checksum(s) interactor B',\n",
    "#                      'Interaction Checksum(s)',\n",
    "#                      'Feature(s) interactor A',\n",
    "#                      'Feature(s) interactor B',\n",
    "#                      'Stoichiometry(s) interactor A',\n",
    "#                      'Stoichiometry(s) interactor B',\n",
    "#                      'Identification method participant A',\n",
    "#                      'Identification method participant B',\n",
    "#                      'Expansion method(s)'\n",
    "#                      ], axis=1, inplace=True)\n",
    "# intact_negative.loc[:, 'intact_score'] = intact_negative.loc[: , 'Confidence value(s)'].apply(intact_score_filter)\n",
    "# intact_negative['pair_id'] = intact_negative.apply(lambda row: str(tuple(sorted([row['#ID(s) interactor A'].replace('uniprotkb:', '').split('-')[0], row['ID(s) interactor B'].replace('uniprotkb:', '').split('-')[0]]))), axis=1)\n",
    "# intact_negative = intact_negative.sort_values('intact_score', ascending=False).drop_duplicates('pair_id', keep='first')\n",
    "# all_pairs_intact_negative = pd.merge(all_pairs, intact_negative, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_intact_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4c239",
   "metadata": {},
   "source": [
    "#### Blohm negatome2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatome2 = pd.read_csv('../../data/negatome2.0/combined.txt', sep='\\t', names=['ID_1', 'ID_2'])\n",
    "# negatome2['pair_id'] = negatome2.apply(lambda row: str(tuple(sorted([row['ID_1'].split('-')[0], row['ID_2'].split('-')[0]]))), axis=1)\n",
    "# all_pairs_negatome2 = pd.merge(all_pairs, negatome2, on='pair_id', how='inner')\n",
    "# print(len(all_pairs_negatome2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63893e",
   "metadata": {},
   "source": [
    "#### Stelzl 2005 negatome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stelzl_neg = pd.read_csv('../../data/16169070_neg.mitab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefb685",
   "metadata": {},
   "source": [
    "### Read in HPC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from all job data\n",
    "results_df_uc = pd.DataFrame(data=find_summary_files([HPC_RESULT_DIR]))\n",
    "\n",
    "# Print basic information about the DataFrame\n",
    "print(f\"Total jobs processed: {len(results_df_uc)}\")\n",
    "\n",
    "results_df_uc['pair_id'] = results_df_uc.apply(create_pair_id, axis=1)\n",
    "\n",
    "print(f\"jobs before cleaning: {len(results_df_uc)}\")\n",
    "results_df = clean_results(results_df_uc)\n",
    "print(f\"jobs after cleaning: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = pd.merge(results_df, string_df, on='pair_id', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, intact_cleaned, on='pair_id', how='left')\n",
    "\n",
    "# Add IUPred data for TF proteins\n",
    "tf_iupred_cols = ['Entry', 'num_disordered_regions', 'iupred3']\n",
    "tf_iupred_data = tf_proteins_curated_ds_IUPred3_diso[tf_iupred_cols].add_suffix('_tf')\n",
    "\n",
    "arm_cols = ['Entry']\n",
    "arm_data = armadillo_proteins[arm_cols].add_suffix('_arm')\n",
    "\n",
    "results_df_annotated['tf_id'] = results_df_annotated['job_name'].apply(lambda x: x.split('_')[2].upper())\n",
    "results_df_annotated['arm_id'] = results_df_annotated['job_name'].apply(lambda x: x.split('_')[0].upper())\n",
    "\n",
    "results_df_annotated = pd.merge(results_df_annotated, tf_iupred_data, left_on='tf_id', right_on='Entry_tf', how='left')\n",
    "results_df_annotated = pd.merge(results_df_annotated, arm_data, left_on='arm_id', right_on='Entry_arm', how='left')\n",
    "\n",
    "# remove rows where tf or arm not in dataset\n",
    "results_df_annotated = results_df_annotated[results_df_annotated['Entry_tf'].notna() & results_df_annotated['Entry_arm'].notna()]\n",
    "\n",
    "# Print information about the merged dataframe\n",
    "print(f\"Total number of modelled pairs: {len(results_df)}\")\n",
    "print(f\"Total rows in merged_df: {len(results_df_annotated)}\")\n",
    "print(f\"Rows with annotated data (STRING): {results_df_annotated['combined_score'].notna().sum()}\")\n",
    "print(f\"Rows with annotated data (IntAct): {results_df_annotated['intact_score'].notna().sum()}\")\n",
    "\n",
    "\n",
    "# convert all STRING scores from 0-1000 to 0-1 (linear conversion)\n",
    "STRING_COLS = ['experimental', 'database', 'textmining', 'combined_score']\n",
    "for col in STRING_COLS:\n",
    "    results_df_annotated[col] = results_df_annotated[col] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c008bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = '/home/markus/MPI_local/HPC_results/'\n",
    "results_df_annotated['model_path'] = results_df_annotated['job_name'].apply(lambda jn: path_prefix + jn.lower() + '_model.cif')\n",
    "results_df_annotated['model_path'].to_csv(PDB2NET_PREFIX + 'pipeline1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_pdb2net\n",
    "\n",
    "importlib.reload(functions_pdb2net)\n",
    "\n",
    "from functions_pdb2net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a617c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated_1 = results_df_annotated[results_df_annotated['job_name'] == \"O60287_1-2271_P11161_1-476\".lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d644d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_annotated = annotate_interface_tf(results_df_annotated, \"/home/markus/MPI_local/data/PDB2Net/pipeline1\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_struct = pd.DataFrame()\n",
    "%store -r up_ids_structure_ds\n",
    "\n",
    "# remove pairs in structure dataset\n",
    "up_ids_structure_ds_sorted = []\n",
    "for up_pair in up_ids_structure_ds:\n",
    "    up_pair_clean = [x.upper() for x in up_pair if pd.notna(x)]\n",
    "    up_pair_clean = tuple(sorted(up_pair_clean))\n",
    "    up_ids_structure_ds_sorted.append(up_pair_clean)\n",
    "\n",
    "for _, row in results_df_annotated.iterrows():\n",
    "    pair_tuple = eval(row['pair_id'])\n",
    "    if pair_tuple not in up_ids_structure_ds_sorted:\n",
    "        no_struct = pd.concat([no_struct, row.to_frame().T], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"removed {row['pair_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'interaction dataset' with modeled structures\n",
    "int_ds_results = no_struct[no_struct['combined_score'].notna() | no_struct['intact_score'].notna()]\n",
    "\n",
    "print(len(int_ds_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'prediction dataset' with modeled structures\n",
    "pred_ds_results = no_struct[~no_struct['combined_score'].notna()]\n",
    "pred_ds_results = pred_ds_results[~pred_ds_results['intact_score'].notna()]\n",
    "\n",
    "print(len(results_df_annotated))\n",
    "print(len(pred_ds_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314eee5",
   "metadata": {},
   "source": [
    "### Comparing AlphaFold ranking scores with STRING combined scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "STRING_SCORE_COLUMN = 'experimental'\n",
    "INTACT_SCORE_COLUMN = 'intact_score'\n",
    "\n",
    "create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'iptm', STRING_SCORE_COLUMN, ax=axes[0], scatter_threshold=2)\n",
    "create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ptm', STRING_SCORE_COLUMN, ax=axes[1], scatter_threshold=2)\n",
    "create_heatmap(results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]), 'ranking_score', STRING_SCORE_COLUMN, ax=axes[2], scatter_threshold=2)\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs STRING score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'iptm', INTACT_SCORE_COLUMN, ax=axes[0])\n",
    "create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ptm', INTACT_SCORE_COLUMN, ax=axes[1])\n",
    "create_heatmap(results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]), 'ranking_score', INTACT_SCORE_COLUMN, ax=axes[2])\n",
    "\n",
    "fig.suptitle('AlphaFold Metrics vs IntAct score, NAs dropped', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52841c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=results_df_annotated,\n",
    "    dataset1_name='all pairs', \n",
    "    dataset2=report_2_pairs,\n",
    "    dataset2_name='report 2',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "co = 0.9\n",
    "string_high_scoring = results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN])\n",
    "string_high_scoring = string_high_scoring[string_high_scoring[STRING_SCORE_COLUMN] > co]\n",
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=string_high_scoring,\n",
    "    dataset1_name=f\"STRING high scoring {co}\", \n",
    "    dataset2=results_df_annotated,\n",
    "    dataset2_name='all pairs',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]),\n",
    "    dataset1_name='STRING', \n",
    "    dataset2=results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]),\n",
    "    dataset2_name='IntAct',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=results_df_annotated.dropna(subset=[STRING_SCORE_COLUMN]),\n",
    "    dataset1_name='STRING', \n",
    "    dataset2=results_df_annotated,\n",
    "    dataset2_name='all pairs',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = create_overlaid_AF_violin_plot(\n",
    "    dataset1=results_df_annotated.dropna(subset=[INTACT_SCORE_COLUMN]),\n",
    "    dataset1_name='IntAct', \n",
    "    dataset2=results_df_annotated,\n",
    "    dataset2_name='all pairs',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32896a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: should I include duplicate values?\n",
    "prediction_IntAct = results_df_annotated[results_df_annotated[INTACT_SCORE_COLUMN].notna()]\n",
    "prediction_IntAct_plot = prediction_IntAct[prediction_IntAct['ranking_score'] >= 0]\n",
    "\n",
    "prediction_STRING = results_df_annotated[results_df_annotated[STRING_SCORE_COLUMN].notna()]\n",
    "prediction_STRING_plot = prediction_STRING[prediction_STRING['ranking_score'] >= 0]\n",
    "\n",
    "print(len(prediction_IntAct[prediction_IntAct['ranking_score']>=0.65]))\n",
    "print(len(prediction_IntAct[prediction_IntAct['ranking_score']<0.65]))\n",
    "print(len(prediction_STRING[prediction_STRING['ranking_score']>=0.65]))\n",
    "print(len(prediction_STRING[prediction_STRING['ranking_score']<0.65]))\n",
    "\n",
    "\n",
    "fig, ax = create_double_violin_plot(prediction_IntAct_plot, 'IntAct', prediction_STRING_plot, 'STRING (experimental)', 'ranking_score', 'ranking_score', (8,8))\n",
    "plt.axhline(y=0.65, color='red', linestyle='--', alpha=0.7, label='y=0.65')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a732c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_IntAct_cutoff = prediction_IntAct[prediction_IntAct[INTACT_SCORE_COLUMN] > 0.4]\n",
    "prediction_IntAct_cutoff_plot = prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score'] >= 0]\n",
    "\n",
    "prediction_STRING_cutoff = prediction_STRING[prediction_STRING[STRING_SCORE_COLUMN] > 0.4]\n",
    "prediction_STRING_cutoff_plot = prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0711e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score']>=0.65]))\n",
    "print(len(prediction_IntAct_cutoff[prediction_IntAct_cutoff['ranking_score']<0.65]))\n",
    "print(len(prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score']>=0.65]))\n",
    "print(len(prediction_STRING_cutoff[prediction_STRING_cutoff['ranking_score']<0.65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7891a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = create_double_violin_plot(prediction_IntAct_cutoff_plot, 'IntAct', prediction_STRING_cutoff_plot, 'STRING (experimental)', 'ranking_score', 'ranking_score', (8,8))\n",
    "plt.axhline(y=0.65, color='red', linestyle='--', alpha=0.7, label='y=0.65')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set = results_df_annotated[(~results_df_annotated[STRING_SCORE_COLUMN].notna()) & (~results_df_annotated[INTACT_SCORE_COLUMN].notna())]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "below_cutoff = prediction_set[(prediction_set['ranking_score'] < 0.65) & (prediction_set['ranking_score'] > 0)]['ranking_score']\n",
    "above_cutoff = prediction_set[prediction_set['ranking_score'] >= 0.65]['ranking_score']\n",
    "\n",
    "plt.hist(below_cutoff, bins=18, alpha=0.7, color='red', label=f'Below 0.65 (n={len(below_cutoff)})')\n",
    "plt.hist(above_cutoff, bins=17, alpha=0.7, color='green', label=f'Above/Equal 0.65 (n={len(above_cutoff)})')\n",
    "\n",
    "plt.axvline(x=0.65, color='black', linestyle='--', linewidth=2, label='Cutoff = 0.65')\n",
    "plt.xlabel('Ranking Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Ranking Scores in Prediction Set')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb91f86",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88025a",
   "metadata": {},
   "source": [
    "creating AUC curves with STRING / IntAct alone does not really make sense. My understanding is that both include only **positive** interaction candidates and assign scores to the certainty. So even a low score means a relatively high probability of interaction because the candidate is in the DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fe871",
   "metadata": {},
   "source": [
    "### new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_query = {\n",
    "  \"query\": {\n",
    "    \"type\": \"group\",\n",
    "    \"nodes\": [\n",
    "      {\n",
    "        \"type\": \"group\",\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entry_info.structure_determination_methodology\",\n",
    "              \"operator\": \"exact_match\",\n",
    "              \"value\": \"experimental\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entry_info.structure_determination_methodology\",\n",
    "              \"operator\": \"exact_match\",\n",
    "              \"value\": \"integrative\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"logical_operator\": \"or\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"group\",\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"1975-01-01\",\n",
    "                \"to\": \"1979-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"1980-01-01\",\n",
    "                \"to\": \"1984-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"1985-01-01\",\n",
    "                \"to\": \"1989-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"1990-01-01\",\n",
    "                \"to\": \"1994-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"1995-01-01\",\n",
    "                \"to\": \"1999-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"2000-01-01\",\n",
    "                \"to\": \"2004-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"2005-01-01\",\n",
    "                \"to\": \"2009-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"2010-01-01\",\n",
    "                \"to\": \"2014-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"2015-01-01\",\n",
    "                \"to\": \"2019-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_accession_info.initial_release_date\",\n",
    "              \"value\": {\n",
    "                \"from\": \"2020-01-01\",\n",
    "                \"to\": \"2024-12-31\",\n",
    "                \"include_lower\": True,\n",
    "                \"include_upper\": True\n",
    "              },\n",
    "              \"operator\": \"range\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"logical_operator\": \"or\",\n",
    "        \"label\": \"rcsb_accession_info.initial_release_date\"\n",
    "      }\n",
    "    ],\n",
    "    \"logical_operator\": \"and\",\n",
    "    \"label\": \"text\"\n",
    "  },\n",
    "  \"return_type\": \"entry\",\n",
    "  \"request_options\": {\n",
    "    \"scoring_strategy\": \"combined\",\n",
    "    \"results_content_type\": [\n",
    "      \"experimental\"\n",
    "    ],\n",
    "    \"sort\": [\n",
    "      {\n",
    "        \"sort_by\": \"rcsb_accession_info.initial_release_date\",\n",
    "        \"direction\": \"desc\"\n",
    "      }\n",
    "    ],\n",
    "    \"paginate\": {\n",
    "      \"start\": 0,\n",
    "      \"rows\": 10000\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_entries = fetch_all_pdb_results(pdb_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04960152",
   "metadata": {},
   "source": [
    "## 4. Structure Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from functions_blastp import *\n",
    "from download_functions import *\n",
    "from functions_job_creation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f955aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/*.csv')\n",
    "# review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/intersect_df - set3.csv')\n",
    "struct_ds = pd.concat([pd.read_csv(f) for f in review_files], ignore_index=True)\n",
    "struct_ds = struct_ds.drop_duplicates(subset=['pdb_id', 'query_tf', 'query_arm', 'chain_tf', 'chain_arm'])\n",
    "struct_ds['pair_id'] = struct_ds.apply(lambda row: str(tuple(sorted([row['query_tf'].split('|')[0].upper(), row['query_arm'].split('|')[0].upper()]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_ids = struct_ds['pdb_id'].drop_duplicates().to_list()\n",
    "for pdb_id in pdb_ids:\n",
    "    sequences = download_pdb_sequence(pdb_id)\n",
    "    if len(sequences) == 0:\n",
    "        print(f\"Empty sequence list: {pdb_id}\")\n",
    "        continue\n",
    "    job = create_alphafold_job_ms(pdb_id, sequences)\n",
    "    file_path = os.path.join('/home/markus/MPI_local/production1/PDB_modelling/', f\"{pdb_id}.json\")\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(job, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ds = struct_ds.drop(columns=[\n",
    "    \"chain_tf\", \"chain_arm\", \"%identity_tf\", \"%identity_arm\",\n",
    "    \"evalue_tf\", \"bit score_tf\", \"evalue_arm\", \"subject_tf\",\n",
    "    \"bit score_arm\", \"alignment length_tf\", \"mismatches_tf\", \"gap opens_tf\", \"q. start_tf\",\n",
    "    \"q. end_tf\", \"s. start_tf\", \"s. end_tf\", \"subject_arm\", \"alignment length_arm\",\n",
    "    \"mismatches_arm\", \"gap opens_arm\", \"q. start_arm\", \"q. end_arm\", \"s. start_arm\", \"s. end_arm\", \"Unnamed: 12\", \"Unnamed: 5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dec79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLAST_IDENTITY_CUTOFF: int|bool = False\n",
    "BLAST_SCORE_CUTOFF: int|bool = False\n",
    "BLAST_EVALUE_CUTOFF: float|bool = 0.00001\n",
    "BLAST_COVERAGE_CUTOFF: float|bool = 0.5\n",
    "TF_OUTPUT_DIR = \"/home/markus/MPI_local/production1/blastp_results/tf_blastp_no_e_lim_new_fmt\"\n",
    "ARM_OUTPUT_DIR = \"/home/markus/MPI_local/production1/blastp_results/tf_blastp_no_e_lim_new_fmt\"\n",
    "\n",
    "\n",
    "columns: List[str] = [\n",
    "    \"query\", \"subject\", \"%identity\", \"alignment length\", \"mismatches\", \"gap opens\",\n",
    "    \"q. start\", \"q. end\", \"s. start\", \"s. end\", \"evalue\", \"bit score\", \"% query coverage per subject\", \"% query coverage per hsp\", \"% query coverage per uniq subject\"\n",
    "]\n",
    "\n",
    "tf_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(TF_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(tf_blast_df))\n",
    "arm_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(ARM_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(arm_blast_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ds = struct_ds.merge(arm_blast_df.add_suffix('_arm'), how='left', left_on='query_arm', right_on='uniprot_id_arm')\n",
    "struct_ds = struct_ds.merge(tf_blast_df.add_suffix('_tf'), how='left', left_on='query_tf', right_on='uniprot_id_tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(struct_ds))\n",
    "struct_ds = struct_ds[struct_ds['pair_id'].isin(all_pairs['pair_id'])]\n",
    "print(len(struct_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_structs = all_pairs_struct_ds_annotated[all_pairs_struct_ds_annotated['comment'].str.contains('looks good|in complex', case=False, na=False)]\n",
    "\n",
    "# Get unique PDB IDs from reviews_df\n",
    "unique_pdb_ids = good_structs['pdb_id'].unique()\n",
    "print(f\"Found {len(unique_pdb_ids)} unique PDB IDs to download\")\n",
    "\n",
    "# Define download directory\n",
    "pdb_download_dir = \"/home/markus/MPI_local/data/PDB\"\n",
    "\n",
    "# Download each PDB structure (function handles duplicate checking)\n",
    "downloaded_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for pdb_id in unique_pdb_ids:\n",
    "    if pd.isna(pdb_id):  # Skip NaN values\n",
    "        continue\n",
    "        \n",
    "    result = download_pdb_structure(pdb_id, pdb_download_dir)\n",
    "    if result:\n",
    "        downloaded_count += 1\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Successfully processed: {downloaded_count}\")\n",
    "print(f\"Failed downloads: {failed_count}\")\n",
    "print(f\"Total processed: {len([pdb_id for pdb_id in unique_pdb_ids if not pd.isna(pdb_id)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "\n",
    "# Calculate DockQ scores using the function from functions_filtering.py\n",
    "good_structs = append_dockq(good_structs, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, all_uniprot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
