{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141332af",
   "metadata": {},
   "source": [
    "## Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_analysis import *\n",
    "import glob\n",
    "import pandas as pd\n",
    "from DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n",
    "from download_functions import *\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a84021",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/*.csv')\n",
    "# review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/intersect_df - set3.csv')\n",
    "reviews_df = pd.concat([pd.read_csv(f) for f in review_files], ignore_index=True)\n",
    "reviews_df = reviews_df.drop_duplicates(subset=['pdb_id', 'query_tf', 'query_arm', 'chain_tf', 'chain_arm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256c6c8",
   "metadata": {},
   "source": [
    "## Download PDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_structs = reviews_df[reviews_df['comment'].str.contains('looks good|in complex', case=False, na=False)]\n",
    "\n",
    "# Get unique PDB IDs from reviews_df\n",
    "unique_pdb_ids = good_structs['pdb_id'].unique()\n",
    "print(f\"Found {len(unique_pdb_ids)} unique PDB IDs to download\")\n",
    "\n",
    "# Define download directory\n",
    "pdb_download_dir = \"/home/markus/MPI_local/data/PDB\"\n",
    "\n",
    "# Download each PDB structure (function handles duplicate checking)\n",
    "downloaded_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for pdb_id in unique_pdb_ids:\n",
    "    if pd.isna(pdb_id):  # Skip NaN values\n",
    "        continue\n",
    "        \n",
    "    result = download_pdb_structure(pdb_id, pdb_download_dir)\n",
    "    if result:\n",
    "        downloaded_count += 1\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Successfully processed: {downloaded_count}\")\n",
    "print(f\"Failed downloads: {failed_count}\")\n",
    "print(f\"Total processed: {len([pdb_id for pdb_id in unique_pdb_ids if not pd.isna(pdb_id)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f395530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_job_name(id_tf: str, id_arm: str, df: pd.DataFrame):\n",
    "    row_arm = df[df['Entry'] == id_arm]\n",
    "    if row_arm.empty:\n",
    "        raise Exception(f\"Not found in df: {id_arm}\")\n",
    "    row_tf = df[df['Entry'] == id_tf]\n",
    "    if row_tf.empty:\n",
    "        raise Exception(f\"Not found in df: {id_tf}\")\n",
    "    length_arm = row_arm['Length'].iloc[0]\n",
    "    length_tf = row_tf['Length'].iloc[0]\n",
    "    return str.lower(f\"{id_arm}_1-{length_arm}_{id_tf}_1-{length_tf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abff039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_chain_mappings(native_chains, model_chains) -> List:\n",
    "    all_mappings = []\n",
    "    \n",
    "    if len(native_chains) > len(model_chains):\n",
    "        all_subsets = itertools.combinations(native_chains, len(model_chains))\n",
    "        for subset in all_subsets:\n",
    "            all_mappings.extend(all_bijective_mappings(subset, model_chains))\n",
    "    elif len(model_chains) > len(native_chains):\n",
    "        all_subsets = itertools.combinations(model_chains, len(native_chains))\n",
    "        for subset in all_subsets:\n",
    "            all_mappings.extend(all_bijective_mappings(native_chains, subset))\n",
    "    else:\n",
    "        all_mappings = all_bijective_mappings(native_chains, model_chains)\n",
    "    return all_mappings\n",
    "\n",
    "def all_bijective_mappings(A, B):\n",
    "    \"\"\"\n",
    "    Return a list containing every dictionary that maps each element of A\n",
    "    to a unique element of B.  A and B must be the same length.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each permutation of B, zip it with A to make a mapping dict\n",
    "    return [dict(zip(A, perm)) for perm in itertools.permutations(B)]\n",
    "\n",
    "# test get_all_chain_mappings()\n",
    "# Case 1: model_chains and native_chains have same length\n",
    "native_chains = ['A', 'B']\n",
    "model_chains = ['X', 'Y']\n",
    "mappings = list(get_all_chain_mappings(native_chains, model_chains))\n",
    "assert mappings == [{'A': 'X', 'B': 'Y'}, {'A': 'Y', 'B': 'X'}]\n",
    "# Case 2: model_chains > native_chains\n",
    "native_chains = ['A', 'B', 'C']\n",
    "model_chains = ['X', 'Y']\n",
    "mappings = get_all_chain_mappings(native_chains, model_chains)\n",
    "assert mappings == [{'A': 'X', 'B': 'Y'}, {'A': 'Y', 'B': 'X'}, {'A': 'X', 'C': 'Y'}, {'A': 'Y', 'C': 'X'}, {'B': 'X', 'C': 'Y'}, {'B': 'Y', 'C': 'X'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f123a",
   "metadata": {},
   "source": [
    "## calculate DockQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTEOME_PATHS = [\n",
    "    '/home/markus/MPI_local/data/Proteome/uniprotkb_proteome_UP000005640_2025_05_28.tsv',\n",
    "    '/home/markus/MPI_local/data/full_UP/uniprotkb_AND_reviewed_true_2025_07_10.tsv'\n",
    "]\n",
    "\n",
    "all_uniprot = pd.concat(\n",
    "    [pd.read_csv(path, low_memory=False, sep='\\t') for path in PROTEOME_PATHS],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_file_path(filename: str, search_dir: str):\n",
    "    \"\"\"search the file with filename in dir and return the full path if it exists, otherwise return False\n",
    "\n",
    "    Args:\n",
    "        filename (str): _description_\n",
    "    \"\"\"\n",
    "    path = Path(search_dir)\n",
    "    for file in path.rglob(filename):\n",
    "        return file  # return the first match\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "\n",
    "good_structs['chain_map'] = None\n",
    "good_structs['dockq_score'] = None\n",
    "\n",
    "no_model = []\n",
    "no_native = []\n",
    "\n",
    "# for every structure, calculate all dockQ score using all possible chain mappings and store the best score/mapping\n",
    "for index, row in good_structs.iterrows():\n",
    "    \n",
    "    job_name = get_job_name(row['query_tf'].split('|')[0], row['query_arm'].split('|')[0], all_uniprot)\n",
    "    model_path = get_file_path(f'{job_name}_model.cif', HPC_FULL_RESULTS_DIR)\n",
    "    native_path_cif = f'{NATIVE_PATH_PREFIX}{row[\"pdb_id\"].lower()}.cif'\n",
    "    \n",
    "    # Check if both paths exist before loading\n",
    "    if not model_path:\n",
    "        no_model.append((model_path,job_name))\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(native_path_cif):\n",
    "        no_native.append((native_path_cif,job_name))\n",
    "        continue\n",
    "    \n",
    "    model = load_PDB(model_path)\n",
    "    native = load_PDB(native_path_cif)\n",
    "    native_chains = [chain.id for chain in model]\n",
    "    model_chains = [chain.id for chain in native]\n",
    "    chain_map_dict = {}\n",
    "    if len(model_chains) > 2:\n",
    "        print(f\"{row[\"pdb_id\"]}: Warning: Native structure ({native}) has more than two chains!\")\n",
    "    for chain_map in get_all_chain_mappings(model_chains, native_chains):\n",
    "        try:\n",
    "            dockQ = run_on_all_native_interfaces(model, native, chain_map=chain_map)[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Exception for {row[\"pdb_id\"]}: {e}. Comment in review: {row['comment']}\")\n",
    "            break\n",
    "        chain_map_dict[str(chain_map)] = dockQ\n",
    "    if chain_map_dict:\n",
    "        best_chain_map = max(chain_map_dict.keys(), key=(lambda key: chain_map_dict[key]))\n",
    "        best_dockq_score = chain_map_dict[best_chain_map]\n",
    "        \n",
    "        # Store results in the DataFrame\n",
    "        good_structs.at[index, 'chain_map'] = best_chain_map\n",
    "        good_structs.at[index, 'dockq_score'] = best_dockq_score\n",
    "        \n",
    "        # print(f\"Best chain map: {best_chain_map}, DockQ: {best_dockq_score}\")\n",
    "\n",
    "if no_model:\n",
    "    print(\"Missing model files:\")\n",
    "    print(str([(t[0], t[1]) for t in no_model]))\n",
    "if no_native:\n",
    "    print(\"Missing native files:\")\n",
    "    for path in no_native:\n",
    "        print(f\"  NATIVE: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pair_id\n",
    "good_structs['pair_id'] = good_structs.apply(lambda row: str(tuple(sorted([row['query_arm'].split('|')[0].upper(), row['query_tf'].split('|')[0].upper()]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR_ALL = \"/home/markus/MPI_local/HPC_results\"\n",
    "from functions_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_all_uc = pd.DataFrame(data=find_summary_files(RESULTS_DIR_ALL))\n",
    "print(f\"Total jobs processed: {len(results_df_all_uc)}\")\n",
    "results_df_all_uc['pair_id'] = results_df_all_uc.apply(create_pair_id, axis=1)\n",
    "results_df_all = clean_results(results_df_all_uc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_structs_annotated = pd.merge(good_structs, results_df_all, how='left', on='pair_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b71c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_plot(good_structs_annotated, 'ranking_score', 'dockq_score')\n",
    "create_scatter_plot(good_structs_annotated, 'iptm', 'dockq_score')\n",
    "create_scatter_plot(good_structs_annotated, 'ptm', 'dockq_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import MMCIFParser, PDBIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836635f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdb_path(cif_path: str, pdb_dir: str) -> str:\n",
    "    \"\"\"for the .cif structure at cif_path, check if there is a pdb file available in pdb_dir.\n",
    "    If not, convert the .cif to a pdb file and store it in pdb_dir. Return the path to the pdb file\n",
    "\n",
    "    Args:\n",
    "        cif_path (str): Path to the input CIF file\n",
    "        pdb_dir (str): Directory to store/find PDB files\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the PDB file\n",
    "    \"\"\"\n",
    "    # Create pdb_dir if it doesn't exist\n",
    "    os.makedirs(pdb_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract filename without extension from cif_path\n",
    "    cif_filename = os.path.basename(cif_path)\n",
    "    pdb_filename = os.path.splitext(cif_filename)[0] + '.pdb'\n",
    "    pdb_path = os.path.join(pdb_dir, pdb_filename)\n",
    "    \n",
    "    # Check if PDB file already exists\n",
    "    if os.path.exists(pdb_path):\n",
    "        return pdb_path\n",
    "    \n",
    "    # Convert CIF to PDB\n",
    "    parser = MMCIFParser(QUIET=False)\n",
    "    structure = parser.get_structure(\"structure\", cif_path)\n",
    "    \n",
    "    # Write to PDB\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(pdb_path)\n",
    "    \n",
    "    return pdb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751049f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "TM_BIN = '/home/markus/MPI_local/bin/TMscore'\n",
    "USALIGN_BIN = '/home/markus/MPI_local/bin/USalign'\n",
    "def calc_USalign(model, native, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_name = Path(model).stem\n",
    "    native_name = Path(native).stem\n",
    "    \n",
    "    out_path = f\"{output_dir}/{model_name}--{native_name}\"\n",
    "    \n",
    "    if os.path.exists(out_path):\n",
    "        return out_path\n",
    "    \n",
    "    try:\n",
    "        with open(out_path, 'w') as output_file:\n",
    "            subprocess.run([\n",
    "                USALIGN_BIN,\n",
    "                model,\n",
    "                native,\n",
    "                \"-ter\", \"1\",\n",
    "                \"-mm\", \"1\",\n",
    "                \"-outfmt\", \"2\"\n",
    "            ], stdout=output_file, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_DIR = '../../production1/pdb_cache'\n",
    "ALIGN_DIR = '../../production1/us_align_out'\n",
    "\n",
    "\n",
    "us_align_score_df = pd.DataFrame()\n",
    "error_count = 0\n",
    "no_model = []\n",
    "no_native = []\n",
    "\n",
    "for index, row in good_structs.iterrows():\n",
    "    \n",
    "    job_name = get_job_name(row['query_tf'].split('|')[0], row['query_arm'].split('|')[0], all_uniprot)\n",
    "    model_path_cif = get_file_path(f'{job_name}_model.cif', HPC_FULL_RESULTS_DIR)\n",
    "    native_path_cif = f'{NATIVE_PATH_PREFIX}{row[\"pdb_id\"].lower()}.cif'\n",
    "    \n",
    "    \n",
    "    # Check if both paths exist before loading\n",
    "    if not model_path_cif:\n",
    "        no_model.append((model_path_cif,job_name))\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(native_path_cif):\n",
    "        no_native.append((native_path_cif,job_name))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        native_path_pdb = get_pdb_path(native_path_cif, PDB_DIR)\n",
    "        model_path_pdb = get_pdb_path(model_path_cif, PDB_DIR)\n",
    "        us_align = calc_USalign(model_path_pdb, native_path_pdb, ALIGN_DIR)\n",
    "        us_align_df = pd.read_csv(us_align, sep='\\t')\n",
    "        us_align_df['job_name'] = job_name\n",
    "        us_align_df['pdb_id'] = row['pdb_id']\n",
    "        us_align_score_df = pd.concat([us_align_score_df, us_align_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(e)\n",
    "        \n",
    "print(error_count)\n",
    "print(f\"{len(no_model)}\")\n",
    "print(f\"{len(no_native)}\")\n",
    "\n",
    "for list in no_model:\n",
    "    print(list)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
