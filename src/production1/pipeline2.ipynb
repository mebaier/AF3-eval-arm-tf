{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bb64e8",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bae167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *\n",
    "\n",
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import functions_pdb2net\n",
    "import functions_cif\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "importlib.reload(functions_pdb2net)\n",
    "importlib.reload(functions_cif)\n",
    "\n",
    "\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n",
    "from functions_pdb2net import *\n",
    "from functions_cif import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ace3d3",
   "metadata": {},
   "source": [
    "# 2. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9865b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/4/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee940caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with NaN in sequence field (ligands etc.)\n",
    "print(f\"Before removing NaN sequences: {len(rep)}\")\n",
    "rep = rep.dropna(subset=['Sequence'])\n",
    "print(f\"After removing NaN sequences: {len(rep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length filter\n",
    "MIN_LENGTH = 20\n",
    "\n",
    "print(f\"Before length filter: {len(rep)}\")\n",
    "rep['Sequence length'] = rep['Sequence'].fillna('').str.len()\n",
    "rep = rep[rep['Sequence length'] >= MIN_LENGTH]\n",
    "print(f\"After length filter: {len(rep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = add_iupred3(rep, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all PDBs that don't have at least 1 disordered chain\n",
    "print(len(rep))\n",
    "keep_pdbs = set()\n",
    "for _, row in rep.iterrows():\n",
    "    if row['num_disordered_regions'] > 0:\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "        \n",
    "rep = rep[rep['Entry ID'].isin(keep_pdbs)]\n",
    "\n",
    "print(len(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78150fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all PDBs that don't have at least 1 ordered chain\n",
    "print(len(rep))\n",
    "keep_pdbs = set()\n",
    "for _, row in rep.iterrows():\n",
    "    if row['num_disordered_regions'] == 0:\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "        \n",
    "rep = rep[rep['Entry ID'].isin(keep_pdbs)]\n",
    "\n",
    "print(len(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3227d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into directory for PDB2Net processing\n",
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = PDB2NET_PREFIX + 'pipeline2/'\n",
    "rep['model_path'] = rep['Entry ID'].apply(lambda id: path_prefix + id.lower() + '.cif')\n",
    "\n",
    "rep.drop_duplicates(subset=['model_path'], inplace=False)['model_path'].to_csv(PDB2NET_PREFIX + 'pipeline2.csv', index=False)\n",
    "\n",
    "# download pdb structures for pdb2net\n",
    "download_pdb_structures(set(rep['Entry ID'].tolist()), path_prefix, 'cif', '/home/markus/MPI_local/data/PDB', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "# define interface as having at least INTERFACE_MIN_ATOMS atoms within INTERFACE_MAX_DISTANCE A of each other\n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "INTERFACE_MAX_DISTANCE = 5 # higher not possible => change PDB2Net data\n",
    "PATH = '/home/markus/MPI_local/data/PDB2Net/pipeline2/2025-08-28_19-38-42'\n",
    "interfaces_df = get_interfaces_pdb2net(PATH, INTERFACE_MIN_ATOMS, INTERFACE_MAX_DISTANCE, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b055768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate interfaces based on UniProt IDs\n",
    "print(f\"Before deduplication: {len(interfaces_df)} interfaces\")\n",
    "\n",
    "interfaces_df['normalized_uniprot'] = interfaces_df['Uniprot IDs'].apply(normalize_uniprot_pair)\n",
    "interfaces_df = interfaces_df.drop_duplicates(subset=['normalized_uniprot'])\n",
    "interfaces_df = interfaces_df.drop('normalized_uniprot', axis=1)\n",
    "\n",
    "print(f\"After deduplication: {len(interfaces_df)} interfaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only interactions between ordered-disordered chain\n",
    "rep_reindex = rep.copy()\n",
    "\n",
    "rep_reindex.set_index(['Entry ID', 'Asym ID'], inplace=True)\n",
    "\n",
    "import json\n",
    "print(len(interfaces_df))\n",
    "\n",
    "for ind,row in interfaces_df.iterrows():\n",
    "    interface_id = json.loads(row['Interface ID'].replace('\\'', '\"'))\n",
    "    chainID_1 = interface_id[0]\n",
    "    chainID_2 = interface_id[1]\n",
    "    try:\n",
    "        disorder_chain1 = int(rep_reindex.loc[(row['Entry ID'], chainID_1), 'num_disordered_regions'])\n",
    "        disorder_chain2 = int(rep_reindex.loc[(row['Entry ID'], chainID_2), 'num_disordered_regions'])\n",
    "    except KeyError as e:\n",
    "        # Skip this interface if entry/chain not found\n",
    "        print(f\"Entry not found: {e}\")\n",
    "        interfaces_df.drop(ind, inplace=True)\n",
    "        continue\n",
    "    \n",
    "    if not ((disorder_chain1 == 0 and disorder_chain2 >= 1) or (disorder_chain2 == 0 and disorder_chain1 >= 1)):\n",
    "        interfaces_df.drop(ind, inplace=True)\n",
    "        \n",
    "print(len(interfaces_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd03826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interfaces with data\n",
    "interfaces_df['Release Date'] = interfaces_df['Entry ID'].map(rep.groupby('Entry ID')['Release Date'].first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaces_df['in_training_set'] = interfaces_df['Release Date'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29763d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_ids_structure_ds = interfaces_df['Uniprot IDs'].tolist()\n",
    "%store up_ids_structure_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3903ceb",
   "metadata": {},
   "source": [
    "# 3. Job Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *\n",
    "\n",
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import functions_pdb2net\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "importlib.reload(functions_pdb2net)\n",
    "\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n",
    "from functions_pdb2net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DIRS = []\n",
    "# BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "# interfaces_df.rename(columns={'Interface ID': 'Chains'}, inplace=True)\n",
    "# new_af_jobs, reference_jobs = create_job_batch_from_PDB_chains(interfaces_df, BATCH_DIRS, 5120)\n",
    "# write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_12')\n",
    "# write_af_jobs_to_individual_files(reference_jobs, '../../production1/PDB_modelling/reference', 'alphafold3', True)\n",
    "# interfaces_df.rename(columns={'Chains': 'Interface ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e1fb5",
   "metadata": {},
   "source": [
    "# 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = interfaces_df.copy()\n",
    "results_df.rename(columns={'Chains': 'Interface ID'}, inplace=True)\n",
    "results_df.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache_p2'\n",
    "JOB_DIR = '/home/markus/MPI_local/production1/PDB_modelling'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "results_df['job_name'] = results_df.apply(lambda row: row['Entry ID'] + \"_\" + \"_\".join(eval(row['Interface ID'])), axis=1)\n",
    "results_df.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "results_df, no_model = append_dockq_two_chainIDs(results_df, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, JOB_DIR, PDB_CACHE, DOCKQ_CACHE)\n",
    "results_df.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa78bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_model)\n",
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = annotate_AF_metrics(results_df, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.dropna(subset=['dockq_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1085a",
   "metadata": {},
   "source": [
    "# 5. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dad660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spd(df: pd.DataFrame, x_metric: str, y_metric: str,\n",
    "                        title: str = '', cmap: str = 'viridis', alpha: float = 0.9, size: int = 25, ax=None, corr=False, legend_position='left') -> None:\n",
    "\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = plt.gca()\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "    \n",
    "    # Create scatter plot with color mapping for boolean values\n",
    "    # Map boolean values to colors manually to ensure consistency\n",
    "    colors = df['in_training_set'].map({False: \"#4C72B0\", True: \"#DD8452\"})  # Orange for False, Blue for True\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        x=df[x_metric], \n",
    "        y=df[y_metric], \n",
    "        c=colors,\n",
    "        alpha=alpha,\n",
    "        s=size,\n",
    "        edgecolors='w'\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_metric, fontsize=12)\n",
    "    ax.set_ylabel('DockQ score', fontsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "    # Add a grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Create custom legend for boolean values with matching colors\n",
    "    import matplotlib.patches as mpatches\n",
    "    false_patch = mpatches.Patch(color='#4C72B0', label='Not in Training Set')\n",
    "    true_patch = mpatches.Patch(color='#DD8452', label='In Training Set')\n",
    "    \n",
    "    # Set legend position based on parameter\n",
    "    if legend_position == 'right':\n",
    "        legend_loc = 'upper right'\n",
    "    else:\n",
    "        legend_loc = 'upper left'\n",
    "    \n",
    "    ax.legend(loc=legend_loc, handles=[false_patch, true_patch])\n",
    "\n",
    "    # Add number of datapoints as a text box\n",
    "    num_points = len(df)\n",
    "    in_training_count = df['in_training_set'].sum()\n",
    "    not_in_training_count = num_points - in_training_count\n",
    "    in_training_pct = (in_training_count / num_points * 100) if num_points > 0 else 0\n",
    "    not_in_training_pct = (not_in_training_count / num_points * 100) if num_points > 0 else 0\n",
    "    \n",
    "    annotation_text = f'Datapoints: {num_points}\\nIn training: {in_training_count} ({in_training_pct:.1f}%)\\nNot in training: {not_in_training_count} ({not_in_training_pct:.1f}%)'\n",
    "    \n",
    "    # Calculate and add correlation coefficient if requested\n",
    "    if corr:\n",
    "        # Filter out rows with missing values for correlation calculation\n",
    "        valid_corr_data = df[[x_metric, y_metric]].dropna()\n",
    "        if len(valid_corr_data) > 1:  # Need at least 2 points for correlation\n",
    "            correlation = valid_corr_data[x_metric].corr(valid_corr_data[y_metric])\n",
    "            annotation_text += f'\\nPearson r: {correlation:.3f}'\n",
    "        else:\n",
    "            annotation_text += f'\\nPearson r: N/A (insufficient data)'\n",
    "    \n",
    "    # Set annotation position based on parameter\n",
    "    if legend_position == 'right':\n",
    "        xy_pos = (0.975, 0.72)\n",
    "        ha_align = 'right'\n",
    "    else:\n",
    "        xy_pos = (0.025, 0.72)\n",
    "        ha_align = 'left'\n",
    "    \n",
    "    ax.annotate(annotation_text, xy=xy_pos, xycoords='axes fraction', \n",
    "                fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                ha=ha_align)\n",
    "\n",
    "    # Show plot only if not using subplots\n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b832f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2-row subplot grid with 4 plots in first row and 3 plots in second row\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# First row: 4 plots\n",
    "spd(results_df, 'iptm', 'dockq_score', ax=axes[0, 0], corr=True)\n",
    "spd(results_df, 'ptm', 'dockq_score', ax=axes[0, 1], corr=True)\n",
    "spd(results_df, 'ranking_score', 'dockq_score', ax=axes[0, 2], corr=True)\n",
    "spd(results_df, 'chain_pair_iptm_max', 'dockq_score', ax=axes[0, 3], corr=True)\n",
    "\n",
    "# Second row: 3 plots (hide the 4th subplot)\n",
    "spd(results_df, 'chain_pair_iptm_min', 'dockq_score', ax=axes[1, 0], corr=True)\n",
    "spd(results_df, 'interface_pae_max', 'dockq_score', ax=axes[1, 1], corr=True, legend_position='right')\n",
    "spd(results_df, 'interface_pae_min', 'dockq_score', ax=axes[1, 2], corr=True, legend_position='right')\n",
    "axes[1, 3].set_visible(False)  # Hide the 4th subplot in second row\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_scatter.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Add a constant column to use for the x-axis to create a single split violin\n",
    "results_df['status'] = 'DockQ'\n",
    "\n",
    "# Use a split violin plot with bounded KDE using seaborn's default 'deep' palette\n",
    "ax = sns.violinplot (data=results_df, x='status', y='dockq_score', hue='in_training_set', \n",
    "                   split=True, palette='deep', inner='box', bw_adjust=0.3, cut=0.1)\n",
    "\n",
    "\n",
    "# Manually add legend for hue\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = ['Not in Training Set', 'In Training Set']\n",
    "ax.legend(handles, labels, title='in_training_set', loc='upper right')\n",
    "\n",
    "\n",
    "# Calculate counts for the legend\n",
    "total_points = len(results_df)\n",
    "in_training_count = results_df['in_training_set'].sum()\n",
    "not_in_training_count = total_points - in_training_count\n",
    "\n",
    "# Create legend text\n",
    "legend_text = (\n",
    "    f'Total points: {total_points}\\n'\n",
    "    f'In training set: {in_training_count}\\n'\n",
    "    f'Not in training set: {not_in_training_count}'\n",
    ")\n",
    "\n",
    "# Add legend to the plot using a text box\n",
    "at = AnchoredText(legend_text, prop=dict(size=10), frameon=True, loc='upper left')\n",
    "at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "ax.add_artist(at)\n",
    "\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('DockQ score by training set inclusion', fontsize=16)\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels([''])\n",
    "ax.set_ylabel('DockQ Score', fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Clean up the temporary column\n",
    "results_df.drop(columns=['status'], inplace=True)\n",
    "\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_violin.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram comparing DockQ > 0.23 success rates between training sets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define DockQ threshold\n",
    "DOCKQ_THRESHOLD = 0.23\n",
    "\n",
    "# Filter out NaN DockQ scores\n",
    "valid_data = results_df.dropna(subset=['dockq_score', 'in_training_set']).copy()\n",
    "\n",
    "# Calculate success rates for each group\n",
    "training_data = valid_data[valid_data['in_training_set'] == True]\n",
    "non_training_data = valid_data[valid_data['in_training_set'] == False]\n",
    "\n",
    "# Calculate percentages of structures with DockQ > 0.23\n",
    "training_success_rate = (training_data['dockq_score'] > DOCKQ_THRESHOLD).mean() * 100\n",
    "non_training_success_rate = (non_training_data['dockq_score'] > DOCKQ_THRESHOLD).mean() * 100\n",
    "\n",
    "# Calculate counts for detailed stats\n",
    "training_total = len(training_data)\n",
    "training_success = (training_data['dockq_score'] > DOCKQ_THRESHOLD).sum()\n",
    "non_training_total = len(non_training_data)\n",
    "non_training_success = (non_training_data['dockq_score'] > DOCKQ_THRESHOLD).sum()\n",
    "\n",
    "# Create the histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['In Training Set', 'Not in Training Set']\n",
    "success_rates = [training_success_rate, non_training_success_rate]\n",
    "colors = ['#DD8452', '#4C72B0']  # Same colors as previous plots\n",
    "\n",
    "# Create bars\n",
    "bars = ax.bar(categories, success_rates, color=colors, alpha=0.8, edgecolor='white', linewidth=1)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, (bar, rate) in enumerate(zip(bars, success_rates)):\n",
    "    height = bar.get_height()\n",
    "    if i == 0:  # Training set\n",
    "        count_text = f'{training_success}/{training_total}'\n",
    "    else:  # Non-training set\n",
    "        count_text = f'{non_training_success}/{non_training_total}'\n",
    "    \n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{rate:.1f}%\\n({count_text})',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_ylabel('Percentage of Structures with DockQ > 0.23', fontsize=12)\n",
    "ax.set_title(f'Success Rate Comparison: DockQ > {DOCKQ_THRESHOLD}', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(success_rates) * 1.2)  # Add some space above the bars\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add total sample sizes as text\n",
    "total_text = f'Total samples: In training = {training_total}, Not in training = {non_training_total}'\n",
    "ax.text(0.5, 0.02, total_text, transform=ax.transAxes, ha='center', \n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_success_rate_histogram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"DockQ Success Rate Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Threshold: DockQ > {DOCKQ_THRESHOLD}\")\n",
    "print()\n",
    "print(\"In Training Set:\")\n",
    "print(f\"  Total structures: {training_total}\")\n",
    "print(f\"  Successful (DockQ > {DOCKQ_THRESHOLD}): {training_success}\")\n",
    "print(f\"  Success rate: {training_success_rate:.2f}%\")\n",
    "print()\n",
    "print(\"Not in Training Set:\")\n",
    "print(f\"  Total structures: {non_training_total}\")\n",
    "print(f\"  Successful (DockQ > {DOCKQ_THRESHOLD}): {non_training_success}\")\n",
    "print(f\"  Success rate: {non_training_success_rate:.2f}%\")\n",
    "print()\n",
    "print(f\"Difference: {training_success_rate - non_training_success_rate:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39c1ea",
   "metadata": {},
   "source": [
    "## AUR / PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c172d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define DockQ cutoff threshold\n",
    "DOCKQ_CUTOFF = 0.23\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['iptm', 'ranking_score', 'ptm', 'chain_pair_iptm_max', 'chain_pair_iptm_min', \n",
    "           'interface_pae_max', 'interface_pae_min']\n",
    "\n",
    "# Create binary labels based on dockQ\n",
    "y_true = (results_df['dockq_score'] >= DOCKQ_CUTOFF).astype(int)\n",
    "\n",
    "# Report number of 1s and 0s in y_true\n",
    "print(\"Dataset Class Distribution:\")\n",
    "print(f\"Total samples: {len(y_true)}\")\n",
    "print(f\"Positive samples (dockQ >= {DOCKQ_CUTOFF}): {y_true.sum()} ({y_true.mean():.3f})\")\n",
    "print(f\"Negative samples (dockQ < {DOCKQ_CUTOFF}): {len(y_true) - y_true.sum()} ({1 - y_true.mean():.3f})\")\n",
    "print()\n",
    "\n",
    "print(\"AUC Results and Optimal Cutoffs:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "auc_results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    # Get predictions (higher values should indicate better models for most metrics)\n",
    "    # For PAE metrics, we need to invert since lower PAE is better\n",
    "    if 'pae' in metric:\n",
    "        y_scores = -results_df[metric]  # Invert PAE scores\n",
    "        original_scores = results_df[metric]  # Keep original for cutoff reporting\n",
    "    else:\n",
    "        y_scores = results_df[metric]\n",
    "        original_scores = results_df[metric]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(y_scores.isna() | y_true.isna())\n",
    "    y_scores_clean = y_scores[mask]\n",
    "    y_true_clean = y_true[mask]\n",
    "    original_scores_clean = original_scores[mask]\n",
    "    \n",
    "    if len(y_scores_clean) == 0:\n",
    "        print(f\"{metric}: No valid data\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate PR curve and AUC\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true_clean, y_scores_clean)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true_clean, y_scores_clean)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Find optimal cutoffs\n",
    "    # 1. F1-score optimal cutoff (from PR curve)\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
    "    f1_scores = np.nan_to_num(f1_scores)  # Handle division by zero\n",
    "    optimal_f1_idx = np.argmax(f1_scores)\n",
    "    optimal_f1_threshold = pr_thresholds[optimal_f1_idx]\n",
    "    optimal_f1_score = f1_scores[optimal_f1_idx]\n",
    "    \n",
    "    # 2. Youden's J statistic optimal cutoff (from ROC curve)\n",
    "    # J = sensitivity + specificity - 1 = tpr - fpr\n",
    "    youden_j = tpr - fpr\n",
    "    optimal_youden_idx = np.argmax(youden_j)\n",
    "    optimal_youden_threshold = roc_thresholds[optimal_youden_idx]\n",
    "    optimal_youden_j = youden_j[optimal_youden_idx]\n",
    "    \n",
    "    # Convert back to original scale for PAE metrics\n",
    "    if 'pae' in metric:\n",
    "        optimal_f1_threshold_original = -optimal_f1_threshold\n",
    "        optimal_youden_threshold_original = -optimal_youden_threshold\n",
    "    else:\n",
    "        optimal_f1_threshold_original = optimal_f1_threshold\n",
    "        optimal_youden_threshold_original = optimal_youden_threshold\n",
    "    \n",
    "    auc_results[metric] = {\n",
    "        'PR_AUC': pr_auc, \n",
    "        'ROC_AUC': roc_auc,\n",
    "        'optimal_f1_threshold': optimal_f1_threshold_original,\n",
    "        'optimal_f1_score': optimal_f1_score,\n",
    "        'optimal_youden_threshold': optimal_youden_threshold_original,\n",
    "        'optimal_youden_j': optimal_youden_j\n",
    "    }\n",
    "    \n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"  Optimal F1 cutoff: {optimal_f1_threshold_original:.4f} (F1 = {optimal_f1_score:.4f})\")\n",
    "    print(f\"  Optimal Youden cutoff: {optimal_youden_threshold_original:.4f} (J = {optimal_youden_j:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Create a histogram of ROC-AUC values for all metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract ROC-AUC values and sort them in descending order\n",
    "roc_auc_values = {m: auc_results[m]['ROC_AUC'] for m in metrics if m in auc_results}\n",
    "sorted_metrics = sorted(roc_auc_values.keys(), key=lambda x: roc_auc_values[x], reverse=True)\n",
    "sorted_values = [roc_auc_values[m] for m in sorted_metrics]\n",
    "\n",
    "# Create more readable labels by replacing underscores with spaces and capitalizing\n",
    "readable_labels = [m.replace('_', ' ').capitalize() for m in sorted_metrics]\n",
    "\n",
    "# Use seaborn deep color palette for the bars\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"deep\", len(sorted_metrics))\n",
    "\n",
    "# Create the bar chart with different colors for each bar\n",
    "bars = plt.bar(range(len(sorted_metrics)), sorted_values, color=colors, edgecolor='none')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(sorted_values):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Get class distribution for legend\n",
    "pos_samples = y_true.sum()\n",
    "neg_samples = len(y_true) - pos_samples\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks(range(len(sorted_metrics)), readable_labels, rotation=45, ha='right')\n",
    "plt.ylim(0.5, 1.0)  # Start y-axis from 0.5 to better show differences\n",
    "plt.title(f'ROC-AUC values by metric (DockQ score cutoff ≥ {DOCKQ_CUTOFF})', fontsize=16)\n",
    "plt.ylabel('ROC-AUC', fontsize=12)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random classifier (AUC=0.5)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.legend(['Random classifier (AUC=0.5)', f'Metrics (pos: {pos_samples}, neg: {neg_samples})'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_bar.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386058e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot for ROC-AUC values at different DockQ cutoffs\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Define DockQ cutoff thresholds\n",
    "DOCKQ_CUTOFFS = [0.23, 0.80]\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['iptm', 'ranking_score', 'ptm', 'chain_pair_iptm_max', 'chain_pair_iptm_min',\n",
    "           'interface_pae_max', 'interface_pae_min']\n",
    "\n",
    "# Store results for both cutoffs\n",
    "comparison_results = {}\n",
    "\n",
    "for cutoff in DOCKQ_CUTOFFS:\n",
    "    # Create binary labels based on dockQ cutoff\n",
    "    y_true = (results_df['dockq_score'] >= cutoff).astype(int)\n",
    "\n",
    "    # Report class distribution\n",
    "    print(f\"Class Distribution for DockQ cutoff ≥ {cutoff}:\")\n",
    "    print(f\"Positive samples: {y_true.sum()} ({y_true.mean():.3f})\")\n",
    "    print(f\"Negative samples: {len(y_true) - y_true.sum()} ({1 - y_true.mean():.3f})\")\n",
    "    print()\n",
    "    \n",
    "    cutoff_results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Get predictions (higher values should indicate better models for most metrics)\n",
    "        # For PAE metrics, we need to invert since lower PAE is better\n",
    "        if 'pae' in metric:\n",
    "            y_scores = -results_df[metric]  # Invert PAE scores\n",
    "        else:\n",
    "            y_scores = results_df[metric]\n",
    "        \n",
    "        # Remove NaN values\n",
    "        mask = ~(y_scores.isna() | y_true.isna())\n",
    "        y_scores_clean = y_scores[mask]\n",
    "        y_true_clean = y_true[mask]\n",
    "        \n",
    "        if len(y_scores_clean) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate ROC curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(y_true_clean, y_scores_clean)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        cutoff_results[metric] = roc_auc\n",
    "    \n",
    "    comparison_results[cutoff] = cutoff_results\n",
    "\n",
    "# Create the comparison bar plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for plotting\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35  # Width of bars\n",
    "\n",
    "# Get ROC-AUC values for both cutoffs\n",
    "roc_auc_023 = [comparison_results[0.23].get(m, np.nan) for m in metrics]\n",
    "roc_auc_080 = [comparison_results[0.80].get(m, np.nan) for m in metrics]\n",
    "\n",
    "# Create more readable labels\n",
    "readable_labels = [m.replace('_', ' ').capitalize() for m in metrics]\n",
    "\n",
    "# Use different color palettes for each cutoff\n",
    "colors_deep = sns.color_palette(\"deep\", len(metrics))\n",
    "colors_bright = sns.color_palette(\"dark\", len(metrics))\n",
    "\n",
    "# Create grouped bar chart with different colors for each metric\n",
    "bars1 = plt.bar(x_pos - width/2, roc_auc_023, width, label='DockQ ≥ 0.23', \n",
    "                color=colors_deep, alpha=0.8, edgecolor='none')\n",
    "bars2 = plt.bar(x_pos + width/2, roc_auc_080, width, label='DockQ ≥ 0.80', \n",
    "                color=colors_bright, alpha=0.8, edgecolor='none')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (v1, v2) in enumerate(zip(roc_auc_023, roc_auc_080)):\n",
    "    if not np.isnan(v1):\n",
    "        plt.text(i - width/2, v1 + 0.01, f'{v1:.3f}', ha='center', fontsize=9)\n",
    "    if not np.isnan(v2):\n",
    "        plt.text(i + width/2, v2 + 0.01, f'{v2:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "# Get sample counts for legend\n",
    "y_true_023 = (results_df['dockq_score'] >= 0.23).astype(int)\n",
    "y_true_080 = (results_df['dockq_score'] >= 0.80).astype(int)\n",
    "pos_023 = y_true_023.sum()\n",
    "neg_023 = len(y_true_023) - pos_023\n",
    "pos_080 = y_true_080.sum()\n",
    "neg_080 = len(y_true_080) - pos_080\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks(x_pos, readable_labels, rotation=45, ha='right')\n",
    "plt.ylim(0.5, 1.0)  # Start y-axis from 0.5 to better show differences\n",
    "plt.title('ROC-AUC comparison across DockQ cutoffs', fontsize=16)\n",
    "plt.ylabel('ROC-AUC', fontsize=12)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random classifier (AUC=0.5)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Create custom legend with sample counts\n",
    "legend_labels = [\n",
    "    'Random classifier (AUC=0.5)',\n",
    "    f'DockQ ≥ 0.23 (pos: {pos_023}, neg: {neg_023})',\n",
    "    f'DockQ ≥ 0.80 (pos: {pos_080}, neg: {neg_080})',\n",
    "]\n",
    "plt.legend(legend_labels, fontsize=11)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_bar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nROC-AUC Comparison Table:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'DockQ ≥ 0.23':<15} {'DockQ ≥ 0.80':<15} {'Difference':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric in metrics:\n",
    "    auc_023 = comparison_results[0.23].get(metric, np.nan)\n",
    "    auc_080 = comparison_results[0.80].get(metric, np.nan)\n",
    "    diff = auc_023 - auc_080 if not (np.isnan(auc_023) or np.isnan(auc_080)) else np.nan\n",
    "    \n",
    "    auc_023_str = f\"{auc_023:.4f}\" if not np.isnan(auc_023) else \"N/A\"\n",
    "    auc_080_str = f\"{auc_080:.4f}\" if not np.isnan(auc_080) else \"N/A\"\n",
    "    diff_str = f\"{diff:+.4f}\" if not np.isnan(diff) else \"N/A\"\n",
    "    \n",
    "    print(f\"{metric:<25} {auc_023_str:<15} {auc_080_str:<15} {diff_str:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall and ROC curves for ranking_score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Get ranking_score data\n",
    "metric = 'ranking_score'\n",
    "y_scores = results_df[metric]\n",
    "y_true = (results_df['dockq_score'] >= 0.23).astype(int)\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~(y_scores.isna() | y_true.isna())\n",
    "y_scores_clean = y_scores[mask]\n",
    "y_true_clean = y_true[mask]\n",
    "\n",
    "# Calculate and plot PR curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_true_clean, y_scores_clean)\n",
    "pr_auc = auc(recall, precision)\n",
    "ax1.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "\n",
    "# Calculate and plot ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_true_clean, y_scores_clean)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "\n",
    "# Find optimal cutoff using Youden's J statistic\n",
    "youden_j = tpr - fpr\n",
    "optimal_youden_idx = np.argmax(youden_j)\n",
    "optimal_youden_threshold = roc_thresholds[optimal_youden_idx]\n",
    "optimal_youden_tpr = tpr[optimal_youden_idx]\n",
    "optimal_youden_fpr = fpr[optimal_youden_idx]\n",
    "\n",
    "# Find corresponding point on PR curve for the same threshold\n",
    "# Find the closest threshold in PR curve\n",
    "pr_threshold_idx = np.argmin(np.abs(pr_thresholds - optimal_youden_threshold))\n",
    "optimal_precision = precision[pr_threshold_idx]\n",
    "optimal_recall = recall[pr_threshold_idx]\n",
    "\n",
    "# Add optimal points to both plots\n",
    "ax1.plot(optimal_recall, optimal_precision, 'ro', markersize=8, \n",
    "         label=f'Youden Optimal (cutoff={optimal_youden_threshold:.3f})')\n",
    "ax2.plot(optimal_youden_fpr, optimal_youden_tpr, 'ro', markersize=8,\n",
    "         label=f'Youden Optimal (cutoff={optimal_youden_threshold:.3f})')\n",
    "\n",
    "# Customize plots\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title(f'Precision-Recall Curve - {metric}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title(f'ROC Curve - {metric}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1803fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Display TPR and FPR for specific cutoff values\n",
    "print(f\"\\nROC Analysis for {metric}:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(y_true_clean)}\")\n",
    "print(f\"Positive samples: {y_true_clean.sum()}\")\n",
    "print(f\"Negative samples: {len(y_true_clean) - y_true_clean.sum()}\")\n",
    "print()\n",
    "\n",
    "# Define specific cutoff values to analyze\n",
    "cutoff_values = [0.23, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "print(\"TPR and FPR for specific cutoff values:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Cutoff':<8} {'TPR':<8} {'FPR':<8} {'Precision':<10} {'Recall':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for cutoff in cutoff_values:\n",
    "    # Find the closest threshold in the ROC curve\n",
    "    if len(roc_thresholds) > 0:\n",
    "        closest_idx = np.argmin(np.abs(roc_thresholds - cutoff))\n",
    "        \n",
    "        # Get TPR and FPR at this threshold\n",
    "        tpr_at_cutoff = tpr[closest_idx]\n",
    "        fpr_at_cutoff = fpr[closest_idx]\n",
    "        actual_threshold = roc_thresholds[closest_idx]\n",
    "        \n",
    "        # Calculate precision at this cutoff\n",
    "        # Precision = TP / (TP + FP)\n",
    "        # We can derive this from TPR and FPR\n",
    "        n_pos = y_true_clean.sum()\n",
    "        n_neg = len(y_true_clean) - n_pos\n",
    "        \n",
    "        tp = tpr_at_cutoff * n_pos\n",
    "        fp = fpr_at_cutoff * n_neg\n",
    "        \n",
    "        if (tp + fp) > 0:\n",
    "            precision_at_cutoff = tp / (tp + fp)\n",
    "        else:\n",
    "            precision_at_cutoff = 0\n",
    "        \n",
    "        print(f\"{actual_threshold:<8.3f} {tpr_at_cutoff:<8.3f} {fpr_at_cutoff:<8.3f} {precision_at_cutoff:<10.3f} {tpr_at_cutoff:<8.3f}\")\n",
    "\n",
    "# Find optimal operating points\n",
    "print(\"\\nOptimal Operating Points:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Youden's J statistic (maximize TPR - FPR)\n",
    "youden_j = tpr - fpr\n",
    "optimal_youden_idx = np.argmax(youden_j)\n",
    "optimal_youden_threshold = roc_thresholds[optimal_youden_idx]\n",
    "optimal_youden_tpr = tpr[optimal_youden_idx]\n",
    "optimal_youden_fpr = fpr[optimal_youden_idx]\n",
    "\n",
    "print(f\"Youden's J optimal point:\")\n",
    "print(f\"  Threshold: {optimal_youden_threshold:.4f}\")\n",
    "print(f\"  TPR: {optimal_youden_tpr:.4f}\")\n",
    "print(f\"  FPR: {optimal_youden_fpr:.4f}\")\n",
    "print(f\"  J-statistic: {youden_j[optimal_youden_idx]:.4f}\")\n",
    "\n",
    "# Point closest to top-left corner (minimize sqrt((1-TPR)^2 + FPR^2))\n",
    "distances = np.sqrt((1 - tpr)**2 + fpr**2)\n",
    "optimal_distance_idx = np.argmin(distances)\n",
    "optimal_distance_threshold = roc_thresholds[optimal_distance_idx]\n",
    "optimal_distance_tpr = tpr[optimal_distance_idx]\n",
    "optimal_distance_fpr = fpr[optimal_distance_idx]\n",
    "\n",
    "print(f\"\\nClosest to top-left corner:\")\n",
    "print(f\"  Threshold: {optimal_distance_threshold:.4f}\")\n",
    "print(f\"  TPR: {optimal_distance_tpr:.4f}\")\n",
    "print(f\"  FPR: {optimal_distance_fpr:.4f}\")\n",
    "print(f\"  Distance: {distances[optimal_distance_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define DockQ score categories with their ranges and labels\n",
    "# Update the High Quality category to use the DOCKQ_CUTOFF variable\n",
    "categories = {\n",
    "    'Incorrect': [0, 0.23],\n",
    "    'Acceptable Quality': [0.23, 0.49],\n",
    "    'Medium Quality': [0.49, 0.80],\n",
    "    'High Quality': [0.80, 1.0]\n",
    "}\n",
    "\n",
    "# First, filter out NaN DockQ scores\n",
    "valid_dockq_df = results_df.dropna(subset=['dockq_score']).copy()\n",
    "print(f\"Total data points: {len(results_df)}\")\n",
    "print(f\"Data points with valid DockQ scores: {len(valid_dockq_df)}\")\n",
    "print(f\"Data points with NaN DockQ scores: {len(results_df) - len(valid_dockq_df)}\")\n",
    "\n",
    "# Create a new column to categorize each data point\n",
    "def categorize_dockq(score):\n",
    "    for category, (lower, upper) in categories.items():\n",
    "        if lower <= score < upper or (category == 'High Quality' and score == upper):\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "valid_dockq_df['dockq_category'] = valid_dockq_df['dockq_score'].apply(categorize_dockq)\n",
    "\n",
    "# Count the number of data points in each category\n",
    "category_counts = valid_dockq_df['dockq_category'].value_counts().reindex(categories.keys())\n",
    "total_points = len(valid_dockq_df)\n",
    "\n",
    "# Calculate percentages for each category\n",
    "percentages = (category_counts / total_points * 100).round(1)\n",
    "\n",
    "# Create labels for the legend with counts and percentages\n",
    "labels = [f'{cat}: {count} ({pct}%)' for cat, count, pct in \n",
    "          zip(category_counts.index, category_counts, percentages)]\n",
    "\n",
    "# Use seaborn colors but arrange them in order for low to high quality\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette(\"deep\")\n",
    "colors = [palette[3], palette[1], palette[0], palette[2]]  # Red, Yellow-ish, Blue, Green\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create the pie chart with labels directly on slices\n",
    "wedges, texts, autotexts = plt.pie(category_counts, colors=colors, labels=labels,\n",
    "                                  wedgeprops={'edgecolor': 'w', 'linewidth': 1},\n",
    "                                  autopct='', labeldistance=1.1, textprops={'fontsize': 11})\n",
    "\n",
    "plt.title(f'Distribution of DockQ score categories \\nn = {total_points}', \n",
    "          fontsize=16, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_pie.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DockQ scores sorted from high to low with release date and entry ID\n",
    "# First, select only the necessary columns and remove rows with NaN DockQ scores\n",
    "score_df = results_df[['Entry ID', 'Interface ID', 'Release Date', 'dockq_score']].dropna(subset=['dockq_score']).copy()\n",
    "\n",
    "# Sort by DockQ score from highest to lowest\n",
    "score_df = score_df.sort_values(by='dockq_score', ascending=False)\n",
    "\n",
    "# Format the output with consistent spacing\n",
    "print(f\"{'Entry ID':<10} {'Release Date':<12} {'DockQ Score':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Print each row\n",
    "for _, row in score_df.iterrows():\n",
    "    print(f\"{row['Entry ID']:<10} {row['Interface ID']:<10} {row['Release Date']:<12} {row['dockq_score']:.4f}\")\n",
    "\n",
    "# Print some summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total entries: {len(score_df)}\")\n",
    "print(f\"Average DockQ score: {score_df['dockq_score'].mean():.4f}\")\n",
    "print(f\"Median DockQ score: {score_df['dockq_score'].median():.4f}\")\n",
    "print(f\"Highest DockQ score: {score_df['dockq_score'].max():.4f}\")\n",
    "print(f\"Lowest DockQ score: {score_df['dockq_score'].min():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
