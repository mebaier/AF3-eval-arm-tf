{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bb64e8",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bae167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *\n",
    "\n",
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import functions_pdb2net\n",
    "import functions_cif\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "importlib.reload(functions_pdb2net)\n",
    "importlib.reload(functions_cif)\n",
    "\n",
    "\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n",
    "from functions_pdb2net import *\n",
    "from functions_cif import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ace3d3",
   "metadata": {},
   "source": [
    "# 2. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9865b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = pd.read_csv('/home/markus/MPI_local/data/PDB_reports/4/combined_pdb_reports_processed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee940caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with NaN in sequence field\n",
    "print(f\"Before removing NaN sequences: {len(rep)}\")\n",
    "rep = rep.dropna(subset=['Sequence'])\n",
    "print(f\"After removing NaN sequences: {len(rep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length filter\n",
    "MIN_LENGTH = 20\n",
    "\n",
    "print(f\"Before length filter: {len(rep)}\")\n",
    "rep['Sequence length'] = rep['Sequence'].fillna('').str.len()\n",
    "rep = rep[rep['Sequence length'] >= MIN_LENGTH]\n",
    "print(f\"After length filter: {len(rep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = add_iupred3(rep, 'long', 'no', IUPRED_CACHE_DIR, IUPRED3_THRESHOLD, MIN_LENGTH_DISORDERED_REGION, IUPRED3_PATH, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all PDBs that don't have at least 1 disordered chain\n",
    "print(len(rep))\n",
    "keep_pdbs = set()\n",
    "for _, row in rep.iterrows():\n",
    "    if row['num_disordered_regions'] > 0:\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "        \n",
    "rep = rep[rep['Entry ID'].isin(keep_pdbs)]\n",
    "\n",
    "print(len(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78150fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all PDBs that don't have at least 1 ordered chain\n",
    "print(len(rep))\n",
    "keep_pdbs = set()\n",
    "for _, row in rep.iterrows():\n",
    "    if row['num_disordered_regions'] == 0:\n",
    "        keep_pdbs.add(row['Entry ID'])\n",
    "        \n",
    "rep = rep[rep['Entry ID'].isin(keep_pdbs)]\n",
    "\n",
    "print(len(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3227d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write into directory for PDB2Net processing\n",
    "PDB2NET_PREFIX = '/home/markus/PDB2Net/in/'\n",
    "path_prefix = PDB2NET_PREFIX + 'pipeline2/'\n",
    "rep['model_path'] = rep['Entry ID'].apply(lambda id: path_prefix + id.lower() + '.cif')\n",
    "\n",
    "rep.drop_duplicates(subset=['model_path'], inplace=False)['model_path'].to_csv(PDB2NET_PREFIX + 'pipeline2.csv', index=False)\n",
    "\n",
    "# download pdb structures for pdb2net\n",
    "download_pdb_structures(set(rep['Entry ID'].tolist()), path_prefix, 'cif', '/home/markus/MPI_local/data/PDB', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interface\n",
    "# define interface as having at least INTERFACE_MIN_ATOMS atoms within INTERFACE_MAX_DISTANCE A of each other\n",
    "INTERFACE_MIN_ATOMS = 10\n",
    "INTERFACE_MAX_DISTANCE = 5 # higher not possible => change PDB2Net data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/markus/MPI_local/data/PDB2Net/pipeline2/2025-08-28_19-38-42'\n",
    "interfaces_df = get_interfaces_pdb2net(PATH, INTERFACE_MIN_ATOMS, INTERFACE_MAX_DISTANCE, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b055768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate interfaces based on UniProt IDs\n",
    "print(f\"Before deduplication: {len(interfaces_df)} interfaces\")\n",
    "\n",
    "interfaces_df['normalized_uniprot'] = interfaces_df['Uniprot IDs'].apply(normalize_uniprot_pair)\n",
    "interfaces_df = interfaces_df.drop_duplicates(subset=['normalized_uniprot'])\n",
    "interfaces_df = interfaces_df.drop('normalized_uniprot', axis=1)\n",
    "\n",
    "print(f\"After deduplication: {len(interfaces_df)} interfaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only interactions between ordered-disordered chain\n",
    "rep_reindex = rep.copy()\n",
    "\n",
    "rep_reindex.set_index(['Entry ID', 'Asym ID'], inplace=True)\n",
    "\n",
    "import json\n",
    "print(len(interfaces_df))\n",
    "\n",
    "for ind,row in interfaces_df.iterrows():\n",
    "    interface_id = json.loads(row['Interface ID'].replace('\\'', '\"'))\n",
    "    chainID_1 = interface_id[0]\n",
    "    chainID_2 = interface_id[1]\n",
    "    try:\n",
    "        disorder_chain1 = int(rep_reindex.loc[(row['Entry ID'], chainID_1), 'num_disordered_regions'])\n",
    "        disorder_chain2 = int(rep_reindex.loc[(row['Entry ID'], chainID_2), 'num_disordered_regions'])\n",
    "    except KeyError as e:\n",
    "        # Skip this interface if entry/chain not found\n",
    "        # print(f\"Entry not found: {e}\")\n",
    "        interfaces_df.drop(ind, inplace=True)\n",
    "        continue\n",
    "    disorder_chain1 = int(rep_reindex.loc[(row['Entry ID'], chainID_1), 'num_disordered_regions'])\n",
    "    disorder_chain2 = int(rep_reindex.loc[(row['Entry ID'], chainID_2), 'num_disordered_regions'])\n",
    "    \n",
    "    if not ((disorder_chain1 == 0 and disorder_chain2 >= 1) or (disorder_chain2 == 0 and disorder_chain1 >= 1)):\n",
    "        interfaces_df.drop(ind, inplace=True)\n",
    "        \n",
    "print(len(interfaces_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd03826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate interfaces with data\n",
    "interfaces_df['Release Date'] = interfaces_df['Entry ID'].map(rep.groupby('Entry ID')['Release Date'].first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaces_df['in_training_set'] = interfaces_df['Release Date'] <= AF_TRAINING_CUTOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29763d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_ids_structure_ds = interfaces_df['Uniprot IDs'].tolist()\n",
    "%store up_ids_structure_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3903ceb",
   "metadata": {},
   "source": [
    "# 3. Job Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functions_filtering import *\n",
    "\n",
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import *\n",
    "\n",
    "import functions_analysis\n",
    "import functions_job_creation\n",
    "import functions_filtering\n",
    "import functions_plotting\n",
    "import functions_download\n",
    "import functions_pdb2net\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(functions_analysis)\n",
    "importlib.reload(functions_job_creation)\n",
    "importlib.reload(functions_filtering)\n",
    "importlib.reload(functions_download)\n",
    "importlib.reload(functions_plotting)\n",
    "importlib.reload(functions_pdb2net)\n",
    "\n",
    "from functions_analysis import *\n",
    "from functions_job_creation import *\n",
    "from functions_download import *\n",
    "from functions_filtering import *\n",
    "from functions_plotting import *\n",
    "from functions_pdb2net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_DIRS = []\n",
    "# BATCH_DIRS.extend([os.path.join('../../production1/PDB_modelling/', d) for d in os.listdir('../../production1/PDB_modelling') if os.path.isdir(os.path.join('../../production1/PDB_modelling', d)) and 'batch' in d])\n",
    "\n",
    "# interfaces_df.rename(columns={'Interface ID': 'Chains'}, inplace=True)\n",
    "# new_af_jobs, reference_jobs = create_job_batch_from_PDB_chains(interfaces_df, BATCH_DIRS, 5120)\n",
    "# write_af_jobs_to_individual_files(new_af_jobs, '../../production1/PDB_modelling/batch_12')\n",
    "# write_af_jobs_to_individual_files(reference_jobs, '../../production1/PDB_modelling/reference', 'alphafold3', True)\n",
    "# interfaces_df.rename(columns={'Chains': 'Interface ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e1fb5",
   "metadata": {},
   "source": [
    "# 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = interfaces_df.copy()\n",
    "results_df.rename(columns={'Chains': 'Interface ID'}, inplace=True)\n",
    "results_df.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NATIVE_PATH_PREFIX = \"/home/markus/MPI_local/data/PDB/\"\n",
    "HPC_FULL_RESULTS_DIR = \"/home/markus/MPI_local/HPC_results_full\"\n",
    "PDB_CACHE = '../../production1/pdb_cache'\n",
    "DOCKQ_CACHE = '../../production1/dockq_cache_p2'\n",
    "JOB_DIR = '/home/markus/MPI_local/production1/PDB_modelling'\n",
    "# rename columns for compatibility with annotate_dockq()\n",
    "results_df['job_name'] = results_df.apply(lambda row: row['Entry ID'] + \"_\" + \"_\".join(eval(row['Interface ID'])), axis=1)\n",
    "results_df.rename(columns={'Entry ID': 'pdb_id'}, inplace=True)\n",
    "results_df, no_model = append_dockq_two_chainIDs(results_df, NATIVE_PATH_PREFIX, HPC_FULL_RESULTS_DIR, JOB_DIR, PDB_CACHE, DOCKQ_CACHE)\n",
    "results_df.rename(columns={'pdb_id': 'Entry ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa78bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_model)\n",
    "find_job_files(no_model, '/home/markus/MPI_local/production1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = annotate_AF_metrics(results_df, '/home/markus/MPI_local/HPC_results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47547eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot_colour(results_df, 'iptm', 'dockq_score', 'in_training_set', ax=axes[0], corr=True)\n",
    "create_scatter_plot_colour(results_df, 'ptm', 'dockq_score',  'in_training_set', ax=axes[1], corr=True)\n",
    "create_scatter_plot(results_df, 'ranking_score', 'dockq_score', ax=axes[2], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['interface_pae_max'] = results_df['chain_pair_pae_min'].apply(lambda c: max([c[0][1], c[1][0]]))\n",
    "results_df['interface_pae_min'] = results_df['chain_pair_pae_min'].apply(lambda c: min([c[0][1], c[1][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(results_df, 'interface_pae_max', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(results_df, 'interface_pae_min', 'dockq_score', ax=axes[1], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4098800",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['chain_pair_iptm_max'] = results_df['chain_pair_iptm'].apply(lambda c: max([c[0][0], c[1][1]]))\n",
    "results_df['chain_pair_iptm_min'] = results_df['chain_pair_iptm'].apply(lambda c: min([c[0][0], c[1][1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "create_scatter_plot(results_df, 'chain_pair_iptm_max', 'dockq_score', ax=axes[0], corr=True)\n",
    "create_scatter_plot(results_df, 'chain_pair_iptm_min', 'dockq_score', ax=axes[1], corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Add a constant column to use for the x-axis to create a single split violin\n",
    "results_df['status'] = 'DockQ'\n",
    "\n",
    "# Use a split violin plot with bounded KDE using seaborn's default 'deep' palette\n",
    "ax = sns.violinplot (data=results_df, x='status', y='dockq_score', hue='in_training_set', \n",
    "                   split=True, palette='deep', inner='box', bw_adjust=0.3, cut=0.1)\n",
    "\n",
    "\n",
    "# Manually add legend for hue\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = ['Not in Training Set', 'In Training Set']\n",
    "ax.legend(handles, labels, title='in_training_set', loc='upper right')\n",
    "\n",
    "\n",
    "# Calculate counts for the legend\n",
    "total_points = len(results_df)\n",
    "in_training_count = results_df['in_training_set'].sum()\n",
    "not_in_training_count = total_points - in_training_count\n",
    "\n",
    "# Create legend text\n",
    "legend_text = (\n",
    "    f'Total points: {total_points}\\n'\n",
    "    f'In training set: {in_training_count}\\n'\n",
    "    f'Not in training set: {not_in_training_count}'\n",
    ")\n",
    "\n",
    "# Add legend to the plot using a text box\n",
    "at = AnchoredText(legend_text, prop=dict(size=10), frameon=True, loc='upper left')\n",
    "at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "ax.add_artist(at)\n",
    "\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('DockQ score by training set inclusion', fontsize=16)\n",
    "ax.set_xlabel('')\n",
    "ax.set_xticklabels([''])\n",
    "ax.set_ylabel('DockQ Score', fontsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Clean up the temporary column\n",
    "results_df.drop(columns=['status'], inplace=True)\n",
    "\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_violin.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39c1ea",
   "metadata": {},
   "source": [
    "## AUR / PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c172d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['iptm', 'ranking_score', 'ptm', 'chain_pair_iptm_max', 'chain_pair_iptm_min', \n",
    "           'interface_pae_max', 'interface_pae_min']\n",
    "\n",
    "# Create binary labels based on dockQ\n",
    "y_true = (results_df['dockq_score'] >= 0.80).astype(int)\n",
    "\n",
    "# Report number of 1s and 0s in y_true\n",
    "print(\"Dataset Class Distribution:\")\n",
    "print(f\"Total samples: {len(y_true)}\")\n",
    "print(f\"Positive samples (dockQ >= 0.80): {y_true.sum()} ({y_true.mean():.3f})\")\n",
    "print(f\"Negative samples (dockQ < 0.80): {len(y_true) - y_true.sum()} ({1 - y_true.mean():.3f})\")\n",
    "print()\n",
    "\n",
    "print(\"AUC Results and Optimal Cutoffs:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "auc_results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    # Get predictions (higher values should indicate better models for most metrics)\n",
    "    # For PAE metrics, we need to invert since lower PAE is better\n",
    "    if 'pae' in metric:\n",
    "        y_scores = -results_df[metric]  # Invert PAE scores\n",
    "        original_scores = results_df[metric]  # Keep original for cutoff reporting\n",
    "    else:\n",
    "        y_scores = results_df[metric]\n",
    "        original_scores = results_df[metric]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(y_scores.isna() | y_true.isna())\n",
    "    y_scores_clean = y_scores[mask]\n",
    "    y_true_clean = y_true[mask]\n",
    "    original_scores_clean = original_scores[mask]\n",
    "    \n",
    "    if len(y_scores_clean) == 0:\n",
    "        print(f\"{metric}: No valid data\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate PR curve and AUC\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true_clean, y_scores_clean)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true_clean, y_scores_clean)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Find optimal cutoffs\n",
    "    # 1. F1-score optimal cutoff (from PR curve)\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
    "    f1_scores = np.nan_to_num(f1_scores)  # Handle division by zero\n",
    "    optimal_f1_idx = np.argmax(f1_scores)\n",
    "    optimal_f1_threshold = pr_thresholds[optimal_f1_idx]\n",
    "    optimal_f1_score = f1_scores[optimal_f1_idx]\n",
    "    \n",
    "    # 2. Youden's J statistic optimal cutoff (from ROC curve)\n",
    "    # J = sensitivity + specificity - 1 = tpr - fpr\n",
    "    youden_j = tpr - fpr\n",
    "    optimal_youden_idx = np.argmax(youden_j)\n",
    "    optimal_youden_threshold = roc_thresholds[optimal_youden_idx]\n",
    "    optimal_youden_j = youden_j[optimal_youden_idx]\n",
    "    \n",
    "    # Convert back to original scale for PAE metrics\n",
    "    if 'pae' in metric:\n",
    "        optimal_f1_threshold_original = -optimal_f1_threshold\n",
    "        optimal_youden_threshold_original = -optimal_youden_threshold\n",
    "    else:\n",
    "        optimal_f1_threshold_original = optimal_f1_threshold\n",
    "        optimal_youden_threshold_original = optimal_youden_threshold\n",
    "    \n",
    "    auc_results[metric] = {\n",
    "        'PR_AUC': pr_auc, \n",
    "        'ROC_AUC': roc_auc,\n",
    "        'optimal_f1_threshold': optimal_f1_threshold_original,\n",
    "        'optimal_f1_score': optimal_f1_score,\n",
    "        'optimal_youden_threshold': optimal_youden_threshold_original,\n",
    "        'optimal_youden_j': optimal_youden_j\n",
    "    }\n",
    "    \n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"  Optimal F1 cutoff: {optimal_f1_threshold_original:.4f} (F1 = {optimal_f1_score:.4f})\")\n",
    "    print(f\"  Optimal Youden cutoff: {optimal_youden_threshold_original:.4f} (J = {optimal_youden_j:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Create a histogram of ROC-AUC values for all metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract ROC-AUC values and sort them in descending order\n",
    "roc_auc_values = {m: auc_results[m]['ROC_AUC'] for m in metrics if m in auc_results}\n",
    "sorted_metrics = sorted(roc_auc_values.keys(), key=lambda x: roc_auc_values[x], reverse=True)\n",
    "sorted_values = [roc_auc_values[m] for m in sorted_metrics]\n",
    "\n",
    "# Create more readable labels by replacing underscores with spaces and capitalizing\n",
    "readable_labels = [m.replace('_', ' ').capitalize() for m in sorted_metrics]\n",
    "\n",
    "# Use seaborn deep color palette for the bars\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"deep\", len(sorted_metrics))\n",
    "\n",
    "# Create the bar chart with different colors for each bar\n",
    "bars = plt.bar(range(len(sorted_metrics)), sorted_values, color=colors, edgecolor='none')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(sorted_values):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "# Get class distribution for legend\n",
    "pos_samples = y_true.sum()\n",
    "neg_samples = len(y_true) - pos_samples\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks(range(len(sorted_metrics)), readable_labels, rotation=45, ha='right')\n",
    "plt.ylim(0.5, 1.0)  # Start y-axis from 0.5 to better show differences\n",
    "plt.title(f'ROC-AUC values by metric (DockQ score cutoff â‰¥ 0.80)', fontsize=16)\n",
    "plt.ylabel('ROC-AUC', fontsize=12)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random classifier (AUC=0.5)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.legend(['Random classifier (AUC=0.5)', f'Metrics (pos: {pos_samples}, neg: {neg_samples})'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_bar.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall and ROC curves for ranking_score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Get ranking_score data\n",
    "metric = 'ranking_score'\n",
    "y_scores = results_df[metric]\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~(y_scores.isna() | y_true.isna())\n",
    "y_scores_clean = y_scores[mask]\n",
    "y_true_clean = y_true[mask]\n",
    "\n",
    "# Calculate and plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_true_clean, y_scores_clean)\n",
    "pr_auc = auc(recall, precision)\n",
    "ax1.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title(f'Precision-Recall Curve - {metric}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate and plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true_clean, y_scores_clean)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title(f'ROC Curve - {metric}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nSummary for {metric}:\")\n",
    "print(f\"Total samples: {len(y_true_clean)}\")\n",
    "print(f\"Positive samples (dockq > 0.23): {y_true_clean.sum()}\")\n",
    "print(f\"Negative samples (dockq <= 0.23): {len(y_true_clean) - y_true_clean.sum()}\")\n",
    "print(f\"Baseline accuracy: {y_true_clean.sum() / len(y_true_clean):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define DockQ score categories with their ranges and labels\n",
    "categories = {\n",
    "    'Incorrect': [0, 0.23],\n",
    "    'Acceptable Quality': [0.23, 0.49],\n",
    "    'Medium Quality': [0.49, 0.80],\n",
    "    'High Quality': [0.80, 1.0]\n",
    "}\n",
    "\n",
    "# First, filter out NaN DockQ scores\n",
    "valid_dockq_df = results_df.dropna(subset=['dockq_score']).copy()\n",
    "print(f\"Total data points: {len(results_df)}\")\n",
    "print(f\"Data points with valid DockQ scores: {len(valid_dockq_df)}\")\n",
    "print(f\"Data points with NaN DockQ scores: {len(results_df) - len(valid_dockq_df)}\")\n",
    "\n",
    "# Create a new column to categorize each data point\n",
    "def categorize_dockq(score):\n",
    "    for category, (lower, upper) in categories.items():\n",
    "        if lower <= score < upper or (category == 'High Quality' and score == upper):\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "valid_dockq_df['dockq_category'] = valid_dockq_df['dockq_score'].apply(categorize_dockq)\n",
    "\n",
    "# Count the number of data points in each category\n",
    "category_counts = valid_dockq_df['dockq_category'].value_counts().reindex(categories.keys())\n",
    "total_points = len(valid_dockq_df)\n",
    "\n",
    "# Calculate percentages for each category\n",
    "percentages = (category_counts / total_points * 100).round(1)\n",
    "\n",
    "# Create labels for the legend with counts and percentages\n",
    "labels = [f'{cat}: {count} ({pct}%)' for cat, count, pct in \n",
    "          zip(category_counts.index, category_counts, percentages)]\n",
    "\n",
    "# Use seaborn colors but arrange them in order for low to high quality\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette(\"deep\")\n",
    "colors = [palette[3], palette[1], palette[0], palette[2]]  # Red, Yellow-ish, Blue, Green\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a smaller pie chart with more space for the legend\n",
    "wedges, texts = plt.pie(category_counts, colors=colors, \n",
    "                        wedgeprops={'edgecolor': 'w', 'linewidth': 1},\n",
    "                        radius=0.7)  # Make the pie chart smaller relative to the figure\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of DockQ score categories', fontsize=16)\n",
    "\n",
    "# Add a legend with counts and percentages with larger font size\n",
    "plt.legend(wedges, labels, title=f\"Total: {total_points} structures\",\n",
    "           loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1), \n",
    "           fontsize=12, title_fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure before showing it\n",
    "plt.savefig('/home/markus/Desktop/Thesis/structure_pie.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DockQ scores sorted from high to low with release date and entry ID\n",
    "# First, select only the necessary columns and remove rows with NaN DockQ scores\n",
    "score_df = results_df[['Entry ID', 'Release Date', 'dockq_score']].dropna(subset=['dockq_score']).copy()\n",
    "\n",
    "# Sort by DockQ score from highest to lowest\n",
    "score_df = score_df.sort_values(by='dockq_score', ascending=False)\n",
    "\n",
    "# Format the output with consistent spacing\n",
    "print(f\"{'Entry ID':<10} {'Release Date':<12} {'DockQ Score':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Print each row\n",
    "for _, row in score_df.iterrows():\n",
    "    print(f\"{row['Entry ID']:<10} {row['Release Date']:<12} {row['dockq_score']:.4f}\")\n",
    "\n",
    "# Print some summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total entries: {len(score_df)}\")\n",
    "print(f\"Average DockQ score: {score_df['dockq_score'].mean():.4f}\")\n",
    "print(f\"Median DockQ score: {score_df['dockq_score'].median():.4f}\")\n",
    "print(f\"Highest DockQ score: {score_df['dockq_score'].max():.4f}\")\n",
    "print(f\"Lowest DockQ score: {score_df['dockq_score'].min():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
