{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a438be9d",
   "metadata": {},
   "source": [
    "This file is used to create a dataset of structures of known interactions.\n",
    "Steps:\n",
    "1. generate dataset of ARMs and TFs\n",
    "2. For all ARMs and TFs: run BLASTp against the pdb database\n",
    "3. Create intersection of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from typing import List\n",
    "\n",
    "BLAST_BIN: str = \"blastp\"\n",
    "BLAST_DB: str = \"/home/markus/MPI_local/data/BLAST_PDB/pdbaa/pdbaa\"\n",
    "E_VAL_CUTOFF: str = \"0.00001\"\n",
    "TF_QUERY_DIR: str = '../../production1/tf_unrev_fasta'\n",
    "ARM_QUERY_DIR: str = '../../production1/arm_all_uniprot_rev_fasta'\n",
    "# don't change these two, instead adjust e-lim filter and others later\n",
    "TF_OUTPUT_DIR: str = '../../production1/blastp_results/tf_blastp_no_e_lim_new_fmt' \n",
    "ARM_OUTPUT_DIR: str = '../../production1/blastp_results/arm_blastp_no_e_lim_new_fmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_blastp(fasta_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Run BLASTp for a single FASTA file against the PDB database.\n",
    "    \n",
    "    Args:\n",
    "        fasta_path: Path to the input FASTA file\n",
    "        output_path: Path where the BLAST output will be saved\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the FASTA file does not exist\n",
    "        subprocess.CalledProcessError: If BLAST command fails\n",
    "    \"\"\"\n",
    "    if os.path.exists(fasta_path):\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                BLAST_BIN,\n",
    "                \"-query\", fasta_path,\n",
    "                \"-db\", BLAST_DB,\n",
    "                # \"-evalue\", E_VAL_CUTOFF,\n",
    "                \"-out\", output_path,\n",
    "                \"-outfmt\", \"7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovs qcovhsp qcovus\"\n",
    "            ], check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"BLAST error for {fasta_path}: {e}\")\n",
    "    else:\n",
    "        raise Exception(f\"No FASTA file found: {fasta_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "def blastp_for_dir(input_dir: str, output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Run BLASTp for all FASTA files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing input FASTA files\n",
    "        output_dir: Directory where BLAST output files will be saved\n",
    "    \"\"\"\n",
    "    fasta_files: List[Path] = list(Path(input_dir).glob('*.fasta'))\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    i: int = 0\n",
    "    duplicates: int = 0\n",
    "    for fasta_file in fasta_files:\n",
    "            if i % max(1, len(fasta_files)//10) == 0:\n",
    "                print(f\"{i} of {len(fasta_files)}\")\n",
    "\n",
    "            fasta_path: Path = fasta_file\n",
    "            uid: str = fasta_file.stem\n",
    "            output_path: str = os.path.join(output_dir, f\"{uid}_blastp.out\")\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                # print(f\"Skipping {uid}, output already exists.\")\n",
    "                duplicates += 1\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            run_blastp(str(fasta_path), output_path)\n",
    "            i += 1\n",
    "    print(f\"Found {i} proteins in {input_dir}.\\nRan blastp on {i-duplicates} proteins.\\nFound existing blast file for {duplicates} proteins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blastp_for_dir(TF_QUERY_DIR, TF_OUTPUT_DIR)\n",
    "blastp_for_dir(ARM_QUERY_DIR, ARM_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "blastp_for_dir(TF_QUERY_DIR, TF_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55091117",
   "metadata": {},
   "source": [
    "## Filter BLAST Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea05c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_blastp_out(df: pd.DataFrame, identity_cutoff :int|bool=False, score_cutoff :int|bool=False, evalue_cutoff :float|bool = False, coverage_cutoff:float|bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and filter BLAST output DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw BLAST output DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Filtered and cleaned DataFrame with additional columns for PDB ID and chain\n",
    "    \"\"\"\n",
    "    df['pdb_id'] = df['subject'].apply(lambda x: x.split('_')[0] if '_' in x else x)\n",
    "    df['chain'] = df['subject'].apply(lambda x: x.split('_')[1] if len(x.split('_')) > 1 else '')\n",
    "    df = df.drop_duplicates(subset='subject')\n",
    "    df['%identity'] = pd.to_numeric(df['%identity'], errors='raise')\n",
    "    df['bit score'] = pd.to_numeric(df['bit score'], errors='raise')\n",
    "    df['evalue'] = pd.to_numeric(df['evalue'], errors='raise')\n",
    "    df['% query coverage per subject'] = pd.to_numeric(df['% query coverage per subject'], errors='raise')\n",
    "    if identity_cutoff:\n",
    "        df = df[df['%identity'] >= identity_cutoff]\n",
    "    if score_cutoff:\n",
    "        df = df[df['bit score'] >= score_cutoff]\n",
    "    if evalue_cutoff:\n",
    "        df = df[df['evalue'] <= evalue_cutoff]\n",
    "    if coverage_cutoff:\n",
    "        df = df[df['% query coverage per subject'] >= coverage_cutoff]\n",
    "    return df\n",
    "\n",
    "def read_blast_to_df(output_dir: str, column_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read BLAST output files from a directory and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing BLAST output files\n",
    "        column_names: List of column names for the DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Combined and cleaned DataFrame with all BLAST results\n",
    "    \"\"\"\n",
    "    output_files: List[Path] = list(Path(output_dir).glob('*_blastp.out'))\n",
    "    output_list: List[List[str]] = []\n",
    "    \n",
    "    for output_file in output_files:\n",
    "        try:\n",
    "            with open(output_file, 'r') as f:\n",
    "                lines: List[str] = [line for line in f if not line.startswith('#')]\n",
    "            output_list.append(lines)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {output_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Flatten the list of lines and split by tab to create rows\n",
    "    rows: List[List[str]] = [line.strip().split('\\t') for file_lines in output_list for line in file_lines if line.strip()]\n",
    "    \n",
    "    if not rows:\n",
    "        print(f\"Warning: No data found in {output_dir}\")\n",
    "        return pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLAST_IDENTITY_CUTOFF: int|bool = False\n",
    "BLAST_SCORE_CUTOFF: int|bool = False\n",
    "BLAST_EVALUE_CUTOFF: float|bool = 0.00001\n",
    "BLAST_COVERAGE_CUTOFF: float|bool = 0.5\n",
    "\n",
    "\n",
    "columns: List[str] = [\n",
    "    \"query\", \"subject\", \"%identity\", \"alignment length\", \"mismatches\", \"gap opens\",\n",
    "    \"q. start\", \"q. end\", \"s. start\", \"s. end\", \"evalue\", \"bit score\", \"% query coverage per subject\", \"% query coverage per hsp\", \"% query coverage per uniq subject\"\n",
    "]\n",
    "\n",
    "tf_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(TF_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(tf_blast_df))\n",
    "arm_blast_df: pd.DataFrame = clean_blastp_out(read_blast_to_df(ARM_OUTPUT_DIR, columns), \n",
    "                                             identity_cutoff=BLAST_IDENTITY_CUTOFF,\n",
    "                                             score_cutoff=BLAST_SCORE_CUTOFF,\n",
    "                                             evalue_cutoff=BLAST_EVALUE_CUTOFF,\n",
    "                                             coverage_cutoff=BLAST_COVERAGE_CUTOFF)\n",
    "print(len(arm_blast_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763b941",
   "metadata": {},
   "source": [
    "## Create dataset with candidate PDBs (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc82bb6",
   "metadata": {},
   "source": [
    "TODO: create filter for previously reviewed structures (PDB ID + chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa845f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "review_files = glob.glob('/home/markus/MPI_local/production1/structure_reviews/*.csv')\n",
    "reviews_df = pd.concat([pd.read_csv(f) for f in review_files], ignore_index=True)\n",
    "reviews_df = reviews_df.drop_duplicates(subset=['pdb_id', 'query_x', 'query_y', 'chain_x', 'chain_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_df: pd.DataFrame = tf_blast_df.merge(\n",
    "    arm_blast_df, \n",
    "    how='inner', \n",
    "    on='pdb_id', \n",
    "    suffixes=('_tf', '_arm')\n",
    ")\n",
    "\n",
    "# filter out candidates with same chains\n",
    "intersect_df = intersect_df[intersect_df['chain_x'] != intersect_df['chain_y']]\n",
    "\n",
    "print(f\"Intersection contains {len(intersect_df)} structures\")\n",
    "\n",
    "# filter out candidates that were reviewed before\n",
    "intersect_df = intersect_df.merge(\n",
    "    reviews_df[['pdb_id', 'query_x', 'query_y', 'chain_x', 'chain_y']],\n",
    "    on=['pdb_id', 'query_x', 'query_y', 'chain_x', 'chain_y'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "intersect_df = intersect_df[intersect_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "print(f\"After removing reviewed structures: {len(intersect_df)} structures remain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
